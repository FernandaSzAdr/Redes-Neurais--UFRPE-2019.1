{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RBF Network",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FernandaSzAdr/Redes-Neurais--UFRPE-2019.1/blob/master/RBF%20-%20Radial%20Basis%20Function/codigos/RBF_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "6jjk-OxT35MI",
        "colab_type": "code",
        "outputId": "a1295db8-3944-4047-b0b8-27b4d5500d71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/FernandaSzAdr/Redes-Neurais--UFRPE-2019.1.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Redes-Neurais--UFRPE-2019.1'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/82)   \u001b[K\rremote: Counting objects:   2% (2/82)   \u001b[K\rremote: Counting objects:   3% (3/82)   \u001b[K\rremote: Counting objects:   4% (4/82)   \u001b[K\rremote: Counting objects:   6% (5/82)   \u001b[K\rremote: Counting objects:   7% (6/82)   \u001b[K\rremote: Counting objects:   8% (7/82)   \u001b[K\rremote: Counting objects:   9% (8/82)   \u001b[K\rremote: Counting objects:  10% (9/82)   \u001b[K\rremote: Counting objects:  12% (10/82)   \u001b[K\rremote: Counting objects:  13% (11/82)   \u001b[K\rremote: Counting objects:  14% (12/82)   \u001b[K\rremote: Counting objects:  15% (13/82)   \u001b[K\rremote: Counting objects:  17% (14/82)   \u001b[K\rremote: Counting objects:  18% (15/82)   \u001b[K\rremote: Counting objects:  19% (16/82)   \u001b[K\rremote: Counting objects:  20% (17/82)   \u001b[K\rremote: Counting objects:  21% (18/82)   \u001b[K\rremote: Counting objects:  23% (19/82)   \u001b[K\rremote: Counting objects:  24% (20/82)   \u001b[K\rremote: Counting objects:  25% (21/82)   \u001b[K\rremote: Counting objects:  26% (22/82)   \u001b[K\rremote: Counting objects:  28% (23/82)   \u001b[K\rremote: Counting objects:  29% (24/82)   \u001b[K\rremote: Counting objects:  30% (25/82)   \u001b[K\rremote: Counting objects:  31% (26/82)   \u001b[K\rremote: Counting objects:  32% (27/82)   \u001b[K\rremote: Counting objects:  34% (28/82)   \u001b[K\rremote: Counting objects:  35% (29/82)   \u001b[K\rremote: Counting objects:  36% (30/82)   \u001b[K\rremote: Counting objects:  37% (31/82)   \u001b[K\rremote: Counting objects:  39% (32/82)   \u001b[K\rremote: Counting objects:  40% (33/82)   \u001b[K\rremote: Counting objects:  41% (34/82)   \u001b[K\rremote: Counting objects:  42% (35/82)   \u001b[K\rremote: Counting objects:  43% (36/82)   \u001b[K\rremote: Counting objects:  45% (37/82)   \u001b[K\rremote: Counting objects:  46% (38/82)   \u001b[K\rremote: Counting objects:  47% (39/82)   \u001b[K\rremote: Counting objects:  48% (40/82)   \u001b[K\rremote: Counting objects:  50% (41/82)   \u001b[K\rremote: Counting objects:  51% (42/82)   \u001b[K\rremote: Counting objects:  52% (43/82)   \u001b[K\rremote: Counting objects:  53% (44/82)   \u001b[K\rremote: Counting objects:  54% (45/82)   \u001b[K\rremote: Counting objects:  56% (46/82)   \u001b[K\rremote: Counting objects:  57% (47/82)   \u001b[K\rremote: Counting objects:  58% (48/82)   \u001b[K\rremote: Counting objects:  59% (49/82)   \u001b[K\rremote: Counting objects:  60% (50/82)   \u001b[K\rremote: Counting objects:  62% (51/82)   \u001b[K\rremote: Counting objects:  63% (52/82)   \u001b[K\rremote: Counting objects:  64% (53/82)   \u001b[K\rremote: Counting objects:  65% (54/82)   \u001b[K\rremote: Counting objects:  67% (55/82)   \u001b[K\rremote: Counting objects:  68% (56/82)   \u001b[K\rremote: Counting objects:  69% (57/82)   \u001b[K\rremote: Counting objects:  70% (58/82)   \u001b[K\rremote: Counting objects:  71% (59/82)   \u001b[K\rremote: Counting objects:  73% (60/82)   \u001b[K\rremote: Counting objects:  74% (61/82)   \u001b[K\rremote: Counting objects:  75% (62/82)   \u001b[K\rremote: Counting objects:  76% (63/82)   \u001b[K\rremote: Counting objects:  78% (64/82)   \u001b[K\rremote: Counting objects:  79% (65/82)   \u001b[K\rremote: Counting objects:  80% (66/82)   \u001b[K\rremote: Counting objects:  81% (67/82)   \u001b[K\rremote: Counting objects:  82% (68/82)   \u001b[K\rremote: Counting objects:  84% (69/82)   \u001b[K\rremote: Counting objects:  85% (70/82)   \u001b[K\rremote: Counting objects:  86% (71/82)   \u001b[K\rremote: Counting objects:  87% (72/82)   \u001b[K\rremote: Counting objects:  89% (73/82)   \u001b[K\rremote: Counting objects:  90% (74/82)   \u001b[K\rremote: Counting objects:  91% (75/82)   \u001b[K\rremote: Counting objects:  92% (76/82)   \u001b[K\rremote: Counting objects:  93% (77/82)   \u001b[K\rremote: Counting objects:  95% (78/82)   \u001b[K\rremote: Counting objects:  96% (79/82)   \u001b[K\rremote: Counting objects:  97% (80/82)   \u001b[K\rremote: Counting objects:  98% (81/82)   \u001b[K\rremote: Counting objects: 100% (82/82)   \u001b[K\rremote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects:   1% (1/66)   \u001b[K\rremote: Compressing objects:   3% (2/66)   \u001b[K\rremote: Compressing objects:   4% (3/66)   \u001b[K\rremote: Compressing objects:   6% (4/66)   \u001b[K\rremote: Compressing objects:   7% (5/66)   \u001b[K\rremote: Compressing objects:   9% (6/66)   \u001b[K\rremote: Compressing objects:  10% (7/66)   \u001b[K\rremote: Compressing objects:  12% (8/66)   \u001b[K\rremote: Compressing objects:  13% (9/66)   \u001b[K\rremote: Compressing objects:  15% (10/66)   \u001b[K\rremote: Compressing objects:  16% (11/66)   \u001b[K\rremote: Compressing objects:  18% (12/66)   \u001b[K\rremote: Compressing objects:  19% (13/66)   \u001b[K\rremote: Compressing objects:  21% (14/66)   \u001b[K\rremote: Compressing objects:  22% (15/66)   \u001b[K\rremote: Compressing objects:  24% (16/66)   \u001b[K\rremote: Compressing objects:  25% (17/66)   \u001b[K\rremote: Compressing objects:  27% (18/66)   \u001b[K\rremote: Compressing objects:  28% (19/66)   \u001b[K\rremote: Compressing objects:  30% (20/66)   \u001b[K\rremote: Compressing objects:  31% (21/66)   \u001b[K\rremote: Compressing objects:  33% (22/66)   \u001b[K\rremote: Compressing objects:  34% (23/66)   \u001b[K\rremote: Compressing objects:  36% (24/66)   \u001b[K\rremote: Compressing objects:  37% (25/66)   \u001b[K\rremote: Compressing objects:  39% (26/66)   \u001b[K\rremote: Compressing objects:  40% (27/66)   \u001b[K\rremote: Compressing objects:  42% (28/66)   \u001b[K\rremote: Compressing objects:  43% (29/66)   \u001b[K\rremote: Compressing objects:  45% (30/66)   \u001b[K\rremote: Compressing objects:  46% (31/66)   \u001b[K\rremote: Compressing objects:  48% (32/66)   \u001b[K\rremote: Compressing objects:  50% (33/66)   \u001b[K\rremote: Compressing objects:  51% (34/66)   \u001b[K\rremote: Compressing objects:  53% (35/66)   \u001b[K\rremote: Compressing objects:  54% (36/66)   \u001b[K\rremote: Compressing objects:  56% (37/66)   \u001b[K\rremote: Compressing objects:  57% (38/66)   \u001b[K\rremote: Compressing objects:  59% (39/66)   \u001b[K\rremote: Compressing objects:  60% (40/66)   \u001b[K\rremote: Compressing objects:  62% (41/66)   \u001b[K\rremote: Compressing objects:  63% (42/66)   \u001b[K\rremote: Compressing objects:  65% (43/66)   \u001b[K\rremote: Compressing objects:  66% (44/66)   \u001b[K\rremote: Compressing objects:  68% (45/66)   \u001b[K\rremote: Compressing objects:  69% (46/66)   \u001b[K\rremote: Compressing objects:  71% (47/66)   \u001b[K\rremote: Compressing objects:  72% (48/66)   \u001b[K\rremote: Compressing objects:  74% (49/66)   \u001b[K\rremote: Compressing objects:  75% (50/66)   \u001b[K\rremote: Compressing objects:  77% (51/66)   \u001b[K\rremote: Compressing objects:  78% (52/66)   \u001b[K\rremote: Compressing objects:  80% (53/66)   \u001b[K\rremote: Compressing objects:  81% (54/66)   \u001b[K\rremote: Compressing objects:  83% (55/66)   \u001b[K\rremote: Compressing objects:  84% (56/66)   \u001b[K\rremote: Compressing objects:  86% (57/66)   \u001b[K\rremote: Compressing objects:  87% (58/66)   \u001b[K\rremote: Compressing objects:  89% (59/66)   \u001b[K\rremote: Compressing objects:  90% (60/66)   \u001b[K\rremote: Compressing objects:  92% (61/66)   \u001b[K\rremote: Compressing objects:  93% (62/66)   \u001b[K\rremote: Compressing objects:  95% (63/66)   \u001b[K\rremote: Compressing objects:  96% (64/66)   \u001b[K\rremote: Compressing objects:  98% (65/66)   \u001b[K\rremote: Compressing objects: 100% (66/66)   \u001b[K\rremote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "Unpacking objects:   1% (1/82)   \rUnpacking objects:   2% (2/82)   \rUnpacking objects:   3% (3/82)   \rUnpacking objects:   4% (4/82)   \rUnpacking objects:   6% (5/82)   \rUnpacking objects:   7% (6/82)   \rUnpacking objects:   8% (7/82)   \rUnpacking objects:   9% (8/82)   \rUnpacking objects:  10% (9/82)   \rUnpacking objects:  12% (10/82)   \rUnpacking objects:  13% (11/82)   \rUnpacking objects:  14% (12/82)   \rUnpacking objects:  15% (13/82)   \rUnpacking objects:  17% (14/82)   \rUnpacking objects:  18% (15/82)   \rUnpacking objects:  19% (16/82)   \rUnpacking objects:  20% (17/82)   \rUnpacking objects:  21% (18/82)   \rUnpacking objects:  23% (19/82)   \rUnpacking objects:  24% (20/82)   \rUnpacking objects:  25% (21/82)   \rUnpacking objects:  26% (22/82)   \rUnpacking objects:  28% (23/82)   \rUnpacking objects:  29% (24/82)   \rUnpacking objects:  30% (25/82)   \rUnpacking objects:  31% (26/82)   \rUnpacking objects:  32% (27/82)   \rUnpacking objects:  34% (28/82)   \rUnpacking objects:  35% (29/82)   \rUnpacking objects:  36% (30/82)   \rUnpacking objects:  37% (31/82)   \rUnpacking objects:  39% (32/82)   \rUnpacking objects:  40% (33/82)   \rUnpacking objects:  41% (34/82)   \rUnpacking objects:  42% (35/82)   \rUnpacking objects:  43% (36/82)   \rUnpacking objects:  45% (37/82)   \rUnpacking objects:  46% (38/82)   \rUnpacking objects:  47% (39/82)   \rUnpacking objects:  48% (40/82)   \rUnpacking objects:  50% (41/82)   \rUnpacking objects:  51% (42/82)   \rUnpacking objects:  52% (43/82)   \rUnpacking objects:  53% (44/82)   \rUnpacking objects:  54% (45/82)   \rUnpacking objects:  56% (46/82)   \rUnpacking objects:  57% (47/82)   \rUnpacking objects:  58% (48/82)   \rUnpacking objects:  59% (49/82)   \rUnpacking objects:  60% (50/82)   \rUnpacking objects:  62% (51/82)   \rUnpacking objects:  63% (52/82)   \rUnpacking objects:  64% (53/82)   \rUnpacking objects:  65% (54/82)   \rUnpacking objects:  67% (55/82)   \rUnpacking objects:  68% (56/82)   \rUnpacking objects:  69% (57/82)   \rUnpacking objects:  70% (58/82)   \rUnpacking objects:  71% (59/82)   \rUnpacking objects:  73% (60/82)   \rUnpacking objects:  74% (61/82)   \rUnpacking objects:  75% (62/82)   \rUnpacking objects:  76% (63/82)   \rUnpacking objects:  78% (64/82)   \rUnpacking objects:  79% (65/82)   \rUnpacking objects:  80% (66/82)   \rUnpacking objects:  81% (67/82)   \rUnpacking objects:  82% (68/82)   \rremote: Total 82 (delta 12), reused 20 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects:  84% (69/82)   \rUnpacking objects:  85% (70/82)   \rUnpacking objects:  86% (71/82)   \rUnpacking objects:  87% (72/82)   \rUnpacking objects:  89% (73/82)   \rUnpacking objects:  90% (74/82)   \rUnpacking objects:  91% (75/82)   \rUnpacking objects:  92% (76/82)   \rUnpacking objects:  93% (77/82)   \rUnpacking objects:  95% (78/82)   \rUnpacking objects:  96% (79/82)   \rUnpacking objects:  97% (80/82)   \rUnpacking objects:  98% (81/82)   \rUnpacking objects: 100% (82/82)   \rUnpacking objects: 100% (82/82), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dpyPuEsxQ_r1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "local = 'Redes-Neurais--UFRPE-2019.1/RBF - Radial Basis Function/codigos/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7A0GQWDGtrpX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Rede Neural de Base Radial (RBF)"
      ]
    },
    {
      "metadata": {
        "id": "dhC2XhrC55_R",
        "colab_type": "code",
        "outputId": "921dd197-5eb3-49f5-b1eb-d260e988837f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install ipdb\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ipdb\n",
            "  Downloading https://files.pythonhosted.org/packages/6d/43/c3c2e866a8803e196d6209595020a4a6db1a3c5d07c01455669497ae23d0/ipdb-0.12.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from ipdb) (40.8.0)\n",
            "Requirement already satisfied: ipython>=5.1.0 in /usr/local/lib/python3.6/dist-packages (from ipdb) (5.5.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.6.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.4.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (1.0.15)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (2.1.3)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.3.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.1.0->ipdb) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0->ipdb) (0.1.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0->ipdb) (1.11.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.1.0->ipdb) (0.2.0)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/59/24/91/695211bd228d40fb22dff0ce3f05ba41ab724ab771736233f3\n",
            "Successfully built ipdb\n",
            "Installing collected packages: ipdb\n",
            "Successfully installed ipdb-0.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QWGJ2VZj_uRo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Um pouco sobre a teoria...\n"
      ]
    },
    {
      "metadata": {
        "id": "E8u87tr1ANMU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As redes RBF são redes de alimentação direta (feedforward) consistindo de três camadas:\n",
        "\n",
        "\n",
        "1.   **Camada de entrada**: propaga os estímulos\n",
        "2.   **Camada escondida**: Unidades de processamento localmente sintonizáveis, utilizando mapeamento não linear.\n",
        "3.   **Camada de saída**: Unidades de processamento lineares.\n",
        "\n",
        "\n",
        "****\n",
        "\n",
        "**O treinamento dessa rede ocorre de forma híbrida**, combinando aprendizagem não supervisionada (ANS) com a supervisionada(AS). Isso ocorre, pois em geral não se sabe quais saídas se desejam para a camada escondida. Sendo assim, a distribuição de trabalhos ocorre:\n",
        "*   **ANS**: Treina a camada escondida definindo seus parâmetros livres (centros, larguras dos campos receptivos e pesos).\n",
        "*   **AS**: Determina os valores dos pesos entre as camadas escondidas e de saída, considerando constantes os parâmetros já definidos.\n",
        "\n",
        "\n",
        "****\n",
        "\n",
        "**O aprendizado consiste em** determinar os valores para:\n",
        "*   centro das funções de base radial,\n",
        "*   largura das funções,\n",
        "*   pesos da camada de saída.\n",
        "\n",
        "\n",
        "Além disso, para cada neurônio da camada escondida, ele computa uma função de base radial.\n",
        "\n",
        "\n",
        "Os passos necessários são:\n",
        "1.   Utilizar um algoritmo ANS para encontrar os centros (protótipo para um cluster) das RBF;\n",
        "2.   Utilizar métodos heurísticos para determinar a largura (área de influência de um cluster) de cada função;\n",
        "3.   Utilizar um AS para determinar os pesos da camada de saída da rede.\n",
        "\n",
        "\n",
        "### O código foi retirado do site [Python Machine Learning](https://pythonmachinelearning.pro/using-neural-networks-for-regression-radial-basis-function-networks/)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_ulVaA8RPSwo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Definição da função de base radial"
      ]
    },
    {
      "metadata": {
        "id": "YeqYYe9hPY4v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rbfGaussiana(x, c, s):\n",
        "  return np.exp(-1 / (2 * s**2) * (x-c)**2)\n",
        "  \n",
        "def rbfMultiquadratica(x, c, s):\n",
        "  return np.sqrt(x**2 + c**2)/c\n",
        "\n",
        "def rbfMultiquadraticaInversa(x, c, s):\n",
        "  return c/np.sqrt(x**2 + c**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EOIor4J3PFBL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1ª Etapa: Inicialização dos grupos com K-Means"
      ]
    },
    {
      "metadata": {
        "id": "_yf95ysF2wAn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def kmeans(X, k):\n",
        "  \n",
        "    \"\"\"Performs k-means clustering for 1D input\n",
        "    \n",
        "    Arguments:\n",
        "        X {ndarray} -- vetor de características da entrada\n",
        "        k {int} -- número de grupos\n",
        "    \n",
        "    Returns:\n",
        "        ndarray -- retorna um array com os centros dos grupos\n",
        "    \"\"\"\n",
        "\n",
        "    # randomly select initial clusters from input data\n",
        "    clusters = np.random.choice(np.squeeze(X), size=k)\n",
        "    prevClusters = clusters.copy()\n",
        "    stds = np.zeros(k)\n",
        "    converged = False\n",
        "\n",
        "    while not converged:\n",
        "        \"\"\"\n",
        "        compute distances for each cluster center to each point \n",
        "        where (distances[i, j] represents the distance between the ith point and jth cluster)\n",
        "        \"\"\"\n",
        "        distances = np.squeeze(np.abs(X[:, np.newaxis] - clusters[np.newaxis, :]))\n",
        "\n",
        "        # find the cluster that's closest to each point\n",
        "        closestCluster = np.argmin(distances, axis=1)\n",
        "\n",
        "        # update clusters by taking the mean of all of the points assigned to that cluster\n",
        "        for i in range(k):\n",
        "            pointsForCluster = X[closestCluster == i]\n",
        "            if len(pointsForCluster) > 0:\n",
        "                clusters[i] = np.mean(pointsForCluster, axis=0)\n",
        "\n",
        "        # converge if clusters haven't moved\n",
        "        converged = np.linalg.norm(clusters - prevClusters) < 1e-6\n",
        "        prevClusters = clusters.copy()\n",
        "\n",
        "    distances = np.squeeze(np.abs(X[:, np.newaxis] - clusters[np.newaxis, :]))\n",
        "    closestCluster = np.argmin(distances, axis=1)\n",
        "\n",
        "    clustersWithNoPoints = []\n",
        "    for i in range(k):\n",
        "        pointsForCluster = X[closestCluster == i]\n",
        "        if len(pointsForCluster) < 2:\n",
        "            # keep track of clusters with no points or 1 point\n",
        "            clustersWithNoPoints.append(i)\n",
        "            continue\n",
        "        else:\n",
        "            stds[i] = np.std(X[closestCluster == i])\n",
        "\n",
        "    # if there are clusters with 0 or 1 points, take the mean std of the other clusters\n",
        "    if len(clustersWithNoPoints) > 0:\n",
        "        pointsToAverage = []\n",
        "        for i in range(k):\n",
        "            if i not in clustersWithNoPoints:\n",
        "                pointsToAverage.append(X[closestCluster == i])\n",
        "        pointsToAverage = np.concatenate(pointsToAverage).ravel()\n",
        "        stds[clustersWithNoPoints] = np.mean(np.std(pointsToAverage))\n",
        "\n",
        "    return clusters, stds\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "52oUNimoPuK3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2ª Etapa - Treinamento de uma Rede Neural"
      ]
    },
    {
      "metadata": {
        "id": "RGdrOhYfPzBu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RBFNet(object):\n",
        "    \"\"\"Implementation of a Radial Basis Function Network\"\"\"\n",
        "    def __init__(self, k=2, lr=0.01, epochs=100, rbf=None, inferStds=True):\n",
        "      self.k = k #grupos\n",
        "      self.lr = lr \n",
        "      self.epochs = epochs  #número de iterações\n",
        "      self.rbf = rbf  # função de base radial\n",
        "      self.inferStds = inferStds  #se vai calcular o tamanho do cluster (std)\n",
        "\n",
        "      self.w = np.random.randn(k)\n",
        "      self.b = np.random.randn(1)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "      if self.inferStds:\n",
        "        # compute stds from data\n",
        "        # centros e a área de influência de um cluster\n",
        "        # cada neuronio possui uma largura própria\n",
        "        self.centers, self.stds = kmeans(X, self.k)\n",
        "      else:\n",
        "        # use a fixed std \n",
        "        self.centers, _ = kmeans(X, self.k)\n",
        "        dMax = max([np.abs(c1 - c2) for c1 in self.centers for c2 in self.centers])\n",
        "        # valor unico de largura para todos os neuronios\n",
        "        self.stds = np.repeat(dMax / np.sqrt(2*self.k), self.k)\n",
        "\n",
        "      # training\n",
        "      for epoch in range(self.epochs):\n",
        "        for i in range(X.shape[0]):\n",
        "          a = np.array([self.rbf(X[i], c, s) for c, s, in zip(self.centers, self.stds)])\n",
        "          F = a.T.dot(self.w) + self.b \n",
        "          \n",
        "          loss = (y[i] - F).flatten() ** 2 \n",
        "          #print('Loss: {0:.2f}'.format(loss[0]))\n",
        "\n",
        "          # backward pass\n",
        "          error = -(y[i] - F).flatten()  # saida desejada - saida obtida \n",
        "\n",
        "          # online update\n",
        "          self.w = self.w - self.lr * a * error  # peso atual - taxa aprendizagem * entrada * erro\n",
        "          self.b = self.b - self.lr * error\n",
        "\n",
        "    def predict(self, X):\n",
        "      y_pred = []\n",
        "      error = 0\n",
        "      for i in range(X.shape[0]):\n",
        "        a = np.array([self.rbf(X[i], c, s) for c, s, in zip(self.centers, self.stds)])\n",
        "        F = a.T.dot(self.w) + self.b\n",
        "        y_pred.append(F)\n",
        "\n",
        "      return np.array(y_pred)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tPLeuHRlQD3b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Executando a Rede Neural"
      ]
    },
    {
      "metadata": {
        "id": "ILtnvEwQQFkV",
        "colab_type": "code",
        "outputId": "1e279db2-538d-4aca-b19b-665464e394c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "cell_type": "code",
      "source": [
        "# sample inputs and add noise\n",
        "NUM_SAMPLES = 100\n",
        "X = np.random.uniform(0., 1., NUM_SAMPLES)\n",
        "X = np.sort(X, axis=0)\n",
        "noise = np.random.uniform(-0.1, 0.1, NUM_SAMPLES)\n",
        "y = np.sin(2 * np.pi * X)  + noise\n",
        "\n",
        "rbfnet = RBFNet(lr=1e-2, k=2, rbf=rbfGaussiana, inferStds=True)\n",
        "rbfnet.fit(X, y)\n",
        "\n",
        "y_pred = rbfnet.predict(X)\n",
        "\n",
        "errorabs = abs(y-y_pred)\n",
        "print('error: ' , np.sum(errorabs[0]/NUM_SAMPLES, axis=0))\n",
        "\n",
        "\n",
        "plt.plot(X, y, '-o', label='true')\n",
        "plt.plot(X, y_pred, '-o', label='RBF-Net')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error:  0.7246898626024048\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAGACAYAAAC6OPj9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3XlAk2e6Pv7rTUI2EnYIYVcQFdxF\nLWOr1WoXba3TOtXafdoZZ07PmTlzpmdz5tf2e6btOZ3p7Ktj7cx0utnF2sVpta12sdK4b6AiKCAQ\nQlhkSYCQ5P39ERNB3iCEEAhcn39q3i0PRsrN89zPfQuiKIogIiIiCmOykR4AERER0VAxoCEiIqKw\nx4CGiIiIwh4DGiIiIgp7DGiIiIgo7DGgISIiorCnGOkBeFmtbUO6PzZWi+Zme5BGQ4HgZzDy+BmM\nPH4GI4+fwcgbrs8gMVHv99yYmaFRKOQjPYRxj5/ByONnMPL4GYw8fgYjbyQ+gzET0BAREdH4xYCG\niIiIwh4DGiIiIgp7DGiIiIgo7DGgISIiorDHgIaIiIjCHgMaIiIiCnsMaIiIiMaRTz/9ZKSHMCxG\nTaVgIiIi6s1UYsGOogrUNtiRkqDFysIsLMgzBPw8s7kWH3+8E9dff0PwBjlKMKAhIiIahUwlFmx6\nt9j3utpq870ONKj5xS+exalTxbjuunm48cZbYDbX4pvf/Da2b38TTz31UwDAypU3YMeOT3D+/Dn8\n8pc/hSAI0Gq12LjxSej1/lsPjDQGNERERCPg9d1lOHC63u/5i+1dkseff78Eb35aLnlu3pQk3LU0\nx+8z7777Pmzb9jomTMhGVVUF/vCH53H48EHJa3/1q5/h3/99I9LTM7Bt2xvYtu11PPDAw/18RSOL\nAQ2NCcGeliUiGmkutzio44M1dWp+v+dLSorx7LNPAQC6u7sxdWpeUN53uDCgobA3HNOyRETD7a6l\nOf3Opjy+xYRqq63P8bREHf7n4flDfv+IiAgAgCAIvY47nU4AgFqtxm9/u6nP+dGKu5wo7O0oqvBz\nvDKk4yAiCqaVhVl+jmcG/EyZTAaXy9XrWGRkJBobGwAAZWVnYbfbAQA5OZPw1Vf7AAAff7wTBw/u\nD/h9Q4EzNDQqDGXJqKah728wAGBulD5ORBQOvP8P3FFUCXOjDcb4SKwszBzSzHNm5gScOXMaRmMK\nYmJiAAA5OblQqzX4zne+ienTZyI5OQUA8P3vP4af/vRpvPzy36BUqvDkk08N/YsaRoIoisFZjBsi\nq7VtSPcnJuqH/AwamkA/gyuXjLw2rMq/6jeuv3uB4E3LhhN+H4w8fgYjj5/ByBuuzyAx0f8uK87Q\nUFCYSizYeeAgquraBj3D0t+S0dWe4e9eYGjTskREFF4Y0NCQDTUpt7bBLnl8IEtG/u4VBCYEExGN\nJ0wKpiEbalJuSoJW8rgxPjLge1MTdAN6byIiGhs4Q0NDNtgZloOWo9hZsRt19npEK/XoynZDndUO\nsSMSztpsuJqMAAB7ZzdMJZZ+Z1pm5MRLbmvkchMR0fjCgIaGLCVBKxlUREcq+xw7aDmKvxS/4nvd\n3NUCwLNEJGjbocw5BtF9HBBE2Dp0eH5vBYBlfoOaI6UNQfkaiIgovDGgoSFbWZiF5/d+DEX6aQhK\nT6lu0aFGy4XJMJXkoGBKAho7m2HtaMC2s+9d9XmCzLPxzhvgvGg+ho/bknFT1lIUGGb5rjtbfRHm\nRunZoYEkFBMR0djBgIYCcuWykTKnpdd5QdUJZc4x/L36NF6ydMMtugN/MwGotdX5Zna8QU1/OTqs\nQUNEJM1srsX996/D5MlTAHjaGkycmIPHHvsvrF27GklJBsjlcnR0dODWW1dh9eo1fe4BgEmTJuP7\n3/9hr2c//fSTsNtt+POf/+Q79s///G387nd/9juevXs/w4IFX/NVLg4UAxoaNH/LRlLc8i5M1Gci\nSZuARE0Cvqw1obnrYsDvvatyDwoMs1BlacPx8kYoI2RwdPcNlgaSUExENNr1/OUxWZvUZ6Y6UBkZ\nmb2CjKeffhIfffQhAOC5534DrVaLjo4O3HXX7bjttq9L3uNPdXU1jh49itTU7AGN5bXXXsacOfOG\nHNAMaZdTaWkpli1bhpdeeqnPuX379mHNmjVYu3Ytfv/73w/lbWiU2Vmxe8DXChDwWMGjuD9vLW6Z\ncANW56wY0nubbRYAwD++8szO3DAnTfI6JgUTUbjz/vJYa6uDW3T7ZqoPWo4G/b3y8qahuvpCr2Ot\nrS2Ijo6BXC4f1LO+9a3v4uc//3mf4w0NVvzwh9/D97//XfzgB4+irq4OH364AyUlJ/HYY99Dd3f3\nkL6GgGdo7HY7fvKTn6CwsFDy/FNPPYUtW7bAYDDg3nvvxU033YScHP9NuGh0kmpJUHcpqBiI2IiE\nXq+9v1nsqtwDs82CaGUUbI5OOMTOAT1PgICPy004cKodGUl6rLk+GxkGPbZ9fg7Wix3QayOwflku\n82eIaNTbVvY+jtSf8Hu+patV8viLJVvxTvkHkudmJ03HHTm3DmocTqcTX3zxGVavvhMffrgDjz32\nPQiCgMrK8/jXf/33QT0LALKzc5Camoq9ez/Htdcu8h3fvPmPWLfuHsybtwBFRXvxt789j//8zx/j\n+ef/hOee+83ILTkplUps3rwZmzdv7nPuwoULiI6OhtHo2X67ePFiFBUVMaAJM68d+gyf1X8MIasL\nyizA6lDhhZJoKOIG3i3jtpxlfY4VGGb1mjLd9nk5Pjj9FYx5ZjQ5rJALcnS7nZBq8OoW3Xi78i0o\n86MwO+MmCIKABXkGxEer8czfD+HaGUYGM0Q0JrhE16COD0ZVVSX++Z+/DQAoLy/DPffcj0WLrsdv\nfvNz35KTzdaOf/3Xf8KkSZOhVCp73QMA8+YtwAMPPCz5/O9///vYsOG7KCxc6Dt28uRxVFVV4m9/\n2wK3242YmNghfx09BRzQKBQKKBTSt1utVsTFxflex8XF4cKFC5LX0uh00HIUX7TsgEx1+Zig6oJM\nVQ+45YCs7zeURq5Bh6sDAKAUI9FenoN6VQyQ0v97NbR0wtVkxPem3YGEGA0A4N9+txfu6BokTa6F\n2WaBMdKAGzOXIEaWhOc+3Qp5vBm7Gt/AhaPHoFVocKj+GNTzgE/dwGe7BSxKK8RduauD9vdBRBRs\nd+Tc2u9sytOmX6DWVtfneKrOiI3zfzCk9+6ZD/PjH/8H0tP7LtNHRuowe/ZcnDx5HHPmFEjm0Hz2\n2R688carAIBf//qPvuNGoxFz5hTggw/e9x1TKCLwk588i4SE3jP3wTJqkoJjY7VQKAa3Tnel/ppW\n0cB9WXUAL5163e95d6cGP7jhLmwv2YnqVjPk3VFor8zEX//tYWjVnilDW0c3/umnn+D9okrcvHAi\nUhL9V+5tsXVDJhOQOzEBcrknrWtiWgwOn3Zg03fuh057uZ7N5u0n4CifiXum34Szrq9wwnLad847\noyNCxGfV+6DRKPHNOWuH8lcRlvh9MPL4GYy8sfAZfGPGCvy66IU+x9dMv2VIX19XVyQUCpnvGT/+\n8UY88sgjWLlyOeRyGRISdIiMjIQoiigrO4PbbluBuLje9/jGsmYV1qxZ5XutVkcgLs6zKeMHP/gX\n3HvvvdBoNEhM1GPu3Nk4cqQI69evR1FRERoaGnDbbbdBoZAjNlaDqKihfWbDEtAkJSWhoeFywTOL\nxYKkpKR+72lulq4nMlDsrhocr5dux2fV+/q9RqaxIVczBf8xdwrcbhGP/vJzJEeroVVH9PoM1i2d\nhD9sP4lfvXoYj62bBUFqDQmAuaEdcXoVmpoub7U2RKsBAEdP1WFyhmdastXuwIdFFYiLUmHxpCm4\nQZ6H79X/t9/p1w/PfgqjMiUoOwLCBb8PRh4/g5E3Vj6DXM0UPJS/3pdz6J2pztVMGdLX19Rkg9Pp\n9j1DrY7Bddctwc9//mu4XG48+OA3IZPJ0NXVhcLChUhPnwSzubbXPf50dnajqcmGtDTA4ZBh2bKb\nsW3bG7Ba27B+/UN45pn/h+3b34UgCNi48QlYrW2YMWM27rprLX772z8jJiam3+eHvNt2Wloa2tvb\nUV1djeTkZOzZswfPPffccLwVBdFAghkAiI2I9/25ttGGrm4XJhij+lw3d3IiZmTH43h5I74qtqBw\nWnKfa/adNONiuwMA8PgWk69Ld9qlGZ1qq80X0Hx88AIcTjdunp8BxaWZnKutJV9Zu4aIKJxcmXMY\nDEZjCrZs+XuvYxs2PNrrvwO5R8qPfvRkr9fr19+P9evvBwAkJCTiF7/4XZ97Nm58YiDDvqqAA5qT\nJ0/i2WefRU1NDRQKBXbu3ImlS5ciLS0Ny5cvx5NPPokf/tBTcGfFihWYMGFCUAZMw+Og5eiAghkA\nyJLP8f35fK0nC39CSt+ARhAE3Ls8Fz+uMuHVT85ienY8dJrLWeymEguef/+U73XPLt1pSd6Aph0A\nYO904pNDNdBrI3DdzMtJOQpBAafo7He83to1REQ0dgUc0EybNg1//7v/aG3evHnYunVroI+nELtq\nbRkRiFZFw3YuB0VmGQpTm5CfFYfz5ksBjVF6GjAhRoPV107E63vK8MaeMjy0YqrvXH9duh9/sABy\nmYDqek9As+dINTq6nLhz8USoIi7nWi1MnX/VQMw8iG3mREQUnoZUWI/Gjqv90M8Vl+CZa3+E791w\nM2SCgE3vFKOhpQPnzK1QyGW+JSIpywrSkJ6kwxfHzThT1ew73l+XboVcBmO8p+lll8OFXQcuQKOS\nY8ns3oX07spdDVVLNkQ3IPrZTR6vjpM+QUREYwYDmnFOFEXsqtwDEf5ryzjrMpCimAQAyE6Nxj3L\nc9He0Y0fbTahytIOmQAcOmP1e79CLsP9N0+GAODFnWfgdHlaFSTFaiSv97YtSEvSoavbhW2fn0Ob\nvRtL56RBq+47qdhamovOgzeju3ym5PMudl7EmaYymEoseHyLCY88uwePbzHBVMKZGyKisYIBzTgm\niiLeLt+Bd8o/QKRCK3nNjKi56K7KQ4T88j8VtdKz5NPt9AQmDqcbm94txudHqv2+V3ZKNK6fkwpz\nox0fmKoAAMlx0u/pbVsguj1B1kcHL0AAEB+llrw+JcHzHFeTEY6ymXDb9RDdAtx2PZx1GXCJbvz2\n6PN44egbsBp2QlnwIayGnXh+78cMaoiIxohRU4eGQsvlduGVM2/hK/NBGLRJ+JdZj6C8paLP9kCV\nLR0mHEOE4nJA4+2jdKU3PjmLxx8o8Puedy7KxuEzVrzzxXnsO2GGpdlThC9Or0KLzQFjfCRWFmZi\nQZ4BphILTKfqffeK8MzuaFSKPpWAVxZm+ZKJXU1GuJqMvb/WZgOUuYegSK7yHRO07VDmHMNbx1RY\nkLdmYH9pREQ0ajGgGYe6Xd34S/ErONZQjAx9Gh6d+TB0ykgUqHtvDzSVWPDip2cAAB8frEZCtAYL\n8gx+c18uWPqvT6BVKzBvahI+PljtC2YAoKmtCxtW5fcKVPpLGL4yoPG+9gY1VxLb4iE61BA0fcfd\npj+Fn792BHMmJ2H2pATE6FQSTyAiotGOAc040+HsxKbjf8XZi+eQG5uDDdPvh1rRdynHVGLpFSC0\n2By+1ykJnmTdK6Ubrl7l8XRls+TxKwOV/hKGpSzIM2BHUYXkuFITdWhUd/S9CYCgaUfxyWYUVzTj\npZ1nkJ0Wjbm5iZiTm4jEGOkcHyIiGn0Y0IxxBy1HsbNiN+rs9UjSJKDb3Y3GzmbMSpyGB/PXI0Im\n/U+gvxmSnks8PX3jhklXHc9AAxV/QZM3YViKv3GtLMzEuw3xuOhs6HMuLiIBP/ju13C41IpDpVac\nrb6IsuoWbN1dhgyDzhPcTE5CSrwW+0/V9+k8zkaYRESjAwOaMeyg5aivUi4A1Nk9OSm5Mdl4eNq9\nkAnSOeGmEotkMAF4Ag/vD/EdRZUwN9p8uS+LZqddtSz2QAOV/oITf/yNa0GeAXLLjb3+LryWTShE\nfLQay+elY/m8dLTaHDha1oBDZ6woqWhClaUdb39xHtGRSrTYHL77ehYBZFBDRDTyGNCMYf6K5dm6\n7X6DmZc/KsUnh/zvVvIGHgvyDAH9IB9ooNJfcNIff+Py5gZ5k571ETq0OFrxZe1+LDAWQHNp2S0q\nUolFM1OwaGYK7J1OHC9vwKFSKw772ZYuldNDREShx4BmDPPOyFzJbJfeqmwqsfQbzAD9z5AMxGAC\nlUCDJn+u7Ini7V31wsmX8Z0ZD0Iu693tXatW4Jr8ZFyTn4xHnt0tWbjPX04PERGFFgOaMcygTYLZ\nVtfnuDFSOkjwlzfjdeVOpEAFO1AJ1J05t8Ha0YiSxjN48+x7WDt5td9rUxIiB53TQ0REocPCemOU\nKIrQRUgXrrsxc4nkcX8JuwCQlqgbFUFIMMllcnwz/x6kRCbj85p9+PTCl36vXVmY5ef40GasiIgo\nOBjQjFEfV32GsxfPIV4dC2NkMmSCDKk6Ix7KX++387S34q6UsfqDW6NQ47szH4JeqcObZ9/FyYZT\nktctyDNgw6p8pCXqIJMJkAmAKkKGmTnxIR4xERFJ4ZLTGHTMehLvlH+AGFU0/m3uPyFGFT2g+/wl\n7N4wN23Mzc70FKeOxXdmPIhfHf4TXih+GT+c+yhSdcY+1/VcKntn73m8s/c8Pj1Si5sXZIR6yERE\ndAXO0IwxVW3V+Gvxq4iQKfCdGQ8OOJgBPD+wp0/0zDjIBAFpiTpsWJWPe5bnDtdwR42sqAzcn7cO\nXS4H/njsL2jpau33+hvmpkGllGPn/ip0O10hGiUREfnDgGYMudjVgj8d+yu63U48mL8e6frUQT8j\nVu8p/f+TR+bjfx6eP6ZnZq40J2kGVk28Gc1dF/Gn43+Fw+Xwe61OE4Gls1PRYnNg74m+iddERBRa\nDGjGiC6XA386/le0OFqxOmcFZibmB/ScTocTAKBWjs/VyBszl+Ca5AJUtVXjbyVb4Rbd/q+dlw6F\nXIYPvqqE0+X/OiIiGn7j86fWGOMW3fhb8au40FaDrxnn44b0RQE/q9PhWT5RK+VXuXJsEgQBd0+5\nA42dTThqPYH3zu3E7dm3SF4brVNh0Uwjdh+uwf5TFnxtWt+8m4EylVjYVoGIaAg4QzMGvFv+IY41\nFCM3JhtrJ6+GIAgBP8sb0KjGaUADAAqZAo9Mvw9JmgTsqtyDotoDfq+9eUEG5DIBO4oq4ZaqvDcA\n3kag1VYb3KLoa6tgKpEugEhERH0xoAlzRbUH8FHVp0jSJuBb0++Dwk+zyYHqdDihUsohG0JQNBbo\nIiLx3ZkPQavQ4JUzb6G0uVzyuoRoDa7JN8DcaMeRUun2CFfTXyNQIiIaGAY0Yay0uRyvnHkLkQot\nvjvjIWj9FNIbjE6Ha9wuN10pSZuIb0+/HwIEbD7xIix26YBlxTWZEAC8v68SYgCzNAPtQE5ERP4x\nhyZM1dut2HziRQgQ8K3p9yFJmxiU53Y6XNCo+M/Ca1JsNu6ecideOvU6fnnoj4iM0KK+owHJ2iTc\nlLUUBYZZMMZHYu6UJBw8XY/i802YNnFwxfYG2oGciIj84wxNGLJ12/HHY3+B3dmBuyffgUmx2UF5\nrqnEgjabA5YmOx7fYmIOxyWFxgJMT8hDW3c76uz1cItu1Nrq8JfiV3DQchQAsPIaTyXl9/dVDPr5\nbKtARDR0DGjCjNPtxPMn/o76jgYsz7gehSnzgvJcb2Kqd8GEiam9NXY0SR7fVbkHAJCZrMeM7HiU\nVreg9MLFQT173pQkaFRyeLOWNCp50BqBEhGNFwxowogoith65m2UXizHzMRpWJV9c9CezcTU/tXZ\n6yWPm22XAz7vjMr7RRWDenZ5bQs6uly4doYRUZFKqJUKBjNERIPEgCaMfFz1GfaZDyBdn4oH8tZB\nJgTv46ttkE5AZWKqR7I2SfK4MfJy4DEpLQaT02Nw8lwTKur6b53Q0+FLu6PmTk5EdkoUmtu60NTa\nObQBExGNMwxowkTPhpPfmfEgVHJl0J5d32z3W7uGiakeN2UtlTy+LH1xr9e3fi0LwMBntkRRxKEz\nVqiVckzNjENOqqf3VllNS+CDJSIahxjQhIGhNJy8moaWDvzs1SNwuaW3GzMx1aPAMAsP5a9Hqs4I\nmSCDRqEGADR0Nva6Li8rFlnJehw+Y/U769XThfp2NLR0YkZ2PCIUMmQzoCEiCgj3545yPRtOfmv6\n/QE1nPSnqbUTP33lCBpbu3Dn4olIiNZgR1ElzI02GOMjsbIwk7kcPRQYZqHAMAsAYO/uwDP7f4kP\nKj7B1LjJmBCdAcDTOuHWr2Xhd9tO4B9fVeKRW/P6faZ3uWlOrmfbfVayHnKZgPKagS9ZERERA5pR\nrWfDya/nrAy44aSU5rYu/OzVI2ho6cTt107wbR1mADMw2ggN7s+7C785shkvlryG/5r/r75lwFmT\nEpCaEImvii24/doJSIzR+H3OoVIrFHIZpl+qXaOMkCPDoEOVpQ3dThciFCxySEQ0EFxyGqWC2XDy\nSi02B5577QgszR1YWZiJVQuzgvbs8SQ3NgdL069DfUcDtp19z3dcJghYUZgJtyjiQ1OV3/stTXbU\nWG3Iz4rtVcwwOyUaLreIirq2YR0/EdFYwhmaUeSg5Sh2VuxGnb0eWoUG7d22oDSc7KnV7sBzrx6B\nudGOm+dn4I5FE4P27PHotuybcaqpFHtrTZiWMBXTEzxLTPOnJmH7F+fwxXEzbluYhRidqtd9phIL\nXvvkLACgqr4dphKLb3YsJy0aHx+qRllNCyalxYT2CyIiClMMaEaJg5aj+EvxK77X7d2ehNJ5hllD\najhpKrFgR1EFahvsSI7ToNPhQlNbF5bNTcM3lmQzmBmiCJkCD+bfjZ8e+A1ePvUmfrTg36BX6iCX\nyXDLNZl48cMz2LX/Au5amuO7x1vE0Ku5rcv3ekGeAdkpnsRg5tEQEQ0cl5xGiZ0VuyWPf1qzL+Bn\nen9wVlttcIsiahvtaGrrQl5WLO5eNonBTJCk6oxYlX0L2rrb8fLpN30NKhdOMyJGp8SeIzVo7+j2\nXf/el+cln+Pd6h0XpUKMTonympaAml0SEY1HnKEZJQZSiXaw/FX/bbU5GMwE2ZL0a3Gy8TRONJRg\nn3k/FqYsQIRChpvnZ+C13WX40eavYOtwIkavRFNrl+QzvEUMBUFAdmo0Dp2xorGlEwn9JBUTEZEH\nZ2hG2EHLUTxt+gXcolvyfM9KtINV22CXPG5ulD5OgZMJMtw/9S5oFBq8efY91NsbAABadQQAoM3e\nDbco+g1mgN5FDFlgj4hocBjQjCBv3kytrc7vNTdmLgno2W5RhF4bIXmO1X+HR6w6ButyV8PhcuBv\nJa/B5XZh1wH/u5yu1LOIobfAHvNoiIgGhktOI8hf3gzgycu4MXOJr5DbYLTZHdiy4xRabA7J86z+\nO3wKkmfjROMpz461yt2obZD+FhMEIDVB57eIYaZBD4VcQFktZ2iIiAaCAc0I8pc3IxNk2Dj/BwE9\n80xVMza9W4yL7Q7kZ8VizuQk7Dlcw+q/IbQ2dzXKLp7HBxWfIDFlMSw1qj7XpCbo8D8Pz/f7jAiF\nDJkGPc6b29DlcEGlZIE9IqL+MKAZQUmaBMmgJpC8GbdbxPtFFXhn73kIEHDn4om45ZpMyAQBS2YH\nr10CXZ02QosH8tbiN0c2Q8w4CpjnAu7e32oDmSXLTo1GeW0rKupaMTkjtte5ntvxUxK0WFmYxUCV\niMY15tCMELfohkyQ/usfbN7MxfYu/HzrUWz/4jxi9Sr85z2zsbIwCzLuZBox3irCba5mzLzOirRE\nHeQyAWmJOmxYlT+g4MNfYvCV2/GrrTZsercYppLAd8QREYU7ztCEmLcasNlmgQgRcapYqBUq1Nnr\nYYw0DDpv5uT5Rjz/Xgla7d2YlZOAb66cCp1GOhmYQstbRbjUdgxxU6ug7mqBSpsEebwSwNUDGn+J\nwf624+8oqsSti3MkzxERjXUMaELoymrAANDU1QzdhfnoqJyNzgQtXDrjQH7WweV2Y/sX57GjqBJy\nmYC7b5iEZQVprC8zikTIFJiXPAfvlP8DTZ3NAIBaW53v38DVAtdYvQrxUSqU13oK7Hk/W//b8W1B\nHD0RUXjhklMI+dvV1Ko7Nailg8aWTjz78hHsKKpEYowaG++bi+Xz0hnMjEIH6g5LHt9VuWdA92en\nRqPN3o36ix2+YykJWslruR2fiMYzBjQhVGeT3tUkqNt7vfaWwJdy5KwVT/5lP8pqWjBvShKeeHA+\nJhijgjpOCp6hVoC+3Nfpch7NysIsyWsL85kUTETjFwOaEBFFESqFUvpcp67Xa6mlA6fLjVc/Povf\nvnUCDqcb9988Gd+5PR9aNVcNR7NkbZLk8YHuZJPKo1mQZ8DiWSkAPPVs4qPUAIDPj9XC1qNnFBHR\neMKfhsPsyiRgKc7aib1eX7l0UN9sx5/eKUZFXRuM8Vp89/ZpSEvqHQTR6HRT1tI+eVPAwHeyZRh0\niFDIes3QAEBHlxMA8JOHFyAlIRJv7CnDB6Yq/PLVw/jWrVO5w42Ixh0GNMNIKgkYAGJVMWhxtEIv\ni4P1TCpcTcZe53vWKNl/yoK/fnAanQ4XFk5Pxr3LJ7PIWhjxJv7uqtwDs80CuSBHt7sbStnAdqIp\n5DJkJetRVtOCji4nNCoF3KKIkopmxOpVMMZ78mnuWDwR582tMBXXIe1SXRoiovGEAc0wOWg5ir+X\nvC55ThuhwVMLN+L9fRXY1nQO0ZFKX/NChVzA5vdK8P6+CkRFKnGqshmqCDkeuXUqvjbNKPk8Gt0K\nDLN8gY3ZZsH/7f8VXizZimhVFOo7GpCsTcJNWUv97nrKTo3G2eoWVJhbMTUrDlWWNrR3dOPa6UZf\nIrhcJsN3bp+Gp148iG2fn0NWchTyJ8SF7GskIhppzKEZBt6ZGafolDxvtlnQ1e3CRwcvQKtS4Jlv\nX4Nv3ZYHAHC6RLhFETUNNpyqbEacXoXHHyxgMDNGGCMNmJE4DR2uTtTZ6+EW3b6t3ActRyXv8SYG\nl9V68miKzzcBQJ+AJSpSif96YB7kMgGb3i1GY0vnMH4lRESjCwOaIOtvZsbLGGnA3uNmtNm7sXRu\nKjQqhd9iaRq1gttxx5g6PztkvupnAAAgAElEQVSc/G3lzkn17GLz5tEUn2+CACAvK7bPtZMz47B+\nWS7aO7rx+7dPoNvpCs6giYhGOS45BZG/nJkrXSxPx8uVpQCAOL1nh4q/Yml1jdLHKXwNdit3tE6F\nhGg1ymta0Olw4mx1CzKS9dBrpXfNLZ6VgvLaFnx5og6/fP042jsc7PlERGMeZ2iCyF/hPC855HCU\nzURD5eWlghd3noGpxMJiaeNIIFu5c1KjYet04vNjZrjcIqb1kx8jCALuu3Ey4qNVOF3VzJ5PRDQu\nMKAJIn+/eXtp6uf22dEEeArp+duVMpCuzBRebspaKnm8v63c3no0H3zlKbqYl9V/wq8yQo4IufRu\nuP4KNxIRhSsuOQWRQZsouWygkClw39S78KcDzYBELRpzo823DLCjqBLmRhuM8ZFYWZjJ5YExqNdW\n7nYL3HAjWqnHzIR8v/d0Xqo702JzQADQ3Hb1hN/65g7J4+z5RERjEQOaIBFFEZER0stD9029CwWG\nWUhJMKHa2veHiXdZaUGegQHMONFzK/frpe/gs+ov8eN9z8Du7OizjdtUYsFbn5/z3SsCeP79U5DL\nZP3+e0lJ0Pb7742IaCzhktMQHbQcxdOmX+Bf9vwXyi6egy4iEimRyZAJMqTqjHgof73vBxOXlUhK\nus7TxqC92ya5jdvfDrirLR3x3xsRjSecoRkCqV1N7d02fCP3dskiaQvyDPjkUDXKalogkwlI4bIS\nAdh94QvJ47sq96DAMMvvDrirLR31XMasaWiHKALzpiTx3xsRjUkMaIbA364m7w8iKY5uF5QKGf7w\nw8Xst0MArr6NeyhLR95lTHunE//95yIcL29Ec1sXYvWqoQ2aiGiU4ZJTAA5ajuLHXz6NWlud5Hmz\nzQJTiQWPbzHhkWf34PEtJphKLHCLIuqa7EiO0zKYIZ+rbeMOxtKRVq3AHYsmoqvbhW2flw96jERE\nox0DmkHyLjM1d7X4vUbs0GHTu8V96n/sPlQNh9ON5HjpmjM0Pvnbxr0843oAnlmWDavykZaog1wm\nIC1Rhw2r8ge9dHTdjBSkJ+nw5Yk6nDe3DnXYRESjCpecBulqxfMAoKt6guTxXQcuAABSuMuEeriy\nI3eELAJdri64RbfvmmDsgJPJBKy7YRJ+9uoRvPrJWfz3PXN8zS2JiMIdA5pB6rd4ngg4ymdKFs8D\ngMZWT+0QztDQlXpu427saMJTpp/jrbL3kBc/GXqlLmjvMzUzFnNyE3G41IoDp+sxfyoThIlobOCS\n0yDFqKL8nnN36P0GMwCgVXniR87QUH/iNXG4Lftm2LrteOvse0F//l1Lc6CQC3hjTxkc3WxeSURj\nAwOaQTjXUoGWLv+5B1HtU/u9Xx+phCAAhjhNsIdGY8z1aQuRqU/HAcsRFDeeDuqzk2I0WD4vHY2t\nXdi5vyqozyYiGikMaAbIYqvHn479FSI8yZqxqhjfuVhVDB7KX487Zlwnea9WpcCGVfmwdXQjMVqD\nCIV0jx0iL5kgw/opd0ImyPDambfR6ewK6vNvLcxCVKQSO76qRHNbcJ9NRDQSmEMzAC1dbfj9sS2w\nOe24d8o3UJgyD6tzVvS90OBpgbD5/RKIIpAYo4b1YieWFaQhf0Ic2uzdmGD0v2RF1FOaPgXLMhZj\nV+UevH9+J9ZMWhW0Z2tUnm3cf/3gNN78tBzfui0vaM8mIhoJAc/QPPPMM1i7di3WrVuH48eP9zq3\ndOlSrF+/Hvfddx/uu+8+WCx9GzaGi05nJ/54/AU0djZj5YTlKEyZ1+/1U7PiIIrAnNxEPPr16QAA\ne6cTdY2eaq9GJgTTINyStQxJmgR8euFLVLZeCOqzr51uREaSDkXFdThXy23cRBTeApqh2b9/Pyor\nK7F161aUl5dj48aN2Lp1a69rNm/ejMjI8E5+dbldeP7kS7jQVoOFKfNxS9ayq95jafIELoY4jS8J\nuKPLidpLZerZGJAGQymPwN1T7sSvj2zCLw7/EW63G8mRvZtXBkomE3D3skl49pUjePWTUmy8dy63\ncRNR2ApohqaoqAjLlnl+uGdnZ6OlpQXt7e1BHdhIE0URL59+E6eaSjEtfgrW5n59QP+zr7sU0CTH\naqFRewIaexdnaChwrY42AIDT7YQbfZtXDsXkjFjMnZyI8ppWmE6F70wqEVFAAU1DQwNiY2N9r+Pi\n4mC1Wntd88QTT+Duu+/Gc889B1EUhzbKEfD++V0w1R1Cpj4d35x2L+SygSXyXp6h0UKjvBTQdHKG\nhgLXX8+wYLhriXcbdzm6uI2biMJUUJKCrwxYvve97+G6665DdHQ0Hn30UezcuRM333xzv8+IjdVC\nMcTdP4mJ+iHd7/VR2Rf4sOITGHSJ+PHSf0a0euCJvM02BwAgf1ISYvQqaNUKOFxutF7sRrROiQkZ\ncUEZ42gVrM+ALvNXzLHOZpH8+x7sZ5CYqMfqxTl4c/dZfHHSgrtvnBzQOOkyfh+MPH4GIy/Un0FA\nAU1SUhIaGhp8r+vr65GYmOh7vXr1at+fFy1ahNLS0qsGNM3N9kCG4pOYqIfV2hbQvQctR7GzYjfq\n7PWIUUWjqbMZuohIfGfaQ3C0CbC2Dfy5VXVt0KjkcHR0wdrpgFopR2NLJ9rsDkxKiwl4jOFgKJ8B\n+ZesTZJshJocaejz9x3oZ7BkphEfmSrx5u5SzMmOQ1yUOuDxjnf8Phh5/AxG3nB9Bv0FSQEtOS1c\nuBA7d+4EABQXFyMpKQk6nac8e1tbGx5++GE4HJ6ZigMHDmDSpEmBvE1IeJtN1trq4BbdaOpsBuAp\nbJakTRjUs9xuEfXNHTDEan35NlqVAq02B0SR+TMUGH/NK69LvSZo76FRKXDH4olwdLvx5mfsxk1E\n4SegGZo5c+YgPz8f69atgyAIeOKJJ7Bt2zbo9XosX74cixYtwtq1a6FSqZCXl3fV2ZmRtL3sH5LH\nv6zdj1smXH1XU09NrZ1wutxIjrscuHh3OgHMn6HAXNm8Uh+hQ4ujFe+UfYDXS99BsjY4u54WTjdi\n96EafFVswQ1z0pCdGh2M4RMRhUTAOTSPPfZYr9dTpkzx/fmBBx7AAw88EPioQqi56+Kgjvenrvly\nQrCXVh3h+zNnaChQPZtXHqg7gr+WvIoOl6fZqXfXEwDckihdrXogZIJnG/f/vXwYr35yFhvvmwsZ\nt3ETUZhg64MgsjR1AOjdq0mjupzozICGgsHf7qZg7HrKTY9BwZQknKtthamE27iJKHyM64Cm3m6F\nAOnfQHv2ahooXw2aXktOnhkapULGREsKCn+7nsy24AQgd12fDYVchjc/LUeXg9u4iSg8jNuAxmKr\nx68Ob4II6Ro5kr2arvZMbw2a2MsBTVO7Z1nA4XTjyRf287deGrJkbZLkcWOkISjPT4jR4Kb56Whu\n68IHpsqgPJOIaLiNy4CmzlaPXx3ZhBZHK+7MuRUP5a9Hqs4ImSBDqs6Ih/LXB5RgaWm2IypSCc2l\nRGBTiQVHSi9vb6+22rDp3WIGNTQk/nY9LUtfHLT3WFmYiehIJT40VaGptTNozyUiGi7jrtu22WbB\nr49sQpujHWsmrcKS9GsBYMg7RLqdbjS0dGJSj50hO4oqJK/dUVSJBXnB+W2axp8rdz2p5SrYnR0B\nJbL7o1YqcOfibLzwj1N449NybFiVH7RnExENh3EV0NS21+HXRzahvduGu3JXY3Ha14L2bOvFDohi\n7x1OtQ3SxQLNl9ogEAWq564ne7cd/2N6Dv+o+Bg3TCmEApqr3D0wX5uejN2Hq2Eq8WzjzknjNm4i\nGr3GfEDjrQJstlkgAHBDxLrJdwS1KBlwOX+mZ0JwSoIW1da+wQvr0VAwaSO0uCt3NbacfAl/Pvgy\nvpv/cFC6Znu3cf/vS4fxs9eOwOUSkZKgxcrCLM4wEtGoM6ZzaHpWARYhwn0pAVijCP5uI6kaNCsL\nsySvXVmYGfT3p/FtduJ0TE+YiuL6UhSZDwbtuU2tXQA8S6puUWQeGBGNWmM6oBnuLsU99eyy7bUg\nz4ANq/KRlqiDXCYgLVGHDavy+dstBZ0gCFib+3WoFSq8XfY+Wh3B6aHSXx4YEdFoMqaXnIa7Xkev\n92rqgAAgKaZ3/sKCPAMDGAqJWHUM1s9YjRcOb8Wbpe/im9PuGfIzmQdGROFiTM/QDHe9jp4szXbE\nR6sRoRjTf6U0yt2YvQgTojJxqP4YTjSUDPl5KQnS1a2ZB0ZEo82Y/unrr17HjZlLgvo+HV1OtLQ7\nei03EY0EmUyG9VPuhFyQ47Uzb6PTObQaMswDI6JwMaYDmgLDrKAVzetPfbOnh1NyLAMaGnkpumTc\nmHk9Lna14N1zO4f0rJ55YN6NU0vnpnIZlYhGnTGdQwP0rtcxXOp8CcHBqf9BNFQ3ZS7F4frj+Kz6\nSxQ3nkZTZzOStUm4KWvpoL8fvHlg1fXtePyF/Wi1dQ/TqImIAjemZ2hCRaoGDdFIipBHYG7STABA\nQ0cj3KIbtbY6/KX4FRy0HA3omamJkUiK1eBEeSO6nWxaSUSjCwOaIJCqQUM00o5aT0oeD7RsgSAI\nmJObiK5uF4rPNw9laEREQceAJggsTXYo5ALio4JfsI8oUMNRtmBObiIA4HCpNeBnEBENBwY0QySK\nIixNHUiM0UAmG3q5eaJgGY6yBRNTohCtU+JoWQNcbnfAzyEiCjYGNEP02bFa2LucMDfa8fgWE0vC\n06gxHGULZIKAOZMS0d7RjbMXWgJ+DhFRsDGgGQJTiQUvfnjG95p9bmg0ubJsgQABckGOCVFDqyHD\nZSciGo3G/Lbt4dRfnxvW6aDRoGfZApP5EF48tRWvnnkLj84MvCP35IwYKBUy7DlSg92Ha9iBm4hG\nBc7QDAH73FA4mZ88B1PjcnGqqRT76w4H/JxDZ6xwON1wuUV24CaiUYMBzRAY49nnhsKHIAi4e/Kd\nUMqVeOvsewF35GYHbiIajRjQDMHMnHjJ4+xzQ6NVvCYWt0+8BTanHW+UvhPQMzgzSUSjEQOaAJlK\nLPj8qBkAIJcJkAlAWqIOG1blM5eARrVFaYWYGJ2Jw/XHccxaPOj72YGbiEYjBjQBePmjUmx6txjt\nnZ6eNp5cAs/MDIMZGu1kggz3TFkDhSDH1jNvw97dMaj72YGbiEYjBjSDZCqx4JND1ZLnmENA4SI5\n0oCbs5ahxdGK7eU7BnWvtwO3UuH530daYiRnJoloxDGgGSR/CZEAcwgovCzPXIyUyGR8Wbsfpc1l\ng7p3QZ4BM7I9OWQ/XDuLwQwRjTgGNINU2+A/aGEOAYUThUyBe6d+AwIEvHz6LThcjkHdH3epd1lj\naxdMJRY8vsWER57dw4rZRDQiGNAMQo21vd9iZMwhoHCTGZWOpRnXoaGjEe+f3zWoe+P0KgCAqaQO\nm94tRrXVxro0RDRiGNAM0JmqZvzvS4fhcouS52+Ym8ZpdwpLt064EQmaeOyu+gKVrRcGfJ93hmb/\naemu3swpI6JQYusDeBJ9dxRVoLbBLlnG/cDpemx+rxiiCHzrtjzIBAE7iiphbrTBGB/J3U0U1pRy\nJe6Zcid+feTPeOnUG/jPed+DQnb1/zXERnlmaFrbpZeqmFNGRKE07gMaU4kFm969XIvDO10OeBIf\nPzpwAa99chYqpRyP3jEd+VlxvnNEY0VubA4WpszHl7X78VHlZ7hlwg1XvSdO75mhUavk6Ohy9TnP\nnDIiCqVxv+Tkv4x7BV7fXYZXPzmLqEgl/uueOb5ghmgsWp29EtFKPT6s+Bh1tqvnv0TrlJDLBOi1\nSsnzzCkjolAa9wGNvzLuNVYbPtxfBWO8Fj+6by4yDPoQj4wotLQRGqydfAecogsvn34TbtHd7/Uy\nQUCsXoVupxsbVuVD1iNfvjDfwFlMIgqpcR/QJESrJY+LAHLSovHf985FQowmtIMiGiEzE/MxJ2kG\nzrVU4vOaoqteH6dX4WJ7F2bmxMMtAikJnmWmFtvgtoATEQ3VuA5o2uwOtNi6JM8lRKvx2NpZ0Gki\nQjwqopH1jdzboVVo8E75B2jsaO732rgoNUQROG/2dO6ekKxHhkGHM1UX0elwhmK4REQAxnFA43aL\n+PN7Jejqlp5WVyvlUEbIQzwqopEXpdRjzaRVcLgcePXMWxBF6VIFwOWdTuU1Lb7XM7Lj4XKLOFXR\nfzBERBRM4zageW9fBYrPN/k9b26Uzq0hGg/mJ8/B1LhcnGoqxf66w36v8+50KrsU0MTp1ZgxMQEA\ncPxc4/APlIjoknEZ0Jw814h3955HfJQaKfFayWu45ZTGM0EQcPfkO6CUK/HW2ffQ6miTvC7uyhka\nvQoTU6IQqVbgeHljv7M7RETBNO4CmqbWTvz5vRLI5QL+6evTcNvCCZLXccspjXfxmjjcPvEW2Jx2\n/L+in+Ff9vwXnjb9AgctR33XeGdobJ2efJlYvQoymYBpE+PR3NaFGiuL6xFRaIyrwnpOlxt/2H4S\n7R3duO/GXEwwRmGCMQoAWPmXSEJkhGcGs9PVCQCotdXhL8WvAAAKDLN8MzRe3nYIMybGw1RiwfFz\njUhL0oVwxEQ0Xo2rgOb13WU4V9uKa/INuH52qu/4gjzWzCCSsqtyj9/jBQbPLsAIhQzdTjeUChki\n1Z7/peRPjIMA4Hh5I1Zcw9lOIhp+Yz6g8fZpqmmwQRQ9U+IP3DSl367ZRORRZ5duPGm+VElYEATE\n6VWwNHcgVq/yfV9FaZWYkBKFsuoW2Du7oVWz/AERDa8xnUPj7dNUbfUEMwDQ3NaFo2UNIzswojCR\nrE2SPG6MvDyj6V1m8v7Xa8bEeLhFEcXcvk1EITCmAxr/fZoqQzoOonB1U9ZSyeNL0q71/dnl8tRy\nOlXZjMe3mGAq8czeTM+OBwAcL+cvEEQ0/Mb0kpO/Pk3mRu68IBqIAsMsAJ6cGbPNAq1Cg/ZuGy60\n16IQnlnQ0uoW3/U9u9XPm5qEKG0ETpxrglsUIeMyLxENozEd0KQkaFEtsW2UNWaIBq7AMMsX2HS7\nuvG/B36Nz6q/RHHDKTR0NEM1LRLO2my4moy+e3YUVWJBngHTJ8bjy5N1qLK0ISs5aqS+BCIaB8b0\nktPKwiw/x7nrgigQEfIIFBhmAgAaOpsAQYRM2w5lzjHI48y+67yzoJeXnVg1mIiG15gOaBbkGbBh\nVT7SEnWQywSkJeqwYVU+t2gTDcGR+hOSxxUp53x/9s6C5k+Ig0wQcIIBDRENszG95ASwxgxRsPnb\nyi2o231/9s6CRqojkJMahbPVLWizO6DXKkMyRiIaf8b0DA0RBZ+/rdwQROhmFeHG5fJev0RMz46H\nCOBkP81giYiGigENEQ2Kv63cggC4lC34omVHr35PM7I93be57EREw4kBDRENSoFhFh7KX49UndHv\nNT1bJqQlRiJWr8KJc41wu9l9m4iGBwMaIhq0AsMsbJz/A8gE6f+F1LbX+f4sCAKmT4yHrdOJc+bW\nUA2RiMYZBjREFDB/+TQixCuWnbh9m4iGFwMaIgqYv3waoPey09TMWMhl3L5NRMOHAQ0RBazAMAsC\npFsaeDtyA4BGpUBuegwqLW242N4VquER0TjCgIaIhqRn5+3+jnuXnU6c4ywNEQUfAxoiGhJ/y07X\nJBf0eu0LaLjsRETDYMxXCiai4XVlR+5oZRSauy7ieEMxrk9f6NsJlRynhV4TgUOlVjzy7G6kJERi\nZWFWryJ8phILdhRVoLbBjpQEbZ/zRET+MKAhoiHr2ZFbFEVsPvl3HLOexN4aExalFQIA9p+qR1tH\nt+caANVWGza9WwzA06LEVGLxvYbEeSKi/nDJiYiCShAErM1dDa1Cg+3lO9DY0QwA2FFUIXn9jqLK\nAZ0nIuoPAxoiCrpoVRTWTFqFLpcDr5x+E6IoorbBLnmtudEGAFc9T0TUHwY0RDQs5ifPQV78ZJxu\nPosi80GkJGglrzPGRwIAonXSnbi954mI+sOAhoiGhSAIWD/5TqjlKmwrew9L5sdLXjc5IwYHT9ej\nuU26Ps3KwszhHCYRjREMaIho2MSqY/D1nJXocHbitPsLLJ2b2ueaTw5V4w/bT0KllGP1dROQFKsB\nAOg0EdiwKp8JwUQ0IAxoiGhYLUxZgNzYHJxsPIUTjcf9XndjQTpWLZyAJx+aBwBIT9IxmCGiAQs4\noHnmmWewdu1arFu3DseP9/6f1L59+7BmzRqsXbsWv//974c8SCIKX4Ig4J4pa6CURaAt/jBU07+A\net5OqKbthTzO7LvuyNkGAIBaqUBCtBo1DUwGJqKBC6gOzf79+1FZWYmtW7eivLwcGzduxNatW33n\nn3rqKWzZsgUGgwH33nsvbrrpJuTk5ARt0EQUXhI0cZidNAOmukMQNJ5ARdC2Q5lzDI4ywNVk7LWb\nKTUhEsfKG9FqdyBKK50sTESjg1RBzFsX60M+joBmaIqKirBs2TIAQHZ2NlpaWtDe3g4AuHDhAqKj\no2E0GiGTybB48WIUFRUFb8REFJYutNVIHleknwHQezdTaqIOAFBr5SwN0WjmLYhZbbXBLYq+gpif\nH6kO+VgCmqFpaGhAfn6+73VcXBysVit0Oh2sVivi4uJ6nbtw4cJVnxkbq4VCIQ9kOD6JiaGPCKk3\nfgYjb7R+BnX2esnjMlUn5HFm3H3Trb6xT5kYj398VYnWTueo/Xr6E45jHmv4GYTGzgMHJY+/8clZ\n/PaxJSEdS1BaH4iiOORnNDdLF9UaqMREPazWtiGPgwLHz2DkjebPIFmbhFpbneS52NxzmJoW7Rt7\nlMrzy83piibMn5wYsjEGw2j+DMYLfgahU1Un/fd8wdI2LJ9Bf4FqQEtOSUlJaGho8L2ur69HYmKi\n5DmLxYKkpKRA3oaIxhB/XbkBwOZuw0HLUd9rY7wWggDUWNtDMTQiCpC/gpnphjDJoVm4cCF27twJ\nACguLkZSUhJ0Os+ad1paGtrb21FdXQ2n04k9e/Zg4cKFwRsxEYWlAsMsxKpi/J7fVbnH9+cIhRyG\nWC1qG2xBmQEmouGxsjBL8vg3bpgU2oEgwCWnOXPmID8/H+vWrYMgCHjiiSewbds26PV6LF++HE8+\n+SR++MMfAgBWrFiBCRMmBHXQRBSeVueswF+KX5E8Z7ZZer1OTYjEoVI7LrY7EKtXhWJ4RDRIC/IM\neO/L86ht9KSNxOiUWLt0EhbNTgv5sl/AOTSPPfZYr9dTpkzx/XnevHm9tnETEQGeWZrtZf9Ac9fF\nPueMkb2L6KUmRuJQqRU1De0MaIhGKVEUcbHdgQiFDN1ON2ZkJ4xYQUxWCiaikFqds0Ly+NL063q9\n9m7druHWbaJRq6GlE/YuJ6ZPjIdMEFDbOHLfrwxoiCikCgyz8FD+eqTqjJAJMugiPPVnatrNva5r\nbOkEAGzdXYbHt5hgKrH0eRYRjawqi2dZKUIuQCYDyqpb8PgWU/jUoSEiGooCwywUGGYBAByubvzv\n/l9iz4W9mJM0AxOiM2EqseD1PWW+673FugCwvxPRKFJp8exENJ26XGeq2mrDz146FPLmspyhIaIR\npZRHYP2UNRAh4qVTb6Db7cSOogrJa3cUVYZ0bETUP+8MjZRQf78yoCGiETcpdiIWpRaizl6PH3/5\nDBqytvVpXgmgV78nIhp5/QU0of5+ZUBDRKNCuj4NANDe3Q5BECG71LyyZ1DTs98TEY2sVpsDF9sd\nUCul2xaF+vuVAQ0RjQp7LnwheVyRcs7355WFmaEaDhFdhXd2Ji8rVvJ8qL9fmRRMRKOCv+aVgrod\nAgCNSo5ZOQmhHRQR+VV5KaApzDdi3hQDdhRVwtxogzE+EnffNBlT06JDOh4GNEQ0KvhrXpkWlYwp\nX8vC+/sqsHN/FVZdy8rjRKNB1aUdTpkGHRJiNL12NI1Eg1AuORHRqOCveWVtex1KlNuhM1rxD1Ml\nmtu6QjwyIpJSZWlDpFqB+Gj1SA8FAAMaIholehbcEyD4josQYbbXwZV+CC59Dd7+4lw/TyGi4WYq\nseDHz5tgae6A0yVi/ynp5eJQY0BDRKNGgWEWNs7/QZ++Tl6a9Ap8edzc71ZRIho+phILNr1bjNoG\nz5bsrm4XNr1bPCoqeTOgIaJRx1+CsKhqgwjg9T1lEEUxtIMiolFd9JJJwUQ06vhLEDbqDFBPiMPJ\n8004ca4RM7K564kolGob7JLHLe6zeNq0F3X2eiRrk/CNGSuQq5kS0rFxhoaIRh1/CcKLUr+Gu5bm\nQBA8TStdbneIR0Y0vqUkaPsci8gogSL7GGptdXCLbtTa6vDrohdw0HI0pGNjQENEo86VHbmjlHoA\nwLmWCqQl6nDdjBSYG+34/Jj5Kk8iomCaNSmx12t5nBmK5CrJa3dV7gnFkHy45EREo1LPjtwutws/\nO/Q7mOoOYV7ybHz9ugkwnbJg+xfncE2eARoV/1dGNNycLjcOl1oBAEmxGjS2dEKTUQGXn+vNttAm\nCnOGhohGPblMjnumrIFMkOHV029BrRGwYkEG2uzd+MdXI5+MSDQefHKoGrUNNiyelYL/21CIzf+x\nBKLK/45Df7sVhwt/rSGisJCuT8UN6YvwUdWneP/cTtw6fwV2HbiAHUWV+OCrSqQkRGJlYVavaqVE\nFBwX27vwzt7ziFQrkJ1vw9OmX6DOXt+rZtSVbsxcEsIRMqAhojCyYsJyHLWewJ4Le6G2Z8DW6QQA\nuEWg2mrDpneLAYBBDVGQvb6nDJ0OF65fArx6dutVr7950vW+JeNQ4ZITEYUNpTwC66esgQgRO83v\nAULfXU6joR4G0VhypqoZXxVbkJmsxwVI71xSyBSQCTKk6ox4KH89vjlnbYhHyRkaIgozubHZWJgy\nH1/W7oci+Tyc5uxe582NthEaGdHY43K78fJHpQCAe2/Mxa/OvCV5nVt047dL/i+UQ+uDMzREFHZW\nZ6+E4FRDkVoGQd3e65wxvm+dDCIKzO5DNai22rBophHZKdFI1iZJXhfqBGApDGiIKOxoIzRYnHAj\nBJkIVX4R1PN2QjVtL+IaLEsAACAASURBVORxZkRFKkd6eERjQkt7F7bvPYdItQJ3LvbMhMaqoyWv\nDXUCsBQGNEQUliak6gAAgtwFQRAh07ZDmXMMZ1pLUFTct20CEQ3O63vK0dHlwh2LJkKvVcJkPoTi\nxjOIVkbBGGnolTMT6gRgKcyhIaKwtLNit+RxZeo5/O3DVKQn6ZCWqAvxqIjGhtILF1FUXIdMgx6L\nZ6WisvUCXjnzFjQKNb4/ZwMM2sSrPyTEOENDRGHJX0duQWODo9uN3799Eh1dzhCPiij8udxuvLTL\nkwh8z425aOtux59PvAiX24WH8u8ZlcEMwBkaIgpT/jpyp+gMyJmfgQ/3V+Hnrx2Bw+lGbYMdKQla\nFt4jGoA9h2tQbW3HtdONyDRG4jdHNuFiVwtWZ69AfvzkkR6eX5yhIaKw5K8j9/VpC3Hn9RORHKfB\nOXMbqq02uEXRV3jPVBLa/jJE4aTF5sDbX5yHVqXAnYsn4vUz23GupRIFhllYlrF4pIfXLwY0RBSW\nruzIrVd68mWq2mogl8kgCNIl2Vl4j8i/Nz8tQ0eXE19fNBHHLh7CPvN+pOtScM+UNX6/p0YLLjkR\nUdjq2ZHb6Xbifw/8Gl/UFGGeYTYsTR2S97DwHpG0suoWfHmiDhlJOqRN6MLvjr0LXUQkvj3jASjl\no78cAmdoiGhMUMgUuGfKnRAg4JXTb8KYoJK8zhgfGeKREY1+breIl3adAQCsWpKMF4pfAgB8a/r9\niFPHjuTQBowBDRGNGROjs3Bd6jWos9cjY7r0LqiVhZkhHhXR6LfnSA2q6ttROC0BO63b0N5tw125\ntyMnZsJID23AGNAQ0ZiyKvtmRCujcNJmwroVyYhUe1bWDbEabFiVz11ORFdotTvw9ufnoFHJ4Uo7\nhur2WixMWYDrUgtHemiDwoCGiMYUjUKDtZNXwym6UGTbAf1sT2sEYfLnkMebR3p4RKPOm5+Ww97l\nRP78ZpxoOoGJ0Vm4K/f2kR7WoDEpmIjGnJmJ05ChT0NVWzUAQBCAFncj/lL8CgDA1WjEjqIK1qeh\nca+8pgV7j5uRlNGGkq4ixKii8a3p90EhC7/wIPxGTEQ0AF0uh+Tx7Wc+Qu2+ub7X3vo0ABjU0Lji\nSQQuhaBuR1fKQSggx7en348opX6khxYQBjRENCZZOxokjzd3Sx/fUVTJgIbGHFOJxe9s5GdHa1DZ\n0ISY2cfR5e7CA3nrkBmVPrIDHgIGNEQ0JvlrjeDukG5Yyfo0NNaYSiy+2Ufg8mxkWU0LSiqaYG60\nQZl7HF2yVtyQvgjzk+eM4GiHjknBRDQm+WuNENU+VfI469PQWLOjqELy+KfnD6DJuAvqeTshj7HC\nbdfB6JgreW04YUBDRGNSz9YIAjwl2zWdKbhjxnWS17M+DY01tQ32PsfkcWYoc45Bpm2Ht5OBTNuO\n7Sf2hXh0wceAhojGrALDLGyc/wP8YvFTkHVHwq4yIzm1GxtW5aNnVxq9NgIFUxJHbJxEwyElQdvn\nmCKlXPLaVl3JcA9n2DGgIaIxTymPwET3QgiCiBdL3sTcyQkQAUzJiMH1s1LQZu/GgdPSlYWJwtXK\nwqw+xwSNdK6YTNM+zKMZfgxoiGhcmG2cCmdDCuo6avH/7XsG6nk7YU74EGm5rRAEzy4ntyiO9DCJ\ngmZBngE3zffsWpLHmaGZ/iUESP8bj41ICOXQhgUDGiIaF7JTouFu8zTZa+1ugyCIcCgu4q2KN5E7\n3Y4aqw3HyxpHeJREwaVWKnx5M9C0oddaaw+rJy8P7cCGAQMaIhoXUhMiEZFcJXnuov4YAOD9ogqI\nnKWhMeRk0wlETDzh93yq7v9v797DoyrvfYF/19xym2SSSSaTewIkXBIuCQQigrJBLlUs7W5BxGp7\nOLbqc6pt3Z56rO2jPvs5WnuO9uneR3u2ttZ6sNZKpZYtilgEFQmBcAmQCySEJCSZTO6XmVzmts4f\nQwaGWWsIJJnJTL6fvzLrXbPmp2/I+s273vf3pmJ7wX0oNhYGMKrJwTo0RDQtKBQChEjpeQL99j7M\nyrfgQhVQ09SLedkJAY6O6ObJFc8rN59Cq/aQ3KAMFIICTy97PJChTiomNEQ0bWiFBFjQLdlmSzwP\nYDE+Km1gQkMhQ654HgDs69/v972pMeFVGZuPnIho2liWuEK2rdvWiXnZCahs6MFFU38AoyK6eXLF\n8/aUNsI86H/l3vrs1RMfUBAxoSGiaWN9XglcI5GSbakxRk9xvT2ljYEMi+imSRXPA9xbecQI8ZJt\noksBW90iOLtSJzO0gGNCQ0TTRmy0BtHdCyTb1mauwrzsBMxIjcOJ8x1o6eTeTjT1SRXPU+pNiFr4\nFSyuHsn32OsXwNmdGnaJOxMaIppW5sYVwFa3CK7BWAhQIFoVBWB0KbeAuy+P0nx8JLz+2FN4urZ4\n3ugSbaem32uJtigCrsFY98hMt3tkJtw2ZGVCQ0TTyqz0ODi7UzFydgUezHwCzy3/H9CqY7Cnfh+6\nhrqxKC8JaUkxOFJpRmfvULDDJfKrJN+IOVnuR0uCIL+1gTgUi5GzKzzJDBB+G7IyoSGiaWVo2OH5\n+c//qMXZ2gF8O+/rsLns+Mv5DyAA2HhLNlyiiI+PStetIZpKBocd0KgU+O2/rIJCZmsDqZIF4bYh\nKxMaIpo2yqrMeP+Les/rzr5hvLa7Eq6uNMxNyENlVw1OdpzBsvxkJOki8WWFCX2WkSBGTOSf3eFE\na6cVmclaRKiVUAsayfP0miRkGLRQKgRkGLR4eFMBSvLDa9k269AQ0bQht8T1oyNNeHTbP+P5o7/G\nzvN/xzx9Hu68JRs7PjmHfccuYcvq3IDGSTRWl9qtQHwretOb8OhnOyHK7NX0zTnrUHx76FcD9ocj\nNEQ0bfhb4pocnYQ7c+5Av20Af7+wFysXpEAXo8GBky2wDtsDHCnR2BxqKocmtwKDQo9XMuMaiYTo\nEq5MBA6zJdpSOEJDRNNGWlI0mjt85xiMTo5cm7UKx8yncKjlCJalLMb6ZZnYeeACfv76EViGHF5l\n5YmmgtOWMumhCacawxX/5Hm5p7Qx7H9vOUJDRNPGtUtcrxx3T45UKVTYNudbECHizzXvIzrS/Sey\nf9AOlyh6ysqXVZkDFTKRX0NCr+TxaycBh9sSbSlMaIho2ijJN+LhTQV+J0fmxs/AirRlaLW24cPa\nA5LXCbeCZBSa7A4nXMO+hfUAQBzWer0OtyXaUvjIiYimlZJ843WH3r856y6c7qhCv64KQoQe4oj3\nTWM6fNulqa/RPCDb5mid6fU63JZoS+EIDRHRNaLV0dic93UIShfU2VXANStHpsO3XZr6/tH4ORRR\nVuiVRqTHpEIhKJCuTcVtuo1IVeaF9RJtKRyhISKSsMRYiL11h2GKb0TEos8haEYgDsXA0ToLG5ev\nDXZ4NM21WEw4O1QK0a7B/QsfwJy0ZO8TlgQnrmBiQkNEJEEQBKzIXoi/1jZCETHsPhZtgSa3AsrE\nAgDh/42Xpp5y8ynsbfgMJmsbIABidwZyU5KCHdaUwEdOREQyDrcekzy+r1F6sjDRZCo3n8Kble+4\nk5nLFCn1ONlxOohRTR1MaIiIZLQNtkseb7W0SR4nmkyfNHwmeZwJthsfORERyUiJTkar1Td5ESGi\n3HwKxcbwLiVPgVdWZcae0ga0dg76FHJss0on2CYr6yIBN5nQ2O12PPXUU2htbYVSqcQvf/lLZGZm\nep1TUFCAxYsXe17/8Y9/hFKpHF+0REQBtCFnDd6sfEeybV/jASY0NKHKqsx4bXel5/VoIUfAXW4g\nUhWJQYfv9h2pMZzPBdxkQvPhhx8iLi4OL7/8Mg4dOoSXX34Zv/nNb7zO0Wq12LFjx4QESUQUDMXG\nQvyx8s+SG/7xWzFNNKnNU5V6E/7UVIodbf2yG0+uz149uYGFiJuaQ1NaWop169YBAG699VacOHFi\nQoMiIpoq5L79pkQnSx4nulnXbp6q1Jugya2AU9PnlcwkRMRDdAlQ2XTYXnAfRwovu6kRms7OTuj1\negCAQqGAIAiw2WzQaDSec2w2G5544gm0tLRgw4YN2L59u99rJiREQ6Ua3yMpgyF2XO+n8WMfBB/7\nYGJtWXgX/q30Dz7H5xlnef2//uJkM3bur0WTeQBZxlhsuSMPtxdlBDJUukoo/jvISolFg6nf81qV\ndkHyvAhlJIbLb8GaW3Nw5/xFgQrvhgW6D66b0OzcuRM7d+70OlZRUeH1WhR9h8GefPJJbNq0CYIg\n4P7770dxcTEWLFgg+zk9Pb7PBW+EwRCLjg75MtA0+dgHwcc+mHizo+biNt1GfNH2BRBhgdKhhaAZ\nxOGmE7gjdQ10EbE+cx8aTP34328fR3//8LSo0DrVhOq/gw1LM71+j4Qo6S02zJdX3xl1kVP2v3Oy\n+sBfknTdhGbLli3YsmWL17GnnnoKHR0dmDt3Lux2O0RR9BqdAYBt27Z5fr7llltw/vx5vwkNEdFU\nVFZlxr5PnQBWeI4pk5vgzKnCuzUfYH3yN7DzYJ3ke/eUNjKhoTEryTfig0P1MHcPAQAUthiIERaf\n88Qh99Ybe440QqNW8nfsspuaQ7NixQrs3bsXAHDgwAGUlJR4tdfX1+OJJ56AKIpwOBw4ceIE8vLy\nxh8tEVGASU3UdLZnwjUQj9NdZ/D83z9Cd/+I5HtbOi145o0yfP9XB/DMG2Uoq+JEYpI3bHOgs3cY\nURFKACKi1ZGS59la3BtPtvcM4bXdlfy9uuym5tDcddddOHz4MLZt2waNRoMXX3wRAPD6669j6dKl\nKCoqQkpKCjZv3gyFQoE1a9Zg4cKFExo4EVEgXDtR002AraEAkfNLEZt3DjiXgr5+l89Zouheegv4\nLsElulpZlRnvf34BiG8F0usRGTkAqwDoNLGIUcegbbAdwnAshi7lwNmd6vVejgS63VRCM1p75loP\nPfSQ5+ef/vSnNx8VEdEUkZYU7UlKrpauTcWyGavxccN+zF1swsmDY7uh8OZD1xqdgzW6qgkAhMtt\nfbYBfCvv6yg2FuL7vzoAl8ScVVOX9Fyb6YZbHxAR+bFxeY7M8WxsyF4DY7QB5wZP4Vtf0yPD4J7b\noFAIEATJt/HmQz5GH2vKrWoa3dogLSlasj01MWYywgo5TGiIiPwoyTfi4U0FyDBooVQIyDBo8fCm\nApTkG6FWqrFtzrchQkTFyGd4dnsxbpmfApdLREoCbz40Nq2dg1DqTRCifCcAA1eKOPpLrol7ORER\nXVdJvlH2MVFewkysSFuGr1qP4ueHn8dAlBUR82PQ3joLQKrP+bz50LXiZp/HiE56dAa4UtyxMC8J\nggColAq4XCJSE2OwcXk2H2FexoSGiGiccmKz8BWOYsBmAQRAEW2BJrcCtjp4JnDq4yKw5Z9yefMh\nL+XmU36TGeDK1gZ1zX0QReCOJRm4Z3VuIMILKXzkREQ0TgeaD0keV6XVe34umSc/ykPT19/O7ZNv\nFIFo81LP1gY1TT0AgLlZCYEILeQwoSEiGqe2y5VbryVEDUCpNwEAmtql50fQ9NZr75JtUzt06G5K\nhM3uBADUNPZAqRCQl6ELVHghhQkNEdE4yW1UKQiAJrcC6kQTmswDktvE0PRVbj4FUZRZDgcgW1EE\nUQRau6wYGnHgomkAOamxiIrgbBEpTGiIiMZpQ84av+2RmQ0YGLSjz2oLUEQ01ZWbT+HNyncgKHwL\nMgJARF8uigzujScvtVtQ29wLlyjycZMfTPOIiMZpdI7Dm5XvSLY7Ne5N+prMFsRrIwIWF01N5eZT\n2FH1nmSb6FLAXr8A3125Fkk699YHze1WmBTuitXzspnQyOEIDRHRBCg2FiItJkWyTYQLEfMPoaz1\nRICjoqlmdGTGITok2wVBxPdXrkVJvhHplws1XmofQHVjD1RKAbnpnD8jhyM0REQTZEPOGtlRGkW0\nBafsn6LcbPCM6FB4K6syY09pA1o7B5GWFI2Ny3Owu9PPqiYASlucZzVcpEaF5PgoNLQNYMTmRF5m\nPDRqZQAiD00coSEimiDFxkL8ePl/RbrWt6DeqNEy9hTeRvdnau6wwiWKns1J/a1qAoCR5hyv1xnJ\nWgzbnBABzM2Kn7yAwwATGiKiCbQiaymeXvY4FIL0n9fRMvYU3kb3Zxql1JsQMf8QREivdBNdCtjq\nFsGoyPM6fvWeYKWVbSir4u+PHCY0RESTQG4pt16TFOBIKBhaOwc9P4/uoq2ItshuWmqvXwBnd6rX\n1hhlVWYcP9fhed3RO4zXdlcyqZHBhIaIaBLILeWepVoc4EgoGK7eGVtuF21RBFyDsbDVLYLOkePZ\n9HTUtaM8V443TmSoYYOTgomIJsHoxN99jQdgsprhEl0QHSo4ejlCMx1sXJ6D13ZXAgCEKKv0SaIA\ng3kDNq6U3mDy6lGeq5m6ZK43zTGhISKaJMXGQk9i8+GFT/Fx46eosh4CsDC4gdGkm5EWB6XeBHV6\nPSAzbyYa8fjXB5fJXiMtKRrNHb7JS2pizESFGVb4yImIKADunLEGypF4DMU04HRHVbDDoUn23onP\nocmtgBA1IDtvJle9xO81Ni7PkTmeLXl8uuMIDRFRACgVSuS5VqHatRt/qv4rcuP/O6LV7nkWUvVK\nuDN36LLZnageOgZESbfHK5PQfi4dBSXz/V5n9HdgT2kjTF1WpCbGYONy6cdTxISGiChg5hgycaY6\nF5bMWuys3Y3v5d/rqVcyarReCQDeuEJUWbUZYqQFUgMzCkGBpYrN2N3d4NnawJ+SfCN/D8aICQ0R\nUYBkGmPh+GwG9Jm9ONp2AnW9F9E91IuI+TFwtM6Cs/tKQb49pY28kYUgURTx2YkWiEkxEKItPu2p\nMUZ0tg8DABLHkNDQ2HEODRFRgGQmawEooLFkAAC6h3sAQYQi2gJNbgWUepPnXK5kCU31pn40tg0g\nGbmS7euzV6OrbxgCAH0cE5qJxISGiChAtFFq6OMi0K2qk2xXpdV7fuZKltD02fEWQOGEU3cJAJAY\nqYcCCrgGYzEPa1BsLERn3zDiYyOgUvIWPJH4yImIKICykmNREyE9v0KIvPKIgitZQk//oA3HasyI\ny6tDj70LqzNXYnPeJvQMjOCJV7+CYm4ynC4XegZGMDM9Ltjhhh0mNEREAZSZrEV1v/T8CggitIWH\ncavhNq/5M1wFNbWN9k9LhxVCfDvsuotIi0nBN2beCQDQaTVQKRXo6B1Cz8AIXKKIJD5umnBMaIiI\nAijLqIWjZhY0uRU+bYIAODX9+LJvD3LNOhQbC2VXQdW19OE762YHMnS6RlmVGTsP1KFP3QBV2gVE\n5FgBiBBdApZErodaqQYAKAQBhvhIdPYOoauPE4InCx/gEREFUKYxFs7uVGQN3450barsefsaDwCQ\n389n//FmblIYRKOJZp+64aqNJ0UIAiAoRByoPud1fpIuCtZhh6fyLxOaiceEhogogJJ0kYjUKDHQ\nasDTyx6HIDmbBmixmFBuPiW7nw/ATQqDaTTRlNt4sl/rXQ06Kd6dwNQ09rhfM6GZcExoiIgCSCEI\nyEzWwtRlhc3uRJwyUfbcNyvfQWJ2l2w7l3YHz2iiKbfxpCLKe46UQecuG1zT5E5oEjmHZsIxoSEi\nCrCs5FiIItDSaYXRvsDvuWJKtWwbl3YHT1pStLtukCg9wpag9t5V3XB5hMY67ADAhGYyMKEhIgqw\nTKMWANBkHoCtIwW2ukWy51pdA8jI7ZNs49Lu4MkvHHZvPqlwSbZ/c846r9eG+CsbO8XFaKBRKyc1\nvumICQ0RUYBljSY07Ra0dFiQ4JwBQ0Sy7PmWOPcoTUJsBAD3DfHhTQVcuh1EFxzHJY+rFCpsL7gP\nxcZCr+NJuqirfubozGRgQkNEFGDpSTFQCAKqGnrQP2hHhkGLNRmrZM8fUfYhOT4KT2x13yQLcxOZ\nzARJufkUni/7NVqtbZLtLtHlk8wAQHSkCjGR7kopfNw0OZjQEBEFmFqlRGpiNMzd7oml6YYYrMhc\nAteI9I3ONaTFvJwEz41wtJYJBVa5+RTerHxHNpkB3JtPSimrMsPmcD+eqm7s4ZL7ScCEhogoCEbn\n0QDuERulQgGlOV/yXGdPMuZlJyBCo4Q2So3O/pFAhUlX+aThs+uesz57tc+x0Zo19ssJjWXIjtd2\nVzKpmWBMaIiIgkG88uPfD11EWZUZOnsOhMYipGtToRAUMEQlAiKgMjQjI81ddTZRF4nu/mGIoihz\nYZosbYPtsm3p2lTJuTOAfHFE1hGaWNz6gIgowMqqzDhy1bdzc88QXttdidTEaAyajXjyu1uhUipg\ndzjx2J//CGX6Obx06tewuxxQp8fBZctG/6AduhhNEP8rws/19szSRyagc8i3LlC6NhVPL3tc9rpy\nxRFZR2hiMaEhIgowuW/svRYbAPcjiXhtBOqa++AcioASwIjT3Tai7IUmtxc7z2vw/aItnvdyA8vx\nkdsz6+JQDS44jqPNaobcmJjUY6arpSVFe7Y8uBrrCE0sPnIiIgowuW/swyPuomv9VnfyUtXYA1Va\nveS5J3uOodx8CsCVm3FzhxUuUfTcjDlHY+ykkkyl3oQv+/ag1doGF0SIl1OahIh4CFDANRiLFXF3\nST5mutrG5Tkyx1lHaCJxhIaIKMDkvrHHxmjQb7Whf9Cd0FQ39kBIl38ssaP6PQDAnlLpScJ7Shs5\nSjNGUkmm3D5N0eooLHbeiw/PNmDxAvmiiKNG+2BPaSNMXVakJsZg4/Js9s0EY0JDRBRgG5fneD3e\nGLVoViK+PG3CgNWOwWEHLpr6oTXGwaGWrhTscDnwZuU7sLsWAfDduZtzNMbu2iRTqTdBuGY/plEm\nqxnGQffS+YS4iDFdvyTfyARmkvGRExFRgJXkG/HwpgJkGLRQKgRkGLR4eFMBFuW69//pH7Th3KUe\niCIwN2Lpda+nyToveZxzNMbu6sdCSr3Jva2B9DZNSI0xovvy0nl97NgSGpp8HKEhIgoCqW/sdc3u\nkZj+QZuneN6amUuRODyIz5sPy15LVA9BqTfB2e09StPSacEzb5RxgvAYlOQb0WQewL7aMqhnnvF7\n7vrs1dh1ahjaKDXUKu7JNFVwhIaIaIqoN7kTmo+PNOEfx5sBAF39w7hn9jexveA+KCF/8zTObYXi\nmhEFUQQnCN8AZ1yL3w0nAWB7wX1YkrwI3QPD0I/xcRMFBhMaIqIpoKzKjHf31/kc//2H1SirMqPY\nWIio9iWy7+9zdkGpVECtkv6zziJu11fe/7nf9nRtKoqNhRgcccBmd0Efyz2ZphImNEREU4BcbRp3\nmzsZ6WpMlN3vSQEFlEUfQzH3Cyj1Jp92ThD2773zH2BIlJ4EPGq03szo/JmxTgimwGBCQ0Q0BcjV\npgGuJCNpSdFwXJojeY5DdEAQRCiiLdDkVkCdVeXVzgnC8srNp/zOUVIpVF7bGvQMuOc3cULw1MKE\nhohoCkhLipZtG01GNi7PgbM7Fba6RXANxkJ0CRBd0n/GVSlNXiM1LOIm73qbTka2LYazyz3huqzK\njD9+XAMAOHCihXOTphCuciIimgLkatO429zJyOhKpfc/j0Tn2VToYjSwF/ynbEl+VVo9hN40uEQR\nszPjJyPssGCyyiclrpFIdDTo8VpDJepa+rD/8mRtAOgeGPH0GVeRBR9HaIiIpoDR2jRXP8bQx0Xg\n4U0FXjfLknwj/ts/zwcALJ2XjNQY+RupKsaK+9bPhksEPj12afKCD2FlpuOeLQ2kXP2I74tTrZLn\ncML11MARGiKiKWKs1WTjte6kp9diw4YFa/Bm5TuS57lEFw6N/AXatHQcOKnEXcuzoY1ST2jMoeyL\n5lL85fzfoFaoYXfZfdodbVletX3sTunl3JxwPTVwhIaIKMTERWsgCECvZQTFxkKsyrhV9lyTtQ3O\njONwxDbjs6sel0x3nzYexF/O/w2xai1+WvwothfcB5VNB9ElwDUYC1vdItib8r3eo1ZK3zI54Xpq\nYEJDRBRijtW0QxAE1DX34Zk3ypDtWI61SV+HazBW9j2a9Hp8Wn4JwzZHACOdekRRxIf1n+CDCx8h\nPkKHxxc/4qkvE910B4bLN2Dk7AqfqssAcNsi32MAJ1xPFXzkREQUQsqqzF6Th0crAa9fmomRsysQ\nvewTyTkhQpQV1mEHvjjVivXLsgIZ8qQqqzJjT2kDWjsHkZYUjY3Lc3D3KunEThRF7Kr7EJ9d+hJJ\nkXr8qOghJEbpPe39VuldyzUqBbbfNQ9xMRp8dqIFUREq2OxO7po9xTChISIKIXIF+MrPtQMA4pR6\n9Dm7fNpjNVo41UrsPdqE1YszZCsKhxK55C4uLhLzMnRe57pEF/5cswuHTUeREmPEY4XfR3yE9znD\nI07Jz3E4XVg6Nxn//v5pAMC/bF2EWWk6yXMpeEL/N5qIaBqRK8DXM+AeXVigLZFs77f1I6fQhF7L\nCEor2yYtvkCSS+527q/1eu10OfFW1bs4bDqKzNh0PF70iE8y43S5ZNc6uUTg8Nk2nL7QhdwMHZOZ\nKYoJDRFRCJErwKeL0QAAZmsLsL3gPqRrU6EQFEjXpuLbuXcjMVKPJuEENDOqsOdIA1wu+aXKoUIu\nubtkHvD8bHfa8fuzb6PcfAozdTn4cdFD0Gp8J/EeOu27XcTV/vBRNQBgZmrcOCKmycRHTkREIUSu\nAN/szHgcrW5HhFqB+cZCT5n+UUuMRfhtxRtoxiX0KUdwpCYLt+ZnBCrsSWGIj4S5Z8jneKbRPYdm\nxGnD66ffQk1PLeYm5OGhhd9DhFLjc35ZlRlv7T03ps/cd+wSZqTGcd7MFMQRGiKiEDJagC/DoIVC\ncB+bmRaH5AT3yI1GrZR8ny4iFj9Z/AhytDlQ6tvxl4Z3YLXJ7x811TldLjhlRpm23JGHIccQXjn1\ne9T01GJBUj4eWfhfJJMZAPiwtOGGPpuF9KYmJjRERCGmJN+If31wGX735GpkGbW4aOpHc7t7p2iN\nWv7PepQqEj8pALPthwAADjpJREFUfgg6ezYckZ14sewV9I70BSrsCbW3rAmdfcPIy9BBcTmzyzBo\n8fCmAhTmx+PfTr6O+r4GFBsL8YP5D0CtlC4o2G+1oaXjxgrjsZDe1MRHTkREIUoQBNxZko3Xdlfi\nVF0nAECjkh6hGaVWqPDwogfwwoH/h25jE14qfxWPFn4fKTHJ44pFavn0RD+WufIZVrhEICpCice+\nvRC/+tMJ9FpG8K8PLkPfSD+e++zXaB4w4dbUZdg291tQCNJJ3vlLvfiPv5+V/Ty1UiFZHZiF9KYm\nJjRERCGseK4B73yqxsCQu3T/K7vO4BsrZ/hNJrJT4jBXtRLVl46hJ7MW/6v83xGriUX3cA9SopOx\nIWeNzxwcf+SWTwMTt2njtZ8BAEMjTlRe7IaQ0ApHyhk89tkeCIICTtGJNZm34Vu5d0MQBJ9riaKI\nvUeb8P7BeneM85JRVt3uc97thWlem1GOYiG9qYkJDRFRCCuv6fAkMwDQ1j04pmTi7uU5OPN2NxKT\nRPRH1WFkyF27ptXa5tkbaqxJze6vLkoe31PaOCEJjSiK+ODLesm2v5z7O0YS6qAA4AIA0V1LJis2\nQzKZsQ7b8caH1ThV1wmdVoNHNhVgTlYCCvPM2FPaCFOX1atgXm66TvI4TT1MaIiIQphcLZbrJRN5\nGfGYnRmPRrFNcjLlvsYDfhMaURRR39qPfxxvhqlLenLxeOaajNicqG7swZn6Lpy+0IWu/mGfc5R6\nE0Z0dZLv/7TpIJamFHkda2jrx2//dhadfcOYl52AhzYVeJa7y20MOtYNQyn4mNAQEYUwuVosY0km\nNi7Pxv+9KH1ei8WESwMtyIxN9zpud7hQXtOOfxy/hIsmd70XlVKAw+m74uhG5pqIoghzzxBOX+jC\nmfounGvq8VwzKkKFqAglbDHNUKVdgBBlhTgUA6h8d8geZbKava594GQL3t1fC6dTxNdvzcE3Vs7w\nTCam8MCEhogohKUlRaNZYpXOWJKJ+TP0UJ2Pg1MjvdLpxWP/hvmJ83DnjDsQrzDi4MkWHDzZgv5B\nOwQBKMpLwtriTPRZRvD6f1b5vP96c01sdidqmnpx5nIS0957paZMVrIWC2YlYsHMRMxKj8POk1/i\ny74KT7sQbfF77dQY96jK0IgDb+2twdHqdmij1Hjo6/mYPzPR73spNDGhISIKYXKF9sYycVUQBKww\n3oYvej70aVuftRp1fRdxtqsaZ7uq4epLgr1lFiKdBnxtWRZWL06HIT7K61p7ShvR2mmBSwQW5Sai\nJN/os/rptoWpUCgUOH2hCzVNPbA73KuIIjVKLJljwIKZ7iQmITbCK54LjuM39P9lffZqNHdY8Nu/\nnUVb9yBy03V45BsF0MdF3tB1KHTcdEJz9OhR/PjHP8YLL7yA1atX+7Tv3r0bb731FhQKBe655x5s\n2bJlXIESEZGv0fkdNztxdcui2/DVDhPsiechRFqgssfhFsMKJA/PRcXZRIxYDFClX4BS14kIXSdy\ndbNQNNM7mRmNoyTfCLvDiX955StcbO3H4bMm/P7Das85zR1W/Hn/lTkvGYYYLJiZiIWzEjErXQeV\n0ns2jyiK6BjqRHV3LVqtY99/6mt5/4SRdiP+5yflsDlc2LAsE99eNcvn+hRebiqhaWpqwptvvonF\nixdLtg8ODuLVV1/FX//6V6jVamzevBnr1q1DfHz8uIIlIiJf45m4eqymHRaTATAZPMf+AReAKggC\nUJg7B2uL1kIV14O9DftR01OL35y8gNz4GbgzZy3mJOR6rSZSq5SYmabDmfour2TmagmxEfj5A0sk\nR0us9kGc66lDTfd5VHfXonu4x2/8CRHxsNuUGHB2QxzWInZgHuq60nHmQjWiIlR4dFMBFs82+L0G\nhYebSmgMBgNeeeUV/PznP5dsr6iowIIFCxAb695PY/HixThx4gTWrFlz85ESEdGEk1slFRulxi++\nV3zVSEwC8hJm4mJfIz5u2I/Krhr8n1O/w4y4bNw5Yy0G7YPY13gAJqsZzugYKPWz4OxOlbx2v9Xm\nSWYcLgcu9jWhpqcW1d3n0dTfDPHyvtdRqigUGhZgnj4PdpcDf63d7XOt+ZErsO9Lp+f1CIBOdCFJ\nF4n/vq0IydeMJFH4uqmEJirK/y9IZ2cn9Hq957Ver0dHR4ff9yQkREN1nQqX12MwxI7r/TR+7IPg\nYx8EXyj1QavMkuvBEQfy83yrBxsM87Esdz4udDfi/aqPUd5Sgd9WvOF1jiLaAk1uBUSxAqItAoAA\nQTMCcSgGjtaZSI1Jw/He46hoq0Jl+3kMO0YAAEpBgbmGWVhgnIdFKfMwKyEbCsWVx0TpSUn4oOoT\nNPebkBGXim/mb8C771kA9PvEGROlRoFE/BQ4gf53cN2EZufOndi5c6fXscceewy33XbbmD9EFK+/\nTX1Pz/g2STMYYtHRMXD9E2nSsA+Cj30QfKHWB2mJ8quk/P13xEGP7XO+g7Vpq/GbE/+BYadvnRhB\nAISIkSuvoy3Q5J5GF07jDyfcx5Kjk1CSsgTz9LORFz8TkarLj6FcQNc1S89nR83Fk0vmeh1rajsg\nGV9zuyWk+iHcTNa/A39J0nUTmi1bttzwhN7k5GR0dnZ6Xre3t6OwcOxltImIKDDGs0oKADJj02Bz\n2W7oM6NUkfjn3I2YmzAbiVEJN/Tea41n2TqFl0mZ8r1o0SKcOXMG/f39sFqtOHHiBIqLiyfjo4iI\naBxK8o14eFMBMgxaKBWCZ8fqG5lknBJ9Y492Rpw2rEgrGXcyA7gTMunj3G9purmpOTQHDx7EG2+8\ngfr6elRWVmLHjh34wx/+gNdffx1Lly5FUVERnnjiCTz44IMQBAE//OEPPROEiYhoahlvef8NOWs8\n+z+NxWjRu4kgtWx924Y5mJehm7DPoNAgiGOZ4BIA433WFmrPrcMR+yD42AfBN137oNx8CvsaD6DV\n0uZZpSRne8F9N7Sb942arn0wlUzJOTRERETXU2ws9CQpo8mNyWqGThMHAOiz9SM1xoj12asnNZmh\n6YsJDRERTairkxuiQGEdaCIiIgp5TGiIiIgo5DGhISIiopDHhIaIiIhCHhMaIiIiCnlMaIiIiCjk\nMaEhIiKikMeEhoiIiEIeExoiIiIKeUxoiIiIKOQxoSEiIqKQN2V22yYiIiK6WRyhISIiopDHhIaI\niIhCHhMaIiIiCnlMaIiIiCjkMaEhIiKikMeEhoiIiEJeyCU0L7zwArZu3Yp7770Xp0+f9mo7fPgw\nNm/ejK1bt+LVV18NUoThz18fHDlyBPfccw/uvfde/OxnP4PL5QpSlOHNXx+Mevnll/HAAw8EOLLp\nw18fmEwmbNu2DZs3b8YzzzwTpAjDn78++NOf/oStW7di27ZteP7554MU4fRw/vx5rF27Fm+//bZP\nW0Dvy2IIKSsrEx966CFRFEWxrq5OvOeee7za77zzTrG1tVV0Op3itm3bxNra2mCEGdau1wfr1q0T\nTSaTKIqi+Nhjj4kHDx4MeIzh7np9IIqiWFtbK27dulW8//77Ax3etHC9PvjRj34k7tu3TxRFUXzu\nuefElpaWgMcY7vz1wcDAgLh69WrRbreLoiiK27dvF0+ePBmUOMOd1WoV77//fvEXv/iFuGPHDp/2\nQN6XQ2qEprS0FGvXrgUAzJo1C319fbBYLACAS5cuQafTITU1FQqFAqtWrUJpaWkwww1L/voAAHbt\n2oWUlBQAgF6vR09PT1DiDGfX6wMAePHFF/H4448HI7xpwV8fuFwuHD9+HGvWrAEAPPvss0hLSwta\nrOHKXx+o1Wqo1WoMDg7C4XBgaGgIOp0umOGGLY1Gg9/97ndITk72aQv0fTmkEprOzk4kJCR4Xuv1\nenR0dAAAOjo6oNfrJdto4vjrAwDQarUAgPb2dnz11VdYtWpVwGMMd9frg127dmHZsmVIT08PRnjT\ngr8+6O7uRkxMDH75y19i27ZtePnll4MVZljz1wcRERH44Q9/iLVr12L16tVYtGgRZsyYEaxQw5pK\npUJkZKRkW6DvyyGV0FxL5K4NQSfVB11dXXjkkUfw7LPPev3BoclxdR/09vZi165d2L59exAjmn6u\n7gNRFGE2m/Hd734Xb7/9NqqqqnDw4MHgBTdNXN0HFosFr732Gvbu3Yv9+/ejoqICNTU1QYyOAiGk\nEprk5GR0dnZ6Xre3t8NgMEi2mc1mySEwGh9/fQC4/5D84Ac/wE9+8hOsXLkyGCGGPX99cOTIEXR3\nd+M73/kOHn30UVRWVuKFF14IVqhhy18fJCQkIC0tDVlZWVAqlVi+fDlqa2uDFWrY8tcHFy5cQGZm\nJvR6PTQaDYqLi3H27NlghTptBfq+HFIJzYoVK/DJJ58AACorK5GcnOx5xJGRkQGLxYLm5mY4HA4c\nOHAAK1asCGa4YclfHwDuuRvf+973cPvttwcrxLDnrw++9rWv4aOPPsJ7772HV155BQUFBXj66aeD\nGW5Y8tcHKpUKmZmZaGho8LTzccfE89cH6enpuHDhAoaHhwEAZ8+eRU5OTrBCnbYCfV8Oud22X3rp\nJZSXl0MQBDz77LOoqqpCbGws1q1bh2PHjuGll14CAKxfvx4PPvhgkKMNT3J9sHLlSixduhRFRUWe\nc++++25s3bo1iNGGJ3//DkY1NzfjZz/7GXbs2BHESMOXvz5obGzEU089BVEUMXv2bDz33HNQKELq\n+2NI8NcH7777Lnbt2gWlUomioiI8+eSTwQ43LJ09exa/+tWv0NLSApVKBaPRiDVr1iAjIyPg9+WQ\nS2iIiIiIrsWvDERERBTymNAQERFRyGNCQ0RERCGPCQ0RERGFPCY0REREFPKY0BAREVHIY0JDRERE\nIY8JDREREYW8/w+Dv00mzT1+wwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "jLO2Gq4Jtso3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Descrição Mini Projeto"
      ]
    },
    {
      "metadata": {
        "id": "kOrl4i0areNo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Utilizando o código acima, modifique a última seção (Executando com Base de Dados) para que ele seja executado com a base de dados do arquivo train-mod.csv. Depois, modifique a função de base radial implementada (Gaussiana), para a Multiquadrática e a Multiquadrática Inversa e calcule a taxa de erro.\n",
        "\n",
        "1 - Calcular o taxa de erro no conjunto de teste na RBF utilizando a base de dados train-mod.csv\n",
        "\n",
        "2- Calcular a taxa de erro usando 3 funções de Base Radial:\n",
        "a) Gaussiana\n",
        "b) Multiquadrática\n",
        "c) Multiquadrática Inversa\n",
        "\n",
        "\n",
        "DATA DE ENTREGA: 02/04/2019\n"
      ]
    },
    {
      "metadata": {
        "id": "n13py9m5IClg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Código baseado em [RBF With Keras](https://medium.com/datadriveninvestor/building-radial-basis-function-network-with-keras-estimating-survivors-of-titanic-a06c2359c5d9)"
      ]
    },
    {
      "metadata": {
        "id": "IrDSpHHxIiTg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports e configurações iniciais"
      ]
    },
    {
      "metadata": {
        "id": "2OjV1yJ7IdsA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import random\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense, LSTM, RepeatVector, TimeDistributed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ydyyM5lTIshw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = local + 'train-mod.csv'\n",
        "kval = 10\n",
        "itertot = 40\n",
        "sigma = 1.2\n",
        "itergd = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qXUHdlWtJha2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def transforminput(param, center, function):\n",
        "  def rbfGaussiana(i, j, sigma):\n",
        "    return np.exp(-(np.sum((param[i] - center[j])**2.0)**0.5) / sigma**2.0)\n",
        "  \n",
        "  def rbfMultiquadratica(i, j, sigma):\n",
        "    return np.sqrt(np.sum(np.sum(\n",
        "           param[i] - center[j])**2.0 + center[j]**2.0))/np.sum(center[j], axis=0)\n",
        "\n",
        "  def rbfMultiquadraticaInversa(i, j, sigma):\n",
        "    return np.sum(center[j], axis=0)/np.sqrt(np.sum(\n",
        "           np.sum(param[i] - center[j])**2.0 + center[j]**2.0))\n",
        "  \n",
        "  switcher = {\n",
        "      'rbfGaussiana': rbfGaussiana,\n",
        "      'rbfMultiquadratica': rbfMultiquadratica,\n",
        "      'rbfMultiquadraticaInversa': rbfMultiquadraticaInversa,\n",
        "  }\n",
        "  \n",
        "  newinput = np.zeros((len(param), len(center))).astype('float32')\n",
        "  for i in range(len(param)):\n",
        "    for j in range(len(center)):\n",
        "      fun = switcher.get(function, \"nothing\")\n",
        "      newinput[i,j] = fun(i, j, sigma)\n",
        "  return newinput"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QruqTWJIJt6U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generatemodel(numparam):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(1, input_dim=numparam, activation='sigmoid'))\n",
        "  # Compile model\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B6-B3a4H65Li",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Normalizando base de dados"
      ]
    },
    {
      "metadata": {
        "id": "P75CdFJ4W7mU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataread = np.genfromtxt(data, delimiter=',')[1:,1:]\n",
        "\n",
        "alldata = []\n",
        "for i in range(len(dataread)):\n",
        "  if np.isnan(dataread[i,-2]):\n",
        "      continue\n",
        "  alldata.append(dataread[i])\n",
        "\n",
        "alldata = np.asarray(alldata)\n",
        "\n",
        "#dividing data\n",
        "# 600 primeiros elementos e para cada elemento não pegará a primeira posição\n",
        "trainparam = alldata[:600,1:]\n",
        "# 600 primeiros elementos e apenas a primeira posição de cada elemento\n",
        "trainlabel = alldata[:600,0]\n",
        "# a partir do elemento 600 ate o ultimo e para cada elemento não pegará a primeira posição\n",
        "testparam = alldata[600:,1:]\n",
        "# a partir do elemento 600 ate o ultimo e apenas a primeira posição de cada elemento\n",
        "testlabel = alldata[600:,0]\n",
        "\n",
        "###############\n",
        "#normalization#\n",
        "###############\n",
        "\n",
        "# Inicializa os vetores com zeros\n",
        "std = np.zeros((len(trainparam[0]))).astype('float32')\n",
        "media = np.zeros((len(trainparam[0]))).astype('float32')\n",
        "trainparamnorm = np.zeros(np.shape(trainparam))\n",
        "testparamnorm = np.zeros(np.shape(testparam))\n",
        "\n",
        "# percorre todos os indices do vetor de caracteristica\n",
        "# realiza a normalização do vetor de caracteristica\n",
        "for i in range(len(trainparam[0])):\n",
        "  std[i] = np.std(trainparam[:,i])\n",
        "  media[i] = np.mean(trainparam[:,i])\n",
        "  trainparamnorm[:,i] = (trainparam[:,i] - media[i]) / std[i]\n",
        "  testparamnorm[:,i] = (testparam[:,i] - media[i]) / std[i]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yJ8R08fORzRF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gaussiana"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "FqnY639CTg4m"
      },
      "cell_type": "markdown",
      "source": [
        "### K-means"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "1bff7bb6-3200-4419-d851-ef092e87f90f",
        "id": "iyQonSeoTg4o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "cell_type": "code",
      "source": [
        "kmean = np.zeros((kval, len(trainparamnorm[0])))\n",
        "for i in range(kval):\n",
        "  for j in range(len(kmean[0])):\n",
        "    kmean[i,j] = random.uniform(min(trainparamnorm[:,j]),max(trainparamnorm[:,j]))\n",
        "    \n",
        "#looping of real algorithm\n",
        "distmin = np.zeros((len(trainparamnorm)))\n",
        "for i in range(itertot):\n",
        "  print('iterasi ke', i)\n",
        "  for j in range(len(distmin)):\n",
        "    #determine euclid distance\n",
        "    distall = np.sum((trainparamnorm[j] - kmean)**2.0, axis=1)**0.5\n",
        "    distmin[j] = np.argmin(distall)\n",
        "    \n",
        "#search new k mean\n",
        "for j in range(kval):\n",
        "  clust = []\n",
        "  for k in range(len(distmin)):\n",
        "    if distmin[k] == j:\n",
        "      clust.append(trainparamnorm[k])\n",
        "    if len(clust) > 0:\n",
        "      kmean[j] = np.mean(np.asarray(clust), axis=0)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iterasi ke 0\n",
            "iterasi ke 1\n",
            "iterasi ke 2\n",
            "iterasi ke 3\n",
            "iterasi ke 4\n",
            "iterasi ke 5\n",
            "iterasi ke 6\n",
            "iterasi ke 7\n",
            "iterasi ke 8\n",
            "iterasi ke 9\n",
            "iterasi ke 10\n",
            "iterasi ke 11\n",
            "iterasi ke 12\n",
            "iterasi ke 13\n",
            "iterasi ke 14\n",
            "iterasi ke 15\n",
            "iterasi ke 16\n",
            "iterasi ke 17\n",
            "iterasi ke 18\n",
            "iterasi ke 19\n",
            "iterasi ke 20\n",
            "iterasi ke 21\n",
            "iterasi ke 22\n",
            "iterasi ke 23\n",
            "iterasi ke 24\n",
            "iterasi ke 25\n",
            "iterasi ke 26\n",
            "iterasi ke 27\n",
            "iterasi ke 28\n",
            "iterasi ke 29\n",
            "iterasi ke 30\n",
            "iterasi ke 31\n",
            "iterasi ke 32\n",
            "iterasi ke 33\n",
            "iterasi ke 34\n",
            "iterasi ke 35\n",
            "iterasi ke 36\n",
            "iterasi ke 37\n",
            "iterasi ke 38\n",
            "iterasi ke 39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7i4JvOLyLJhb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Treinando a rede"
      ]
    },
    {
      "metadata": {
        "id": "bqBBppMvLOgX",
        "colab_type": "code",
        "outputId": "ee5b68fc-334d-420b-e719-639e20b8616b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "cell_type": "code",
      "source": [
        "newinput = transforminput(trainparamnorm, kmean, 'rbfGaussiana')\n",
        "\n",
        "mod = generatemodel(kval)\n",
        "mod.fit(newinput, trainlabel, batch_size=20, epochs=itergd, verbose=1, shuffle=True)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "600/600 [==============================] - 0s 490us/step - loss: 0.6780 - acc: 0.5833\n",
            "Epoch 2/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.6762 - acc: 0.5950\n",
            "Epoch 3/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.6745 - acc: 0.5950\n",
            "Epoch 4/300\n",
            "600/600 [==============================] - 0s 203us/step - loss: 0.6731 - acc: 0.5950\n",
            "Epoch 5/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.6718 - acc: 0.5950\n",
            "Epoch 6/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.6707 - acc: 0.5950\n",
            "Epoch 7/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.6695 - acc: 0.5950\n",
            "Epoch 8/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.6685 - acc: 0.5950\n",
            "Epoch 9/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.6675 - acc: 0.5950\n",
            "Epoch 10/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.6667 - acc: 0.5950\n",
            "Epoch 11/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.6657 - acc: 0.5950\n",
            "Epoch 12/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.6649 - acc: 0.5950\n",
            "Epoch 13/300\n",
            "600/600 [==============================] - 0s 218us/step - loss: 0.6641 - acc: 0.5950\n",
            "Epoch 14/300\n",
            "600/600 [==============================] - 0s 197us/step - loss: 0.6633 - acc: 0.5950\n",
            "Epoch 15/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.6625 - acc: 0.5950\n",
            "Epoch 16/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.6617 - acc: 0.5950\n",
            "Epoch 17/300\n",
            "600/600 [==============================] - 0s 169us/step - loss: 0.6611 - acc: 0.5950\n",
            "Epoch 18/300\n",
            "600/600 [==============================] - 0s 168us/step - loss: 0.6603 - acc: 0.5950\n",
            "Epoch 19/300\n",
            "600/600 [==============================] - 0s 183us/step - loss: 0.6596 - acc: 0.5950\n",
            "Epoch 20/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.6589 - acc: 0.5950\n",
            "Epoch 21/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.6583 - acc: 0.5950\n",
            "Epoch 22/300\n",
            "600/600 [==============================] - 0s 167us/step - loss: 0.6576 - acc: 0.5983\n",
            "Epoch 23/300\n",
            "600/600 [==============================] - 0s 206us/step - loss: 0.6569 - acc: 0.5983\n",
            "Epoch 24/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.6563 - acc: 0.5983\n",
            "Epoch 25/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.6556 - acc: 0.5983\n",
            "Epoch 26/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.6549 - acc: 0.5967\n",
            "Epoch 27/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.6543 - acc: 0.5967\n",
            "Epoch 28/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.6537 - acc: 0.5967\n",
            "Epoch 29/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.6530 - acc: 0.5967\n",
            "Epoch 30/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.6524 - acc: 0.5967\n",
            "Epoch 31/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.6518 - acc: 0.6000\n",
            "Epoch 32/300\n",
            "600/600 [==============================] - 0s 201us/step - loss: 0.6512 - acc: 0.6033\n",
            "Epoch 33/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.6506 - acc: 0.6100\n",
            "Epoch 34/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.6500 - acc: 0.6117\n",
            "Epoch 35/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.6494 - acc: 0.6117\n",
            "Epoch 36/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.6488 - acc: 0.6117\n",
            "Epoch 37/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.6482 - acc: 0.6133\n",
            "Epoch 38/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.6476 - acc: 0.6133\n",
            "Epoch 39/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.6471 - acc: 0.6150\n",
            "Epoch 40/300\n",
            "600/600 [==============================] - 0s 170us/step - loss: 0.6465 - acc: 0.6150\n",
            "Epoch 41/300\n",
            "600/600 [==============================] - 0s 170us/step - loss: 0.6459 - acc: 0.6150\n",
            "Epoch 42/300\n",
            "600/600 [==============================] - 0s 207us/step - loss: 0.6453 - acc: 0.6150\n",
            "Epoch 43/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.6448 - acc: 0.6100\n",
            "Epoch 44/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.6442 - acc: 0.6117\n",
            "Epoch 45/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.6437 - acc: 0.6133\n",
            "Epoch 46/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.6432 - acc: 0.6150\n",
            "Epoch 47/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.6427 - acc: 0.6150\n",
            "Epoch 48/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.6421 - acc: 0.6133\n",
            "Epoch 49/300\n",
            "600/600 [==============================] - 0s 168us/step - loss: 0.6415 - acc: 0.6150\n",
            "Epoch 50/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.6409 - acc: 0.6183\n",
            "Epoch 51/300\n",
            "600/600 [==============================] - 0s 203us/step - loss: 0.6404 - acc: 0.6217\n",
            "Epoch 52/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.6399 - acc: 0.6217\n",
            "Epoch 53/300\n",
            "600/600 [==============================] - 0s 168us/step - loss: 0.6394 - acc: 0.6217\n",
            "Epoch 54/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.6389 - acc: 0.6200\n",
            "Epoch 55/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.6383 - acc: 0.6200\n",
            "Epoch 56/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.6378 - acc: 0.6200\n",
            "Epoch 57/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.6374 - acc: 0.6200\n",
            "Epoch 58/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.6368 - acc: 0.6200\n",
            "Epoch 59/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.6363 - acc: 0.6200\n",
            "Epoch 60/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.6359 - acc: 0.6200\n",
            "Epoch 61/300\n",
            "600/600 [==============================] - 0s 210us/step - loss: 0.6354 - acc: 0.6217\n",
            "Epoch 62/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.6348 - acc: 0.6233\n",
            "Epoch 63/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.6344 - acc: 0.6250\n",
            "Epoch 64/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.6339 - acc: 0.6267\n",
            "Epoch 65/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.6334 - acc: 0.6267\n",
            "Epoch 66/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.6329 - acc: 0.6267\n",
            "Epoch 67/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.6325 - acc: 0.6283\n",
            "Epoch 68/300\n",
            "600/600 [==============================] - 0s 166us/step - loss: 0.6320 - acc: 0.6283\n",
            "Epoch 69/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.6315 - acc: 0.6283\n",
            "Epoch 70/300\n",
            "600/600 [==============================] - 0s 204us/step - loss: 0.6311 - acc: 0.6283\n",
            "Epoch 71/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.6306 - acc: 0.6283\n",
            "Epoch 72/300\n",
            "600/600 [==============================] - 0s 167us/step - loss: 0.6303 - acc: 0.6283\n",
            "Epoch 73/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.6297 - acc: 0.6283\n",
            "Epoch 74/300\n",
            "600/600 [==============================] - 0s 182us/step - loss: 0.6293 - acc: 0.6283\n",
            "Epoch 75/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.6288 - acc: 0.6300\n",
            "Epoch 76/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.6284 - acc: 0.6300\n",
            "Epoch 77/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.6279 - acc: 0.6300\n",
            "Epoch 78/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.6275 - acc: 0.6300\n",
            "Epoch 79/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.6271 - acc: 0.6300\n",
            "Epoch 80/300\n",
            "600/600 [==============================] - 0s 205us/step - loss: 0.6266 - acc: 0.6300\n",
            "Epoch 81/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.6262 - acc: 0.6300\n",
            "Epoch 82/300\n",
            "600/600 [==============================] - 0s 170us/step - loss: 0.6258 - acc: 0.6317\n",
            "Epoch 83/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.6254 - acc: 0.6317\n",
            "Epoch 84/300\n",
            "600/600 [==============================] - 0s 183us/step - loss: 0.6249 - acc: 0.6317\n",
            "Epoch 85/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.6245 - acc: 0.6317\n",
            "Epoch 86/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.6241 - acc: 0.6350\n",
            "Epoch 87/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.6237 - acc: 0.6350\n",
            "Epoch 88/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.6233 - acc: 0.6350\n",
            "Epoch 89/300\n",
            "600/600 [==============================] - 0s 205us/step - loss: 0.6229 - acc: 0.6367\n",
            "Epoch 90/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.6225 - acc: 0.6383\n",
            "Epoch 91/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.6221 - acc: 0.6383\n",
            "Epoch 92/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.6217 - acc: 0.6383\n",
            "Epoch 93/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.6213 - acc: 0.6400\n",
            "Epoch 94/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.6209 - acc: 0.6383\n",
            "Epoch 95/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.6205 - acc: 0.6400\n",
            "Epoch 96/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.6201 - acc: 0.6417\n",
            "Epoch 97/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.6197 - acc: 0.6400\n",
            "Epoch 98/300\n",
            "600/600 [==============================] - 0s 194us/step - loss: 0.6194 - acc: 0.6450\n",
            "Epoch 99/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.6190 - acc: 0.6450\n",
            "Epoch 100/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.6186 - acc: 0.6467\n",
            "Epoch 101/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.6183 - acc: 0.6467\n",
            "Epoch 102/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.6179 - acc: 0.6517\n",
            "Epoch 103/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.6175 - acc: 0.6517\n",
            "Epoch 104/300\n",
            "600/600 [==============================] - 0s 170us/step - loss: 0.6172 - acc: 0.6517\n",
            "Epoch 105/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.6169 - acc: 0.6550\n",
            "Epoch 106/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.6165 - acc: 0.6583\n",
            "Epoch 107/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.6161 - acc: 0.6567\n",
            "Epoch 108/300\n",
            "600/600 [==============================] - 0s 197us/step - loss: 0.6159 - acc: 0.6567\n",
            "Epoch 109/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.6155 - acc: 0.6617\n",
            "Epoch 110/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.6152 - acc: 0.6617\n",
            "Epoch 111/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.6147 - acc: 0.6600\n",
            "Epoch 112/300\n",
            "600/600 [==============================] - 0s 169us/step - loss: 0.6144 - acc: 0.6600\n",
            "Epoch 113/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.6141 - acc: 0.6617\n",
            "Epoch 114/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.6138 - acc: 0.6617\n",
            "Epoch 115/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.6134 - acc: 0.6617\n",
            "Epoch 116/300\n",
            "600/600 [==============================] - 0s 182us/step - loss: 0.6131 - acc: 0.6617\n",
            "Epoch 117/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.6128 - acc: 0.6633\n",
            "Epoch 118/300\n",
            "600/600 [==============================] - 0s 205us/step - loss: 0.6125 - acc: 0.6633\n",
            "Epoch 119/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.6122 - acc: 0.6633\n",
            "Epoch 120/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.6119 - acc: 0.6683\n",
            "Epoch 121/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.6115 - acc: 0.6683\n",
            "Epoch 122/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.6113 - acc: 0.6683\n",
            "Epoch 123/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.6110 - acc: 0.6717\n",
            "Epoch 124/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.6106 - acc: 0.6717\n",
            "Epoch 125/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.6103 - acc: 0.6700\n",
            "Epoch 126/300\n",
            "600/600 [==============================] - 0s 169us/step - loss: 0.6099 - acc: 0.6717\n",
            "Epoch 127/300\n",
            "600/600 [==============================] - 0s 201us/step - loss: 0.6096 - acc: 0.6717\n",
            "Epoch 128/300\n",
            "600/600 [==============================] - 0s 165us/step - loss: 0.6093 - acc: 0.6717\n",
            "Epoch 129/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.6090 - acc: 0.6717\n",
            "Epoch 130/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.6087 - acc: 0.6717\n",
            "Epoch 131/300\n",
            "600/600 [==============================] - 0s 182us/step - loss: 0.6084 - acc: 0.6717\n",
            "Epoch 132/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.6081 - acc: 0.6717\n",
            "Epoch 133/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.6078 - acc: 0.6717\n",
            "Epoch 134/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.6075 - acc: 0.6717\n",
            "Epoch 135/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.6072 - acc: 0.6717\n",
            "Epoch 136/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.6069 - acc: 0.6733\n",
            "Epoch 137/300\n",
            "600/600 [==============================] - 0s 196us/step - loss: 0.6066 - acc: 0.6733\n",
            "Epoch 138/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.6063 - acc: 0.6733\n",
            "Epoch 139/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.6060 - acc: 0.6733\n",
            "Epoch 140/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.6058 - acc: 0.6733\n",
            "Epoch 141/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.6055 - acc: 0.6733\n",
            "Epoch 142/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.6052 - acc: 0.6733\n",
            "Epoch 143/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.6049 - acc: 0.6733\n",
            "Epoch 144/300\n",
            "600/600 [==============================] - 0s 184us/step - loss: 0.6046 - acc: 0.6733\n",
            "Epoch 145/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.6044 - acc: 0.6733\n",
            "Epoch 146/300\n",
            "600/600 [==============================] - 0s 210us/step - loss: 0.6041 - acc: 0.6733\n",
            "Epoch 147/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.6038 - acc: 0.6733\n",
            "Epoch 148/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.6035 - acc: 0.6733\n",
            "Epoch 149/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.6033 - acc: 0.6733\n",
            "Epoch 150/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.6031 - acc: 0.6750\n",
            "Epoch 151/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.6028 - acc: 0.6733\n",
            "Epoch 152/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.6025 - acc: 0.6750\n",
            "Epoch 153/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.6022 - acc: 0.6767\n",
            "Epoch 154/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.6020 - acc: 0.6767\n",
            "Epoch 155/300\n",
            "600/600 [==============================] - 0s 204us/step - loss: 0.6017 - acc: 0.6783\n",
            "Epoch 156/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.6014 - acc: 0.6783\n",
            "Epoch 157/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.6012 - acc: 0.6783\n",
            "Epoch 158/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.6009 - acc: 0.6783\n",
            "Epoch 159/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.6007 - acc: 0.6783\n",
            "Epoch 160/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.6004 - acc: 0.6783\n",
            "Epoch 161/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.6002 - acc: 0.6800\n",
            "Epoch 162/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5999 - acc: 0.6783\n",
            "Epoch 163/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5996 - acc: 0.6800\n",
            "Epoch 164/300\n",
            "600/600 [==============================] - 0s 184us/step - loss: 0.5994 - acc: 0.6800\n",
            "Epoch 165/300\n",
            "600/600 [==============================] - 0s 202us/step - loss: 0.5992 - acc: 0.6800\n",
            "Epoch 166/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5989 - acc: 0.6783\n",
            "Epoch 167/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5987 - acc: 0.6800\n",
            "Epoch 168/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5984 - acc: 0.6800\n",
            "Epoch 169/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5982 - acc: 0.6800\n",
            "Epoch 170/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5980 - acc: 0.6800\n",
            "Epoch 171/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5978 - acc: 0.6800\n",
            "Epoch 172/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5975 - acc: 0.6783\n",
            "Epoch 173/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5973 - acc: 0.6833\n",
            "Epoch 174/300\n",
            "600/600 [==============================] - 0s 197us/step - loss: 0.5970 - acc: 0.6833\n",
            "Epoch 175/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5968 - acc: 0.6833\n",
            "Epoch 176/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5966 - acc: 0.6833\n",
            "Epoch 177/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5964 - acc: 0.6833\n",
            "Epoch 178/300\n",
            "600/600 [==============================] - 0s 169us/step - loss: 0.5961 - acc: 0.6883\n",
            "Epoch 179/300\n",
            "600/600 [==============================] - 0s 168us/step - loss: 0.5959 - acc: 0.6900\n",
            "Epoch 180/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.5957 - acc: 0.6917\n",
            "Epoch 181/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5955 - acc: 0.6917\n",
            "Epoch 182/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5953 - acc: 0.6900\n",
            "Epoch 183/300\n",
            "600/600 [==============================] - 0s 167us/step - loss: 0.5951 - acc: 0.6917\n",
            "Epoch 184/300\n",
            "600/600 [==============================] - 0s 202us/step - loss: 0.5948 - acc: 0.6917\n",
            "Epoch 185/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5947 - acc: 0.7000\n",
            "Epoch 186/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5944 - acc: 0.6967\n",
            "Epoch 187/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5942 - acc: 0.6950\n",
            "Epoch 188/300\n",
            "600/600 [==============================] - 0s 182us/step - loss: 0.5940 - acc: 0.6983\n",
            "Epoch 189/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5938 - acc: 0.6967\n",
            "Epoch 190/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.5936 - acc: 0.6983\n",
            "Epoch 191/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5934 - acc: 0.7000\n",
            "Epoch 192/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5931 - acc: 0.7000\n",
            "Epoch 193/300\n",
            "600/600 [==============================] - 0s 202us/step - loss: 0.5929 - acc: 0.7000\n",
            "Epoch 194/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5927 - acc: 0.7000\n",
            "Epoch 195/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5925 - acc: 0.7033\n",
            "Epoch 196/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5923 - acc: 0.7033\n",
            "Epoch 197/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5921 - acc: 0.7033\n",
            "Epoch 198/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5919 - acc: 0.7033\n",
            "Epoch 199/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5917 - acc: 0.7033\n",
            "Epoch 200/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5915 - acc: 0.7050\n",
            "Epoch 201/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5913 - acc: 0.7067\n",
            "Epoch 202/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5911 - acc: 0.7067\n",
            "Epoch 203/300\n",
            "600/600 [==============================] - 0s 189us/step - loss: 0.5909 - acc: 0.7067\n",
            "Epoch 204/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5907 - acc: 0.7067\n",
            "Epoch 205/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5905 - acc: 0.7067\n",
            "Epoch 206/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5903 - acc: 0.7067\n",
            "Epoch 207/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5902 - acc: 0.7067\n",
            "Epoch 208/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5900 - acc: 0.7067\n",
            "Epoch 209/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5898 - acc: 0.7067\n",
            "Epoch 210/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5896 - acc: 0.7067\n",
            "Epoch 211/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5894 - acc: 0.7067\n",
            "Epoch 212/300\n",
            "600/600 [==============================] - 0s 198us/step - loss: 0.5892 - acc: 0.7067\n",
            "Epoch 213/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5890 - acc: 0.7067\n",
            "Epoch 214/300\n",
            "600/600 [==============================] - 0s 183us/step - loss: 0.5888 - acc: 0.7067\n",
            "Epoch 215/300\n",
            "600/600 [==============================] - 0s 182us/step - loss: 0.5887 - acc: 0.7100\n",
            "Epoch 216/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.5885 - acc: 0.7067\n",
            "Epoch 217/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5883 - acc: 0.7083\n",
            "Epoch 218/300\n",
            "600/600 [==============================] - 0s 169us/step - loss: 0.5881 - acc: 0.7083\n",
            "Epoch 219/300\n",
            "600/600 [==============================] - 0s 169us/step - loss: 0.5879 - acc: 0.7100\n",
            "Epoch 220/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5878 - acc: 0.7100\n",
            "Epoch 221/300\n",
            "600/600 [==============================] - 0s 193us/step - loss: 0.5876 - acc: 0.7083\n",
            "Epoch 222/300\n",
            "600/600 [==============================] - 0s 169us/step - loss: 0.5874 - acc: 0.7083\n",
            "Epoch 223/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5872 - acc: 0.7083\n",
            "Epoch 224/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5871 - acc: 0.7083\n",
            "Epoch 225/300\n",
            "600/600 [==============================] - 0s 183us/step - loss: 0.5870 - acc: 0.7083\n",
            "Epoch 226/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5867 - acc: 0.7100\n",
            "Epoch 227/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5866 - acc: 0.7067\n",
            "Epoch 228/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5864 - acc: 0.7100\n",
            "Epoch 229/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5862 - acc: 0.7067\n",
            "Epoch 230/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5861 - acc: 0.7067\n",
            "Epoch 231/300\n",
            "600/600 [==============================] - 0s 203us/step - loss: 0.5859 - acc: 0.7067\n",
            "Epoch 232/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5857 - acc: 0.7067\n",
            "Epoch 233/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5855 - acc: 0.7067\n",
            "Epoch 234/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5854 - acc: 0.7067\n",
            "Epoch 235/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5852 - acc: 0.7067\n",
            "Epoch 236/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5851 - acc: 0.7067\n",
            "Epoch 237/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5849 - acc: 0.7067\n",
            "Epoch 238/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5847 - acc: 0.7067\n",
            "Epoch 239/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5847 - acc: 0.7067\n",
            "Epoch 240/300\n",
            "600/600 [==============================] - 0s 201us/step - loss: 0.5844 - acc: 0.7067\n",
            "Epoch 241/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5843 - acc: 0.7067\n",
            "Epoch 242/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5841 - acc: 0.7067\n",
            "Epoch 243/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5840 - acc: 0.7067\n",
            "Epoch 244/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5838 - acc: 0.7067\n",
            "Epoch 245/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5837 - acc: 0.7067\n",
            "Epoch 246/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5835 - acc: 0.7067\n",
            "Epoch 247/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5834 - acc: 0.7083\n",
            "Epoch 248/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5832 - acc: 0.7083\n",
            "Epoch 249/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5830 - acc: 0.7083\n",
            "Epoch 250/300\n",
            "600/600 [==============================] - 0s 199us/step - loss: 0.5829 - acc: 0.7083\n",
            "Epoch 251/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5828 - acc: 0.7067\n",
            "Epoch 252/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5827 - acc: 0.7083\n",
            "Epoch 253/300\n",
            "600/600 [==============================] - 0s 195us/step - loss: 0.5825 - acc: 0.7067\n",
            "Epoch 254/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5823 - acc: 0.7067\n",
            "Epoch 255/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5822 - acc: 0.7067\n",
            "Epoch 256/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5821 - acc: 0.7067\n",
            "Epoch 257/300\n",
            "600/600 [==============================] - 0s 169us/step - loss: 0.5819 - acc: 0.7067\n",
            "Epoch 258/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5817 - acc: 0.7067\n",
            "Epoch 259/300\n",
            "600/600 [==============================] - 0s 202us/step - loss: 0.5816 - acc: 0.7067\n",
            "Epoch 260/300\n",
            "600/600 [==============================] - 0s 169us/step - loss: 0.5815 - acc: 0.7067\n",
            "Epoch 261/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5813 - acc: 0.7083\n",
            "Epoch 262/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5812 - acc: 0.7067\n",
            "Epoch 263/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5811 - acc: 0.7067\n",
            "Epoch 264/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5809 - acc: 0.7083\n",
            "Epoch 265/300\n",
            "600/600 [==============================] - 0s 182us/step - loss: 0.5808 - acc: 0.7083\n",
            "Epoch 266/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5806 - acc: 0.7083\n",
            "Epoch 267/300\n",
            "600/600 [==============================] - 0s 186us/step - loss: 0.5805 - acc: 0.7083\n",
            "Epoch 268/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5804 - acc: 0.7100\n",
            "Epoch 269/300\n",
            "600/600 [==============================] - 0s 199us/step - loss: 0.5802 - acc: 0.7083\n",
            "Epoch 270/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5801 - acc: 0.7100\n",
            "Epoch 271/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.5801 - acc: 0.7100\n",
            "Epoch 272/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5798 - acc: 0.7083\n",
            "Epoch 273/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5797 - acc: 0.7083\n",
            "Epoch 274/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5795 - acc: 0.7100\n",
            "Epoch 275/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5794 - acc: 0.7083\n",
            "Epoch 276/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5793 - acc: 0.7100\n",
            "Epoch 277/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5792 - acc: 0.7100\n",
            "Epoch 278/300\n",
            "600/600 [==============================] - 0s 204us/step - loss: 0.5790 - acc: 0.7100\n",
            "Epoch 279/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5789 - acc: 0.7100\n",
            "Epoch 280/300\n",
            "600/600 [==============================] - 0s 186us/step - loss: 0.5788 - acc: 0.7100\n",
            "Epoch 281/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.5787 - acc: 0.7100\n",
            "Epoch 282/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5785 - acc: 0.7100\n",
            "Epoch 283/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5784 - acc: 0.7100\n",
            "Epoch 284/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5783 - acc: 0.7100\n",
            "Epoch 285/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5782 - acc: 0.7100\n",
            "Epoch 286/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5780 - acc: 0.7100\n",
            "Epoch 287/300\n",
            "600/600 [==============================] - 0s 201us/step - loss: 0.5780 - acc: 0.7117\n",
            "Epoch 288/300\n",
            "600/600 [==============================] - 0s 164us/step - loss: 0.5778 - acc: 0.7117\n",
            "Epoch 289/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5777 - acc: 0.7100\n",
            "Epoch 290/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5776 - acc: 0.7117\n",
            "Epoch 291/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5774 - acc: 0.7117\n",
            "Epoch 292/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5773 - acc: 0.7100\n",
            "Epoch 293/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5772 - acc: 0.7117\n",
            "Epoch 294/300\n",
            "600/600 [==============================] - 0s 169us/step - loss: 0.5771 - acc: 0.7117\n",
            "Epoch 295/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5770 - acc: 0.7117\n",
            "Epoch 296/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5769 - acc: 0.7117\n",
            "Epoch 297/300\n",
            "600/600 [==============================] - 0s 188us/step - loss: 0.5767 - acc: 0.7117\n",
            "Epoch 298/300\n",
            "600/600 [==============================] - 0s 168us/step - loss: 0.5766 - acc: 0.7117\n",
            "Epoch 299/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5765 - acc: 0.7117\n",
            "Epoch 300/300\n",
            "600/600 [==============================] - 0s 183us/step - loss: 0.5764 - acc: 0.7133\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb5b5921f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "cdaVUwLtLXG8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testando a rede"
      ]
    },
    {
      "metadata": {
        "id": "EFKoQkPXLajE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "newinputtest = transforminput(testparamnorm, kmean, 'rbfGaussiana')\n",
        "\n",
        "lifeprob = mod.predict(newinputtest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cj4CtgJYOlh0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Erro \n"
      ]
    },
    {
      "metadata": {
        "id": "02Vjczo4NWVG",
        "colab_type": "code",
        "outputId": "c6cf9ed5-74ee-4c68-d82a-62d88ddb5389",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "binpred = np.zeros((len(lifeprob)))\n",
        "for i in range(len(lifeprob)):\n",
        "  if lifeprob[i] > 0.5:\n",
        "    binpred[i] = 1.\n",
        "\n",
        "score = 0\n",
        "for i in range(len(testlabel)):\n",
        "  if binpred[i] == testlabel[i]:\n",
        "    score += 1\n",
        "\n",
        "errorabs = abs(score-len(lifeprob))\n",
        "print('error: ' , np.sum(errorabs/len(testlabel), axis=0))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error:  0.2719298245614035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_SjtdfoySm8q"
      },
      "cell_type": "markdown",
      "source": [
        "## Multiquadratica"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "3uxeZiY4Ti5U"
      },
      "cell_type": "markdown",
      "source": [
        "### K-means"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "6046995a-90ff-4b5b-fbbc-8cabf6ed0e0a",
        "id": "1DzSFlguTi5W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "cell_type": "code",
      "source": [
        "kmean = np.zeros((kval, len(trainparamnorm[0])))\n",
        "for i in range(kval):\n",
        "  for j in range(len(kmean[0])):\n",
        "    kmean[i,j] = random.uniform(min(trainparamnorm[:,j]),max(trainparamnorm[:,j]))\n",
        "    \n",
        "#looping of real algorithm\n",
        "distmin = np.zeros((len(trainparamnorm)))\n",
        "for i in range(itertot):\n",
        "  print('iterasi ke', i)\n",
        "  for j in range(len(distmin)):\n",
        "    #determine euclid distance\n",
        "    distall = np.sum((trainparamnorm[j] - kmean)**2.0, axis=1)**0.5\n",
        "    distmin[j] = np.argmin(distall)\n",
        "    \n",
        "#search new k mean\n",
        "for j in range(kval):\n",
        "  clust = []\n",
        "  for k in range(len(distmin)):\n",
        "    if distmin[k] == j:\n",
        "      clust.append(trainparamnorm[k])\n",
        "    if len(clust) > 0:\n",
        "      kmean[j] = np.mean(np.asarray(clust), axis=0)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iterasi ke 0\n",
            "iterasi ke 1\n",
            "iterasi ke 2\n",
            "iterasi ke 3\n",
            "iterasi ke 4\n",
            "iterasi ke 5\n",
            "iterasi ke 6\n",
            "iterasi ke 7\n",
            "iterasi ke 8\n",
            "iterasi ke 9\n",
            "iterasi ke 10\n",
            "iterasi ke 11\n",
            "iterasi ke 12\n",
            "iterasi ke 13\n",
            "iterasi ke 14\n",
            "iterasi ke 15\n",
            "iterasi ke 16\n",
            "iterasi ke 17\n",
            "iterasi ke 18\n",
            "iterasi ke 19\n",
            "iterasi ke 20\n",
            "iterasi ke 21\n",
            "iterasi ke 22\n",
            "iterasi ke 23\n",
            "iterasi ke 24\n",
            "iterasi ke 25\n",
            "iterasi ke 26\n",
            "iterasi ke 27\n",
            "iterasi ke 28\n",
            "iterasi ke 29\n",
            "iterasi ke 30\n",
            "iterasi ke 31\n",
            "iterasi ke 32\n",
            "iterasi ke 33\n",
            "iterasi ke 34\n",
            "iterasi ke 35\n",
            "iterasi ke 36\n",
            "iterasi ke 37\n",
            "iterasi ke 38\n",
            "iterasi ke 39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cCRfX4f3Sm8s"
      },
      "cell_type": "markdown",
      "source": [
        "### Treinando a rede"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "802b890d-d6aa-4174-ab24-994bb3d31e3a",
        "id": "pAg__vz4Sm8t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "cell_type": "code",
      "source": [
        "newinput = transforminput(trainparamnorm, kmean, 'rbfMultiquadratica')\n",
        "\n",
        "mod = generatemodel(kval)\n",
        "mod.fit(newinput, trainlabel, batch_size=20, epochs=itergd, verbose=1, shuffle=True)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "600/600 [==============================] - 0s 513us/step - loss: 1.1391 - acc: 0.4067\n",
            "Epoch 2/300\n",
            "600/600 [==============================] - 0s 201us/step - loss: 0.8401 - acc: 0.4017\n",
            "Epoch 3/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.7293 - acc: 0.4683\n",
            "Epoch 4/300\n",
            "600/600 [==============================] - 0s 195us/step - loss: 0.7006 - acc: 0.5683\n",
            "Epoch 5/300\n",
            "600/600 [==============================] - 0s 182us/step - loss: 0.6851 - acc: 0.5967\n",
            "Epoch 6/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.6718 - acc: 0.6233\n",
            "Epoch 7/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.6589 - acc: 0.6383\n",
            "Epoch 8/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.6464 - acc: 0.6617\n",
            "Epoch 9/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.6351 - acc: 0.6700\n",
            "Epoch 10/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.6244 - acc: 0.6933\n",
            "Epoch 11/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.6161 - acc: 0.7100\n",
            "Epoch 12/300\n",
            "600/600 [==============================] - 0s 201us/step - loss: 0.6059 - acc: 0.7067\n",
            "Epoch 13/300\n",
            "600/600 [==============================] - 0s 182us/step - loss: 0.5977 - acc: 0.7233\n",
            "Epoch 14/300\n",
            "600/600 [==============================] - 0s 183us/step - loss: 0.5900 - acc: 0.7350\n",
            "Epoch 15/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5836 - acc: 0.7367\n",
            "Epoch 16/300\n",
            "600/600 [==============================] - 0s 193us/step - loss: 0.5770 - acc: 0.7417\n",
            "Epoch 17/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5716 - acc: 0.7417\n",
            "Epoch 18/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5670 - acc: 0.7483\n",
            "Epoch 19/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5627 - acc: 0.7500\n",
            "Epoch 20/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5581 - acc: 0.7483\n",
            "Epoch 21/300\n",
            "600/600 [==============================] - 0s 198us/step - loss: 0.5545 - acc: 0.7517\n",
            "Epoch 22/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5514 - acc: 0.7517\n",
            "Epoch 23/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.5486 - acc: 0.7533\n",
            "Epoch 24/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5468 - acc: 0.7517\n",
            "Epoch 25/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5440 - acc: 0.7650\n",
            "Epoch 26/300\n",
            "600/600 [==============================] - 0s 168us/step - loss: 0.5414 - acc: 0.7650\n",
            "Epoch 27/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5396 - acc: 0.7617\n",
            "Epoch 28/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5381 - acc: 0.7617\n",
            "Epoch 29/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5369 - acc: 0.7717\n",
            "Epoch 30/300\n",
            "600/600 [==============================] - 0s 198us/step - loss: 0.5353 - acc: 0.7733\n",
            "Epoch 31/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5342 - acc: 0.7683\n",
            "Epoch 32/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5335 - acc: 0.7717\n",
            "Epoch 33/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5323 - acc: 0.7717\n",
            "Epoch 34/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5314 - acc: 0.7733\n",
            "Epoch 35/300\n",
            "600/600 [==============================] - 0s 170us/step - loss: 0.5311 - acc: 0.7733\n",
            "Epoch 36/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5302 - acc: 0.7750\n",
            "Epoch 37/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5299 - acc: 0.7733\n",
            "Epoch 38/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5296 - acc: 0.7750\n",
            "Epoch 39/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5287 - acc: 0.7750\n",
            "Epoch 40/300\n",
            "600/600 [==============================] - 0s 202us/step - loss: 0.5286 - acc: 0.7733\n",
            "Epoch 41/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5283 - acc: 0.7733\n",
            "Epoch 42/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5289 - acc: 0.7700\n",
            "Epoch 43/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5279 - acc: 0.7733\n",
            "Epoch 44/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5272 - acc: 0.7750\n",
            "Epoch 45/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5269 - acc: 0.7700\n",
            "Epoch 46/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5267 - acc: 0.7733\n",
            "Epoch 47/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5267 - acc: 0.7733\n",
            "Epoch 48/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5271 - acc: 0.7750\n",
            "Epoch 49/300\n",
            "600/600 [==============================] - 0s 195us/step - loss: 0.5259 - acc: 0.7750\n",
            "Epoch 50/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5262 - acc: 0.7733\n",
            "Epoch 51/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.5260 - acc: 0.7733\n",
            "Epoch 52/300\n",
            "600/600 [==============================] - 0s 187us/step - loss: 0.5266 - acc: 0.7700\n",
            "Epoch 53/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.5265 - acc: 0.7717\n",
            "Epoch 54/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5257 - acc: 0.7733\n",
            "Epoch 55/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5260 - acc: 0.7717\n",
            "Epoch 56/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5258 - acc: 0.7717\n",
            "Epoch 57/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5253 - acc: 0.7750\n",
            "Epoch 58/300\n",
            "600/600 [==============================] - 0s 170us/step - loss: 0.5258 - acc: 0.7700\n",
            "Epoch 59/300\n",
            "600/600 [==============================] - 0s 192us/step - loss: 0.5252 - acc: 0.7700\n",
            "Epoch 60/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.5253 - acc: 0.7700\n",
            "Epoch 61/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5253 - acc: 0.7750\n",
            "Epoch 62/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5254 - acc: 0.7733\n",
            "Epoch 63/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5250 - acc: 0.7700\n",
            "Epoch 64/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5250 - acc: 0.7733\n",
            "Epoch 65/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5258 - acc: 0.7700\n",
            "Epoch 66/300\n",
            "600/600 [==============================] - 0s 170us/step - loss: 0.5250 - acc: 0.7733\n",
            "Epoch 67/300\n",
            "600/600 [==============================] - 0s 167us/step - loss: 0.5260 - acc: 0.7717\n",
            "Epoch 68/300\n",
            "600/600 [==============================] - 0s 202us/step - loss: 0.5263 - acc: 0.7667\n",
            "Epoch 69/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5266 - acc: 0.7717\n",
            "Epoch 70/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5250 - acc: 0.7700\n",
            "Epoch 71/300\n",
            "600/600 [==============================] - 0s 170us/step - loss: 0.5251 - acc: 0.7717\n",
            "Epoch 72/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5263 - acc: 0.7733\n",
            "Epoch 73/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5260 - acc: 0.7717\n",
            "Epoch 74/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5246 - acc: 0.7717\n",
            "Epoch 75/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5249 - acc: 0.7733\n",
            "Epoch 76/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5250 - acc: 0.7750\n",
            "Epoch 77/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5265 - acc: 0.7683\n",
            "Epoch 78/300\n",
            "600/600 [==============================] - 0s 230us/step - loss: 0.5243 - acc: 0.7667\n",
            "Epoch 79/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5253 - acc: 0.7717\n",
            "Epoch 80/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5249 - acc: 0.7733\n",
            "Epoch 81/300\n",
            "600/600 [==============================] - 0s 202us/step - loss: 0.5249 - acc: 0.7733\n",
            "Epoch 82/300\n",
            "600/600 [==============================] - 0s 182us/step - loss: 0.5251 - acc: 0.7733\n",
            "Epoch 83/300\n",
            "600/600 [==============================] - 0s 185us/step - loss: 0.5255 - acc: 0.7650\n",
            "Epoch 84/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5251 - acc: 0.7733\n",
            "Epoch 85/300\n",
            "600/600 [==============================] - 0s 170us/step - loss: 0.5249 - acc: 0.7733\n",
            "Epoch 86/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5254 - acc: 0.7683\n",
            "Epoch 87/300\n",
            "600/600 [==============================] - 0s 205us/step - loss: 0.5253 - acc: 0.7700\n",
            "Epoch 88/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5254 - acc: 0.7733\n",
            "Epoch 89/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5252 - acc: 0.7733\n",
            "Epoch 90/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5252 - acc: 0.7733\n",
            "Epoch 91/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5255 - acc: 0.7733\n",
            "Epoch 92/300\n",
            "600/600 [==============================] - 0s 182us/step - loss: 0.5255 - acc: 0.7750\n",
            "Epoch 93/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5252 - acc: 0.7717\n",
            "Epoch 94/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5262 - acc: 0.7750\n",
            "Epoch 95/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5270 - acc: 0.7700\n",
            "Epoch 96/300\n",
            "600/600 [==============================] - 0s 189us/step - loss: 0.5267 - acc: 0.7750\n",
            "Epoch 97/300\n",
            "600/600 [==============================] - 0s 169us/step - loss: 0.5262 - acc: 0.7650\n",
            "Epoch 98/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5261 - acc: 0.7683\n",
            "Epoch 99/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5248 - acc: 0.7750\n",
            "Epoch 100/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5247 - acc: 0.7733\n",
            "Epoch 101/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5249 - acc: 0.7717\n",
            "Epoch 102/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5253 - acc: 0.7733\n",
            "Epoch 103/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5255 - acc: 0.7733\n",
            "Epoch 104/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5252 - acc: 0.7700\n",
            "Epoch 105/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5263 - acc: 0.7667\n",
            "Epoch 106/300\n",
            "600/600 [==============================] - 0s 196us/step - loss: 0.5255 - acc: 0.7717\n",
            "Epoch 107/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5246 - acc: 0.7733\n",
            "Epoch 108/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5250 - acc: 0.7767\n",
            "Epoch 109/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5248 - acc: 0.7750\n",
            "Epoch 110/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5245 - acc: 0.7700\n",
            "Epoch 111/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5250 - acc: 0.7717\n",
            "Epoch 112/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5250 - acc: 0.7717\n",
            "Epoch 113/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5245 - acc: 0.7717\n",
            "Epoch 114/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5251 - acc: 0.7750\n",
            "Epoch 115/300\n",
            "600/600 [==============================] - 0s 201us/step - loss: 0.5253 - acc: 0.7750\n",
            "Epoch 116/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5265 - acc: 0.7717\n",
            "Epoch 117/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5261 - acc: 0.7700\n",
            "Epoch 118/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5251 - acc: 0.7700\n",
            "Epoch 119/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5255 - acc: 0.7700\n",
            "Epoch 120/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5250 - acc: 0.7717\n",
            "Epoch 121/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5248 - acc: 0.7733\n",
            "Epoch 122/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5258 - acc: 0.7733\n",
            "Epoch 123/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5249 - acc: 0.7733\n",
            "Epoch 124/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5250 - acc: 0.7750\n",
            "Epoch 125/300\n",
            "600/600 [==============================] - 0s 196us/step - loss: 0.5251 - acc: 0.7750\n",
            "Epoch 126/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5254 - acc: 0.7733\n",
            "Epoch 127/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5250 - acc: 0.7717\n",
            "Epoch 128/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5251 - acc: 0.7750\n",
            "Epoch 129/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.5250 - acc: 0.7717\n",
            "Epoch 130/300\n",
            "600/600 [==============================] - 0s 198us/step - loss: 0.5252 - acc: 0.7750\n",
            "Epoch 131/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5257 - acc: 0.7667\n",
            "Epoch 132/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5251 - acc: 0.7733\n",
            "Epoch 133/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5253 - acc: 0.7733\n",
            "Epoch 134/300\n",
            "600/600 [==============================] - 0s 192us/step - loss: 0.5255 - acc: 0.7767\n",
            "Epoch 135/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5247 - acc: 0.7733\n",
            "Epoch 136/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5262 - acc: 0.7750\n",
            "Epoch 137/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5256 - acc: 0.7717\n",
            "Epoch 138/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5255 - acc: 0.7683\n",
            "Epoch 139/300\n",
            "600/600 [==============================] - 0s 170us/step - loss: 0.5251 - acc: 0.7750\n",
            "Epoch 140/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5259 - acc: 0.7700\n",
            "Epoch 141/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5261 - acc: 0.7750\n",
            "Epoch 142/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5248 - acc: 0.7767\n",
            "Epoch 143/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5259 - acc: 0.7683\n",
            "Epoch 144/300\n",
            "600/600 [==============================] - 0s 204us/step - loss: 0.5245 - acc: 0.7717\n",
            "Epoch 145/300\n",
            "600/600 [==============================] - 0s 169us/step - loss: 0.5257 - acc: 0.7767\n",
            "Epoch 146/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5275 - acc: 0.7733\n",
            "Epoch 147/300\n",
            "600/600 [==============================] - 0s 183us/step - loss: 0.5256 - acc: 0.7750\n",
            "Epoch 148/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5242 - acc: 0.7733\n",
            "Epoch 149/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5253 - acc: 0.7683\n",
            "Epoch 150/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5252 - acc: 0.7733\n",
            "Epoch 151/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5248 - acc: 0.7733\n",
            "Epoch 152/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5246 - acc: 0.7717\n",
            "Epoch 153/300\n",
            "600/600 [==============================] - 0s 198us/step - loss: 0.5249 - acc: 0.7750\n",
            "Epoch 154/300\n",
            "600/600 [==============================] - 0s 169us/step - loss: 0.5255 - acc: 0.7683\n",
            "Epoch 155/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5249 - acc: 0.7767\n",
            "Epoch 156/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5246 - acc: 0.7733\n",
            "Epoch 157/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5247 - acc: 0.7700\n",
            "Epoch 158/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5250 - acc: 0.7717\n",
            "Epoch 159/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5250 - acc: 0.7733\n",
            "Epoch 160/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5250 - acc: 0.7700\n",
            "Epoch 161/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5250 - acc: 0.7717\n",
            "Epoch 162/300\n",
            "600/600 [==============================] - 0s 184us/step - loss: 0.5251 - acc: 0.7700\n",
            "Epoch 163/300\n",
            "600/600 [==============================] - 0s 196us/step - loss: 0.5245 - acc: 0.7733\n",
            "Epoch 164/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5252 - acc: 0.7700\n",
            "Epoch 165/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5260 - acc: 0.7700\n",
            "Epoch 166/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5250 - acc: 0.7733\n",
            "Epoch 167/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5250 - acc: 0.7700\n",
            "Epoch 168/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5251 - acc: 0.7700\n",
            "Epoch 169/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5247 - acc: 0.7717\n",
            "Epoch 170/300\n",
            "600/600 [==============================] - 0s 182us/step - loss: 0.5254 - acc: 0.7767\n",
            "Epoch 171/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5251 - acc: 0.7750\n",
            "Epoch 172/300\n",
            "600/600 [==============================] - 0s 196us/step - loss: 0.5251 - acc: 0.7700\n",
            "Epoch 173/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5250 - acc: 0.7717\n",
            "Epoch 174/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5245 - acc: 0.7717\n",
            "Epoch 175/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5244 - acc: 0.7733\n",
            "Epoch 176/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5246 - acc: 0.7733\n",
            "Epoch 177/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5247 - acc: 0.7750\n",
            "Epoch 178/300\n",
            "600/600 [==============================] - 0s 182us/step - loss: 0.5250 - acc: 0.7700\n",
            "Epoch 179/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5254 - acc: 0.7733\n",
            "Epoch 180/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5258 - acc: 0.7667\n",
            "Epoch 181/300\n",
            "600/600 [==============================] - 0s 200us/step - loss: 0.5249 - acc: 0.7750\n",
            "Epoch 182/300\n",
            "600/600 [==============================] - 0s 168us/step - loss: 0.5246 - acc: 0.7717\n",
            "Epoch 183/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5247 - acc: 0.7717\n",
            "Epoch 184/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5249 - acc: 0.7733\n",
            "Epoch 185/300\n",
            "600/600 [==============================] - 0s 198us/step - loss: 0.5249 - acc: 0.7750\n",
            "Epoch 186/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5252 - acc: 0.7750\n",
            "Epoch 187/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5246 - acc: 0.7700\n",
            "Epoch 188/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5245 - acc: 0.7717\n",
            "Epoch 189/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5248 - acc: 0.7733\n",
            "Epoch 190/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5245 - acc: 0.7733\n",
            "Epoch 191/300\n",
            "600/600 [==============================] - 0s 208us/step - loss: 0.5246 - acc: 0.7750\n",
            "Epoch 192/300\n",
            "600/600 [==============================] - 0s 195us/step - loss: 0.5247 - acc: 0.7700\n",
            "Epoch 193/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5247 - acc: 0.7767\n",
            "Epoch 194/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5247 - acc: 0.7733\n",
            "Epoch 195/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5256 - acc: 0.7683\n",
            "Epoch 196/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5245 - acc: 0.7750\n",
            "Epoch 197/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5246 - acc: 0.7733\n",
            "Epoch 198/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5244 - acc: 0.7717\n",
            "Epoch 199/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5247 - acc: 0.7750\n",
            "Epoch 200/300\n",
            "600/600 [==============================] - 0s 203us/step - loss: 0.5257 - acc: 0.7683\n",
            "Epoch 201/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5248 - acc: 0.7717\n",
            "Epoch 202/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5262 - acc: 0.7733\n",
            "Epoch 203/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5243 - acc: 0.7733\n",
            "Epoch 204/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5248 - acc: 0.7700\n",
            "Epoch 205/300\n",
            "600/600 [==============================] - 0s 193us/step - loss: 0.5244 - acc: 0.7767\n",
            "Epoch 206/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5245 - acc: 0.7750\n",
            "Epoch 207/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5247 - acc: 0.7750\n",
            "Epoch 208/300\n",
            "600/600 [==============================] - 0s 168us/step - loss: 0.5245 - acc: 0.7767\n",
            "Epoch 209/300\n",
            "600/600 [==============================] - 0s 182us/step - loss: 0.5248 - acc: 0.7750\n",
            "Epoch 210/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5243 - acc: 0.7733\n",
            "Epoch 211/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5246 - acc: 0.7717\n",
            "Epoch 212/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5249 - acc: 0.7733\n",
            "Epoch 213/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5243 - acc: 0.7750\n",
            "Epoch 214/300\n",
            "600/600 [==============================] - 0s 166us/step - loss: 0.5265 - acc: 0.7733\n",
            "Epoch 215/300\n",
            "600/600 [==============================] - 0s 188us/step - loss: 0.5239 - acc: 0.7683\n",
            "Epoch 216/300\n",
            "600/600 [==============================] - 0s 182us/step - loss: 0.5251 - acc: 0.7750\n",
            "Epoch 217/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5246 - acc: 0.7667\n",
            "Epoch 218/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5248 - acc: 0.7717\n",
            "Epoch 219/300\n",
            "600/600 [==============================] - 0s 198us/step - loss: 0.5253 - acc: 0.7717\n",
            "Epoch 220/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5245 - acc: 0.7733\n",
            "Epoch 221/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5254 - acc: 0.7667\n",
            "Epoch 222/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5249 - acc: 0.7733\n",
            "Epoch 223/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5264 - acc: 0.7700\n",
            "Epoch 224/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5248 - acc: 0.7733\n",
            "Epoch 225/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5243 - acc: 0.7733\n",
            "Epoch 226/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5260 - acc: 0.7717\n",
            "Epoch 227/300\n",
            "600/600 [==============================] - 0s 195us/step - loss: 0.5246 - acc: 0.7717\n",
            "Epoch 228/300\n",
            "600/600 [==============================] - 0s 195us/step - loss: 0.5248 - acc: 0.7717\n",
            "Epoch 229/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5245 - acc: 0.7683\n",
            "Epoch 230/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5250 - acc: 0.7717\n",
            "Epoch 231/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5261 - acc: 0.7733\n",
            "Epoch 232/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5243 - acc: 0.7733\n",
            "Epoch 233/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5245 - acc: 0.7767\n",
            "Epoch 234/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5247 - acc: 0.7700\n",
            "Epoch 235/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5256 - acc: 0.7750\n",
            "Epoch 236/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5244 - acc: 0.7717\n",
            "Epoch 237/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5249 - acc: 0.7750\n",
            "Epoch 238/300\n",
            "600/600 [==============================] - 0s 204us/step - loss: 0.5252 - acc: 0.7700\n",
            "Epoch 239/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5248 - acc: 0.7700\n",
            "Epoch 240/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5254 - acc: 0.7733\n",
            "Epoch 241/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5246 - acc: 0.7767\n",
            "Epoch 242/300\n",
            "600/600 [==============================] - 0s 182us/step - loss: 0.5256 - acc: 0.7700\n",
            "Epoch 243/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5257 - acc: 0.7733\n",
            "Epoch 244/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5260 - acc: 0.7683\n",
            "Epoch 245/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5250 - acc: 0.7733\n",
            "Epoch 246/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5246 - acc: 0.7750\n",
            "Epoch 247/300\n",
            "600/600 [==============================] - 0s 197us/step - loss: 0.5252 - acc: 0.7683\n",
            "Epoch 248/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5243 - acc: 0.7700\n",
            "Epoch 249/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5248 - acc: 0.7750\n",
            "Epoch 250/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5244 - acc: 0.7683\n",
            "Epoch 251/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5241 - acc: 0.7750\n",
            "Epoch 252/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5244 - acc: 0.7750\n",
            "Epoch 253/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5244 - acc: 0.7733\n",
            "Epoch 254/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5251 - acc: 0.7750\n",
            "Epoch 255/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5254 - acc: 0.7733\n",
            "Epoch 256/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5243 - acc: 0.7717\n",
            "Epoch 257/300\n",
            "600/600 [==============================] - 0s 200us/step - loss: 0.5244 - acc: 0.7733\n",
            "Epoch 258/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5242 - acc: 0.7733\n",
            "Epoch 259/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5249 - acc: 0.7750\n",
            "Epoch 260/300\n",
            "600/600 [==============================] - 0s 170us/step - loss: 0.5248 - acc: 0.7683\n",
            "Epoch 261/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5244 - acc: 0.7733\n",
            "Epoch 262/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5247 - acc: 0.7733\n",
            "Epoch 263/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.5245 - acc: 0.7767\n",
            "Epoch 264/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5244 - acc: 0.7750\n",
            "Epoch 265/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5244 - acc: 0.7683\n",
            "Epoch 266/300\n",
            "600/600 [==============================] - 0s 187us/step - loss: 0.5249 - acc: 0.7683\n",
            "Epoch 267/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5246 - acc: 0.7717\n",
            "Epoch 268/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5244 - acc: 0.7733\n",
            "Epoch 269/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5246 - acc: 0.7733\n",
            "Epoch 270/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5242 - acc: 0.7750\n",
            "Epoch 271/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5246 - acc: 0.7750\n",
            "Epoch 272/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5251 - acc: 0.7700\n",
            "Epoch 273/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5241 - acc: 0.7750\n",
            "Epoch 274/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5242 - acc: 0.7733\n",
            "Epoch 275/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5241 - acc: 0.7717\n",
            "Epoch 276/300\n",
            "600/600 [==============================] - 0s 206us/step - loss: 0.5252 - acc: 0.7750\n",
            "Epoch 277/300\n",
            "600/600 [==============================] - 0s 168us/step - loss: 0.5251 - acc: 0.7717\n",
            "Epoch 278/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5271 - acc: 0.7683\n",
            "Epoch 279/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.5253 - acc: 0.7750\n",
            "Epoch 280/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5243 - acc: 0.7733\n",
            "Epoch 281/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5244 - acc: 0.7717\n",
            "Epoch 282/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5251 - acc: 0.7733\n",
            "Epoch 283/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5253 - acc: 0.7683\n",
            "Epoch 284/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5244 - acc: 0.7733\n",
            "Epoch 285/300\n",
            "600/600 [==============================] - 0s 205us/step - loss: 0.5243 - acc: 0.7700\n",
            "Epoch 286/300\n",
            "600/600 [==============================] - 0s 170us/step - loss: 0.5255 - acc: 0.7700\n",
            "Epoch 287/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5245 - acc: 0.7733\n",
            "Epoch 288/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5241 - acc: 0.7750\n",
            "Epoch 289/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5242 - acc: 0.7717\n",
            "Epoch 290/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.5243 - acc: 0.7700\n",
            "Epoch 291/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5251 - acc: 0.7750\n",
            "Epoch 292/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5243 - acc: 0.7700\n",
            "Epoch 293/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5243 - acc: 0.7750\n",
            "Epoch 294/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5259 - acc: 0.7683\n",
            "Epoch 295/300\n",
            "600/600 [==============================] - 0s 203us/step - loss: 0.5246 - acc: 0.7750\n",
            "Epoch 296/300\n",
            "600/600 [==============================] - 0s 169us/step - loss: 0.5244 - acc: 0.7733\n",
            "Epoch 297/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5245 - acc: 0.7733\n",
            "Epoch 298/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5245 - acc: 0.7717\n",
            "Epoch 299/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5241 - acc: 0.7717\n",
            "Epoch 300/300\n",
            "600/600 [==============================] - 0s 170us/step - loss: 0.5243 - acc: 0.7717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb628d7d240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "D52mm6-kSm8z"
      },
      "cell_type": "markdown",
      "source": [
        "### Testando a rede"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ACNcYz-mSm8z",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "newinputtest = transforminput(testparamnorm, kmean, 'rbfMultiquadratica')\n",
        "\n",
        "lifeprob = mod.predict(newinputtest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cKfUjreqSm82"
      },
      "cell_type": "markdown",
      "source": [
        "### Erro \n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "0536af23-df3a-45d7-9a0f-36fef341966e",
        "id": "mEihWe57Sm82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "binpred = np.zeros((len(lifeprob)))\n",
        "for i in range(len(lifeprob)):\n",
        "  if lifeprob[i] > 0.5:\n",
        "    binpred[i] = 1.\n",
        "\n",
        "score = 0\n",
        "for i in range(len(testlabel)):\n",
        "  if binpred[i] == testlabel[i]:\n",
        "    score += 1\n",
        "\n",
        "errorabs = abs(score-len(lifeprob))\n",
        "print('error: ' , np.sum(errorabs/len(testlabel), axis=0))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error:  0.24561403508771928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Gzf1OdLRS2n9"
      },
      "cell_type": "markdown",
      "source": [
        "## MultiquadraticaInversa"
      ]
    },
    {
      "metadata": {
        "id": "RDCDs1HFKlLg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### K-means"
      ]
    },
    {
      "metadata": {
        "id": "ASL3ZQPDKuI6",
        "colab_type": "code",
        "outputId": "d2e3e97a-ada7-4278-de25-7fb16b5b76ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "cell_type": "code",
      "source": [
        "kmean = np.zeros((kval, len(trainparamnorm[0])))\n",
        "for i in range(kval):\n",
        "  for j in range(len(kmean[0])):\n",
        "    kmean[i,j] = random.uniform(min(trainparamnorm[:,j]),max(trainparamnorm[:,j]))\n",
        "    \n",
        "#looping of real algorithm\n",
        "distmin = np.zeros((len(trainparamnorm)))\n",
        "for i in range(itertot):\n",
        "  print('iterasi ke', i)\n",
        "  for j in range(len(distmin)):\n",
        "    #determine euclid distance\n",
        "    distall = np.sum((trainparamnorm[j] - kmean)**2.0, axis=1)**0.5\n",
        "    distmin[j] = np.argmin(distall)\n",
        "    \n",
        "#search new k mean\n",
        "for j in range(kval):\n",
        "  clust = []\n",
        "  for k in range(len(distmin)):\n",
        "    if distmin[k] == j:\n",
        "      clust.append(trainparamnorm[k])\n",
        "    if len(clust) > 0:\n",
        "      kmean[j] = np.mean(np.asarray(clust), axis=0)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iterasi ke 0\n",
            "iterasi ke 1\n",
            "iterasi ke 2\n",
            "iterasi ke 3\n",
            "iterasi ke 4\n",
            "iterasi ke 5\n",
            "iterasi ke 6\n",
            "iterasi ke 7\n",
            "iterasi ke 8\n",
            "iterasi ke 9\n",
            "iterasi ke 10\n",
            "iterasi ke 11\n",
            "iterasi ke 12\n",
            "iterasi ke 13\n",
            "iterasi ke 14\n",
            "iterasi ke 15\n",
            "iterasi ke 16\n",
            "iterasi ke 17\n",
            "iterasi ke 18\n",
            "iterasi ke 19\n",
            "iterasi ke 20\n",
            "iterasi ke 21\n",
            "iterasi ke 22\n",
            "iterasi ke 23\n",
            "iterasi ke 24\n",
            "iterasi ke 25\n",
            "iterasi ke 26\n",
            "iterasi ke 27\n",
            "iterasi ke 28\n",
            "iterasi ke 29\n",
            "iterasi ke 30\n",
            "iterasi ke 31\n",
            "iterasi ke 32\n",
            "iterasi ke 33\n",
            "iterasi ke 34\n",
            "iterasi ke 35\n",
            "iterasi ke 36\n",
            "iterasi ke 37\n",
            "iterasi ke 38\n",
            "iterasi ke 39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7aN7t-xVUJ1H"
      },
      "cell_type": "markdown",
      "source": [
        "### Treinando a rede"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "fbf90246-ae64-4710-bf83-fe9cf8ee37d8",
        "id": "nLjf_H97UJ1L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "cell_type": "code",
      "source": [
        "newinput = transforminput(trainparamnorm, kmean, 'rbfMultiquadraticaInversa')\n",
        "\n",
        "mod = generatemodel(kval)\n",
        "mod.fit(newinput, trainlabel, batch_size=20, epochs=itergd, verbose=1, shuffle=True)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "600/600 [==============================] - 0s 574us/step - loss: 0.7458 - acc: 0.4067\n",
            "Epoch 2/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.7183 - acc: 0.4067\n",
            "Epoch 3/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.6971 - acc: 0.4067\n",
            "Epoch 4/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.6797 - acc: 0.4717\n",
            "Epoch 5/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.6666 - acc: 0.6317\n",
            "Epoch 6/300\n",
            "600/600 [==============================] - 0s 184us/step - loss: 0.6560 - acc: 0.7417\n",
            "Epoch 7/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.6479 - acc: 0.7733\n",
            "Epoch 8/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.6417 - acc: 0.7517\n",
            "Epoch 9/300\n",
            "600/600 [==============================] - 0s 222us/step - loss: 0.6373 - acc: 0.7433\n",
            "Epoch 10/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.6327 - acc: 0.7367\n",
            "Epoch 11/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.6294 - acc: 0.7233\n",
            "Epoch 12/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.6264 - acc: 0.7167\n",
            "Epoch 13/300\n",
            "600/600 [==============================] - 0s 182us/step - loss: 0.6239 - acc: 0.7150\n",
            "Epoch 14/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.6215 - acc: 0.7017\n",
            "Epoch 15/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.6194 - acc: 0.7100\n",
            "Epoch 16/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.6176 - acc: 0.6967\n",
            "Epoch 17/300\n",
            "600/600 [==============================] - 0s 184us/step - loss: 0.6155 - acc: 0.7017\n",
            "Epoch 18/300\n",
            "600/600 [==============================] - 0s 215us/step - loss: 0.6137 - acc: 0.6983\n",
            "Epoch 19/300\n",
            "600/600 [==============================] - 0s 169us/step - loss: 0.6119 - acc: 0.7050\n",
            "Epoch 20/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.6101 - acc: 0.7083\n",
            "Epoch 21/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.6085 - acc: 0.7183\n",
            "Epoch 22/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.6068 - acc: 0.7217\n",
            "Epoch 23/300\n",
            "600/600 [==============================] - 0s 170us/step - loss: 0.6053 - acc: 0.7183\n",
            "Epoch 24/300\n",
            "600/600 [==============================] - 0s 170us/step - loss: 0.6037 - acc: 0.7200\n",
            "Epoch 25/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.6022 - acc: 0.7217\n",
            "Epoch 26/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.6007 - acc: 0.7233\n",
            "Epoch 27/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5993 - acc: 0.7233\n",
            "Epoch 28/300\n",
            "600/600 [==============================] - 0s 205us/step - loss: 0.5980 - acc: 0.7233\n",
            "Epoch 29/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5963 - acc: 0.7267\n",
            "Epoch 30/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5951 - acc: 0.7300\n",
            "Epoch 31/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5939 - acc: 0.7283\n",
            "Epoch 32/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5925 - acc: 0.7300\n",
            "Epoch 33/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5913 - acc: 0.7333\n",
            "Epoch 34/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5900 - acc: 0.7283\n",
            "Epoch 35/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5889 - acc: 0.7333\n",
            "Epoch 36/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5877 - acc: 0.7417\n",
            "Epoch 37/300\n",
            "600/600 [==============================] - 0s 200us/step - loss: 0.5865 - acc: 0.7417\n",
            "Epoch 38/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5854 - acc: 0.7433\n",
            "Epoch 39/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5845 - acc: 0.7417\n",
            "Epoch 40/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5833 - acc: 0.7450\n",
            "Epoch 41/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5822 - acc: 0.7433\n",
            "Epoch 42/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5813 - acc: 0.7433\n",
            "Epoch 43/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5802 - acc: 0.7433\n",
            "Epoch 44/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5792 - acc: 0.7433\n",
            "Epoch 45/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5784 - acc: 0.7433\n",
            "Epoch 46/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5773 - acc: 0.7433\n",
            "Epoch 47/300\n",
            "600/600 [==============================] - 0s 198us/step - loss: 0.5765 - acc: 0.7467\n",
            "Epoch 48/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5756 - acc: 0.7467\n",
            "Epoch 49/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5748 - acc: 0.7483\n",
            "Epoch 50/300\n",
            "600/600 [==============================] - 0s 170us/step - loss: 0.5738 - acc: 0.7500\n",
            "Epoch 51/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5731 - acc: 0.7500\n",
            "Epoch 52/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5722 - acc: 0.7500\n",
            "Epoch 53/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5716 - acc: 0.7500\n",
            "Epoch 54/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5710 - acc: 0.7517\n",
            "Epoch 55/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5700 - acc: 0.7500\n",
            "Epoch 56/300\n",
            "600/600 [==============================] - 0s 197us/step - loss: 0.5692 - acc: 0.7500\n",
            "Epoch 57/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5685 - acc: 0.7500\n",
            "Epoch 58/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.5678 - acc: 0.7500\n",
            "Epoch 59/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5671 - acc: 0.7517\n",
            "Epoch 60/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5667 - acc: 0.7500\n",
            "Epoch 61/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5660 - acc: 0.7533\n",
            "Epoch 62/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5651 - acc: 0.7517\n",
            "Epoch 63/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5645 - acc: 0.7517\n",
            "Epoch 64/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5639 - acc: 0.7533\n",
            "Epoch 65/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5634 - acc: 0.7533\n",
            "Epoch 66/300\n",
            "600/600 [==============================] - 0s 204us/step - loss: 0.5628 - acc: 0.7567\n",
            "Epoch 67/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5622 - acc: 0.7550\n",
            "Epoch 68/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5618 - acc: 0.7583\n",
            "Epoch 69/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5613 - acc: 0.7550\n",
            "Epoch 70/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5606 - acc: 0.7583\n",
            "Epoch 71/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5602 - acc: 0.7567\n",
            "Epoch 72/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5598 - acc: 0.7600\n",
            "Epoch 73/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5592 - acc: 0.7600\n",
            "Epoch 74/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5587 - acc: 0.7600\n",
            "Epoch 75/300\n",
            "600/600 [==============================] - 0s 196us/step - loss: 0.5582 - acc: 0.7583\n",
            "Epoch 76/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5577 - acc: 0.7600\n",
            "Epoch 77/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.5574 - acc: 0.7583\n",
            "Epoch 78/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5569 - acc: 0.7600\n",
            "Epoch 79/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5564 - acc: 0.7583\n",
            "Epoch 80/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5562 - acc: 0.7600\n",
            "Epoch 81/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5557 - acc: 0.7583\n",
            "Epoch 82/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5552 - acc: 0.7583\n",
            "Epoch 83/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5550 - acc: 0.7583\n",
            "Epoch 84/300\n",
            "600/600 [==============================] - 0s 197us/step - loss: 0.5545 - acc: 0.7583\n",
            "Epoch 85/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5541 - acc: 0.7583\n",
            "Epoch 86/300\n",
            "600/600 [==============================] - 0s 202us/step - loss: 0.5538 - acc: 0.7583\n",
            "Epoch 87/300\n",
            "600/600 [==============================] - 0s 182us/step - loss: 0.5535 - acc: 0.7583\n",
            "Epoch 88/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5531 - acc: 0.7583\n",
            "Epoch 89/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5527 - acc: 0.7600\n",
            "Epoch 90/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5524 - acc: 0.7600\n",
            "Epoch 91/300\n",
            "600/600 [==============================] - 0s 196us/step - loss: 0.5521 - acc: 0.7617\n",
            "Epoch 92/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5517 - acc: 0.7633\n",
            "Epoch 93/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5514 - acc: 0.7633\n",
            "Epoch 94/300\n",
            "600/600 [==============================] - 0s 204us/step - loss: 0.5513 - acc: 0.7633\n",
            "Epoch 95/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5508 - acc: 0.7633\n",
            "Epoch 96/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5506 - acc: 0.7633\n",
            "Epoch 97/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5503 - acc: 0.7633\n",
            "Epoch 98/300\n",
            "600/600 [==============================] - 0s 165us/step - loss: 0.5500 - acc: 0.7633\n",
            "Epoch 99/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5497 - acc: 0.7617\n",
            "Epoch 100/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5495 - acc: 0.7633\n",
            "Epoch 101/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5494 - acc: 0.7650\n",
            "Epoch 102/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5490 - acc: 0.7650\n",
            "Epoch 103/300\n",
            "600/600 [==============================] - 0s 197us/step - loss: 0.5487 - acc: 0.7633\n",
            "Epoch 104/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5484 - acc: 0.7633\n",
            "Epoch 105/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5483 - acc: 0.7633\n",
            "Epoch 106/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5480 - acc: 0.7633\n",
            "Epoch 107/300\n",
            "600/600 [==============================] - 0s 170us/step - loss: 0.5479 - acc: 0.7633\n",
            "Epoch 108/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5476 - acc: 0.7633\n",
            "Epoch 109/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5473 - acc: 0.7633\n",
            "Epoch 110/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.5472 - acc: 0.7633\n",
            "Epoch 111/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5470 - acc: 0.7633\n",
            "Epoch 112/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5467 - acc: 0.7633\n",
            "Epoch 113/300\n",
            "600/600 [==============================] - 0s 200us/step - loss: 0.5465 - acc: 0.7633\n",
            "Epoch 114/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5463 - acc: 0.7633\n",
            "Epoch 115/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5461 - acc: 0.7650\n",
            "Epoch 116/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5459 - acc: 0.7650\n",
            "Epoch 117/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5458 - acc: 0.7633\n",
            "Epoch 118/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5457 - acc: 0.7650\n",
            "Epoch 119/300\n",
            "600/600 [==============================] - 0s 182us/step - loss: 0.5454 - acc: 0.7650\n",
            "Epoch 120/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5452 - acc: 0.7650\n",
            "Epoch 121/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5452 - acc: 0.7650\n",
            "Epoch 122/300\n",
            "600/600 [==============================] - 0s 205us/step - loss: 0.5449 - acc: 0.7650\n",
            "Epoch 123/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5448 - acc: 0.7633\n",
            "Epoch 124/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5446 - acc: 0.7650\n",
            "Epoch 125/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5444 - acc: 0.7650\n",
            "Epoch 126/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5444 - acc: 0.7650\n",
            "Epoch 127/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5443 - acc: 0.7650\n",
            "Epoch 128/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5440 - acc: 0.7667\n",
            "Epoch 129/300\n",
            "600/600 [==============================] - 0s 170us/step - loss: 0.5439 - acc: 0.7650\n",
            "Epoch 130/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5441 - acc: 0.7700\n",
            "Epoch 131/300\n",
            "600/600 [==============================] - 0s 182us/step - loss: 0.5441 - acc: 0.7633\n",
            "Epoch 132/300\n",
            "600/600 [==============================] - 0s 183us/step - loss: 0.5435 - acc: 0.7667\n",
            "Epoch 133/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5433 - acc: 0.7667\n",
            "Epoch 134/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5431 - acc: 0.7650\n",
            "Epoch 135/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5431 - acc: 0.7683\n",
            "Epoch 136/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5429 - acc: 0.7683\n",
            "Epoch 137/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5428 - acc: 0.7700\n",
            "Epoch 138/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5428 - acc: 0.7650\n",
            "Epoch 139/300\n",
            "600/600 [==============================] - 0s 169us/step - loss: 0.5427 - acc: 0.7667\n",
            "Epoch 140/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5425 - acc: 0.7683\n",
            "Epoch 141/300\n",
            "600/600 [==============================] - 0s 198us/step - loss: 0.5423 - acc: 0.7700\n",
            "Epoch 142/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5422 - acc: 0.7700\n",
            "Epoch 143/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5422 - acc: 0.7683\n",
            "Epoch 144/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5421 - acc: 0.7700\n",
            "Epoch 145/300\n",
            "600/600 [==============================] - 0s 183us/step - loss: 0.5420 - acc: 0.7683\n",
            "Epoch 146/300\n",
            "600/600 [==============================] - 0s 182us/step - loss: 0.5418 - acc: 0.7700\n",
            "Epoch 147/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5417 - acc: 0.7683\n",
            "Epoch 148/300\n",
            "600/600 [==============================] - 0s 170us/step - loss: 0.5416 - acc: 0.7683\n",
            "Epoch 149/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5414 - acc: 0.7667\n",
            "Epoch 150/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5414 - acc: 0.7700\n",
            "Epoch 151/300\n",
            "600/600 [==============================] - 0s 204us/step - loss: 0.5413 - acc: 0.7700\n",
            "Epoch 152/300\n",
            "600/600 [==============================] - 0s 166us/step - loss: 0.5412 - acc: 0.7700\n",
            "Epoch 153/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5412 - acc: 0.7700\n",
            "Epoch 154/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5413 - acc: 0.7683\n",
            "Epoch 155/300\n",
            "600/600 [==============================] - 0s 196us/step - loss: 0.5410 - acc: 0.7667\n",
            "Epoch 156/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5409 - acc: 0.7700\n",
            "Epoch 157/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5407 - acc: 0.7700\n",
            "Epoch 158/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5411 - acc: 0.7700\n",
            "Epoch 159/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5407 - acc: 0.7700\n",
            "Epoch 160/300\n",
            "600/600 [==============================] - 0s 206us/step - loss: 0.5406 - acc: 0.7700\n",
            "Epoch 161/300\n",
            "600/600 [==============================] - 0s 193us/step - loss: 0.5405 - acc: 0.7700\n",
            "Epoch 162/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5404 - acc: 0.7683\n",
            "Epoch 163/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5403 - acc: 0.7700\n",
            "Epoch 164/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5402 - acc: 0.7683\n",
            "Epoch 165/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5401 - acc: 0.7700\n",
            "Epoch 166/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5400 - acc: 0.7700\n",
            "Epoch 167/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5401 - acc: 0.7683\n",
            "Epoch 168/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5400 - acc: 0.7700\n",
            "Epoch 169/300\n",
            "600/600 [==============================] - 0s 205us/step - loss: 0.5398 - acc: 0.7700\n",
            "Epoch 170/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5399 - acc: 0.7700\n",
            "Epoch 171/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5398 - acc: 0.7683\n",
            "Epoch 172/300\n",
            "600/600 [==============================] - 0s 170us/step - loss: 0.5396 - acc: 0.7683\n",
            "Epoch 173/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5397 - acc: 0.7700\n",
            "Epoch 174/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5395 - acc: 0.7683\n",
            "Epoch 175/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5394 - acc: 0.7700\n",
            "Epoch 176/300\n",
            "600/600 [==============================] - 0s 182us/step - loss: 0.5395 - acc: 0.7683\n",
            "Epoch 177/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5393 - acc: 0.7683\n",
            "Epoch 178/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5393 - acc: 0.7683\n",
            "Epoch 179/300\n",
            "600/600 [==============================] - 0s 206us/step - loss: 0.5392 - acc: 0.7683\n",
            "Epoch 180/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5391 - acc: 0.7700\n",
            "Epoch 181/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5390 - acc: 0.7683\n",
            "Epoch 182/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5390 - acc: 0.7683\n",
            "Epoch 183/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5390 - acc: 0.7700\n",
            "Epoch 184/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5389 - acc: 0.7700\n",
            "Epoch 185/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5388 - acc: 0.7683\n",
            "Epoch 186/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5387 - acc: 0.7683\n",
            "Epoch 187/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5389 - acc: 0.7683\n",
            "Epoch 188/300\n",
            "600/600 [==============================] - 0s 205us/step - loss: 0.5386 - acc: 0.7717\n",
            "Epoch 189/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5385 - acc: 0.7683\n",
            "Epoch 190/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5386 - acc: 0.7683\n",
            "Epoch 191/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5385 - acc: 0.7700\n",
            "Epoch 192/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5385 - acc: 0.7683\n",
            "Epoch 193/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5384 - acc: 0.7700\n",
            "Epoch 194/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5384 - acc: 0.7700\n",
            "Epoch 195/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5382 - acc: 0.7683\n",
            "Epoch 196/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5382 - acc: 0.7700\n",
            "Epoch 197/300\n",
            "600/600 [==============================] - 0s 183us/step - loss: 0.5382 - acc: 0.7700\n",
            "Epoch 198/300\n",
            "600/600 [==============================] - 0s 189us/step - loss: 0.5381 - acc: 0.7700\n",
            "Epoch 199/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5380 - acc: 0.7700\n",
            "Epoch 200/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5381 - acc: 0.7683\n",
            "Epoch 201/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.5381 - acc: 0.7700\n",
            "Epoch 202/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5379 - acc: 0.7700\n",
            "Epoch 203/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5379 - acc: 0.7700\n",
            "Epoch 204/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5378 - acc: 0.7683\n",
            "Epoch 205/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5377 - acc: 0.7700\n",
            "Epoch 206/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5377 - acc: 0.7700\n",
            "Epoch 207/300\n",
            "600/600 [==============================] - 0s 202us/step - loss: 0.5376 - acc: 0.7700\n",
            "Epoch 208/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5380 - acc: 0.7700\n",
            "Epoch 209/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5376 - acc: 0.7700\n",
            "Epoch 210/300\n",
            "600/600 [==============================] - 0s 183us/step - loss: 0.5376 - acc: 0.7700\n",
            "Epoch 211/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5377 - acc: 0.7683\n",
            "Epoch 212/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5376 - acc: 0.7700\n",
            "Epoch 213/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5375 - acc: 0.7717\n",
            "Epoch 214/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.5374 - acc: 0.7683\n",
            "Epoch 215/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5376 - acc: 0.7717\n",
            "Epoch 216/300\n",
            "600/600 [==============================] - 0s 202us/step - loss: 0.5372 - acc: 0.7700\n",
            "Epoch 217/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5373 - acc: 0.7700\n",
            "Epoch 218/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5372 - acc: 0.7683\n",
            "Epoch 219/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5373 - acc: 0.7700\n",
            "Epoch 220/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5371 - acc: 0.7700\n",
            "Epoch 221/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5371 - acc: 0.7700\n",
            "Epoch 222/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5371 - acc: 0.7700\n",
            "Epoch 223/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5370 - acc: 0.7683\n",
            "Epoch 224/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5370 - acc: 0.7700\n",
            "Epoch 225/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5370 - acc: 0.7700\n",
            "Epoch 226/300\n",
            "600/600 [==============================] - 0s 202us/step - loss: 0.5370 - acc: 0.7683\n",
            "Epoch 227/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5368 - acc: 0.7683\n",
            "Epoch 228/300\n",
            "600/600 [==============================] - 0s 183us/step - loss: 0.5368 - acc: 0.7700\n",
            "Epoch 229/300\n",
            "600/600 [==============================] - 0s 188us/step - loss: 0.5371 - acc: 0.7700\n",
            "Epoch 230/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5367 - acc: 0.7700\n",
            "Epoch 231/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5367 - acc: 0.7683\n",
            "Epoch 232/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.5367 - acc: 0.7700\n",
            "Epoch 233/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5366 - acc: 0.7700\n",
            "Epoch 234/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5366 - acc: 0.7700\n",
            "Epoch 235/300\n",
            "600/600 [==============================] - 0s 197us/step - loss: 0.5366 - acc: 0.7700\n",
            "Epoch 236/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5366 - acc: 0.7700\n",
            "Epoch 237/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.5365 - acc: 0.7700\n",
            "Epoch 238/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5364 - acc: 0.7700\n",
            "Epoch 239/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5364 - acc: 0.7700\n",
            "Epoch 240/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5364 - acc: 0.7700\n",
            "Epoch 241/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5364 - acc: 0.7700\n",
            "Epoch 242/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5363 - acc: 0.7700\n",
            "Epoch 243/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5364 - acc: 0.7700\n",
            "Epoch 244/300\n",
            "600/600 [==============================] - 0s 202us/step - loss: 0.5362 - acc: 0.7700\n",
            "Epoch 245/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5363 - acc: 0.7700\n",
            "Epoch 246/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5362 - acc: 0.7700\n",
            "Epoch 247/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5361 - acc: 0.7700\n",
            "Epoch 248/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5362 - acc: 0.7700\n",
            "Epoch 249/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5363 - acc: 0.7700\n",
            "Epoch 250/300\n",
            "600/600 [==============================] - 0s 183us/step - loss: 0.5360 - acc: 0.7683\n",
            "Epoch 251/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.5360 - acc: 0.7700\n",
            "Epoch 252/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5361 - acc: 0.7683\n",
            "Epoch 253/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5361 - acc: 0.7700\n",
            "Epoch 254/300\n",
            "600/600 [==============================] - 0s 203us/step - loss: 0.5359 - acc: 0.7700\n",
            "Epoch 255/300\n",
            "600/600 [==============================] - 0s 173us/step - loss: 0.5360 - acc: 0.7700\n",
            "Epoch 256/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5359 - acc: 0.7700\n",
            "Epoch 257/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5359 - acc: 0.7700\n",
            "Epoch 258/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5358 - acc: 0.7700\n",
            "Epoch 259/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5358 - acc: 0.7700\n",
            "Epoch 260/300\n",
            "600/600 [==============================] - 0s 183us/step - loss: 0.5359 - acc: 0.7700\n",
            "Epoch 261/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5358 - acc: 0.7683\n",
            "Epoch 262/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5357 - acc: 0.7700\n",
            "Epoch 263/300\n",
            "600/600 [==============================] - 0s 201us/step - loss: 0.5357 - acc: 0.7700\n",
            "Epoch 264/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5356 - acc: 0.7700\n",
            "Epoch 265/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5356 - acc: 0.7683\n",
            "Epoch 266/300\n",
            "600/600 [==============================] - 0s 181us/step - loss: 0.5357 - acc: 0.7683\n",
            "Epoch 267/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5356 - acc: 0.7700\n",
            "Epoch 268/300\n",
            "600/600 [==============================] - 0s 186us/step - loss: 0.5355 - acc: 0.7700\n",
            "Epoch 269/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5355 - acc: 0.7700\n",
            "Epoch 270/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5356 - acc: 0.7683\n",
            "Epoch 271/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5356 - acc: 0.7700\n",
            "Epoch 272/300\n",
            "600/600 [==============================] - 0s 200us/step - loss: 0.5354 - acc: 0.7700\n",
            "Epoch 273/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5356 - acc: 0.7700\n",
            "Epoch 274/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5355 - acc: 0.7683\n",
            "Epoch 275/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5354 - acc: 0.7700\n",
            "Epoch 276/300\n",
            "600/600 [==============================] - 0s 183us/step - loss: 0.5353 - acc: 0.7700\n",
            "Epoch 277/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5355 - acc: 0.7700\n",
            "Epoch 278/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5353 - acc: 0.7683\n",
            "Epoch 279/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5354 - acc: 0.7700\n",
            "Epoch 280/300\n",
            "600/600 [==============================] - 0s 172us/step - loss: 0.5353 - acc: 0.7700\n",
            "Epoch 281/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5352 - acc: 0.7683\n",
            "Epoch 282/300\n",
            "600/600 [==============================] - 0s 204us/step - loss: 0.5352 - acc: 0.7683\n",
            "Epoch 283/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5353 - acc: 0.7700\n",
            "Epoch 284/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5351 - acc: 0.7700\n",
            "Epoch 285/300\n",
            "600/600 [==============================] - 0s 177us/step - loss: 0.5351 - acc: 0.7683\n",
            "Epoch 286/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5352 - acc: 0.7717\n",
            "Epoch 287/300\n",
            "600/600 [==============================] - 0s 184us/step - loss: 0.5350 - acc: 0.7700\n",
            "Epoch 288/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5350 - acc: 0.7700\n",
            "Epoch 289/300\n",
            "600/600 [==============================] - 0s 176us/step - loss: 0.5350 - acc: 0.7700\n",
            "Epoch 290/300\n",
            "600/600 [==============================] - 0s 179us/step - loss: 0.5349 - acc: 0.7683\n",
            "Epoch 291/300\n",
            "600/600 [==============================] - 0s 199us/step - loss: 0.5350 - acc: 0.7683\n",
            "Epoch 292/300\n",
            "600/600 [==============================] - 0s 174us/step - loss: 0.5351 - acc: 0.7700\n",
            "Epoch 293/300\n",
            "600/600 [==============================] - 0s 178us/step - loss: 0.5349 - acc: 0.7700\n",
            "Epoch 294/300\n",
            "600/600 [==============================] - 0s 175us/step - loss: 0.5349 - acc: 0.7683\n",
            "Epoch 295/300\n",
            "600/600 [==============================] - 0s 180us/step - loss: 0.5349 - acc: 0.7683\n",
            "Epoch 296/300\n",
            "600/600 [==============================] - 0s 182us/step - loss: 0.5350 - acc: 0.7700\n",
            "Epoch 297/300\n",
            "600/600 [==============================] - 0s 170us/step - loss: 0.5349 - acc: 0.7700\n",
            "Epoch 298/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5348 - acc: 0.7700\n",
            "Epoch 299/300\n",
            "600/600 [==============================] - 0s 171us/step - loss: 0.5347 - acc: 0.7683\n",
            "Epoch 300/300\n",
            "600/600 [==============================] - 0s 195us/step - loss: 0.5348 - acc: 0.7700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb5b47ece10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Ikr7AEadUJ1Q"
      },
      "cell_type": "markdown",
      "source": [
        "### Testando a rede"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Jglro_7OUJ1Q",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "newinputtest = transforminput(testparamnorm, kmean, 'rbfMultiquadraticaInversa')\n",
        "\n",
        "lifeprob = mod.predict(newinputtest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Zrmcq2YvUJ1U"
      },
      "cell_type": "markdown",
      "source": [
        "### Erro \n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "446a3b85-2870-46db-f12a-4f02801e6e47",
        "id": "0AUbjT-NUJ1V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "binpred = np.zeros((len(lifeprob)))\n",
        "for i in range(len(lifeprob)):\n",
        "  if lifeprob[i] > 0.5:\n",
        "    binpred[i] = 1.\n",
        "\n",
        "score = 0\n",
        "for i in range(len(testlabel)):\n",
        "  if binpred[i] == testlabel[i]:\n",
        "    score += 1\n",
        "\n",
        "errorabs = abs(score-len(lifeprob))\n",
        "print('error: ' , np.sum(errorabs/len(testlabel), axis=0))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error:  0.2543859649122807\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}